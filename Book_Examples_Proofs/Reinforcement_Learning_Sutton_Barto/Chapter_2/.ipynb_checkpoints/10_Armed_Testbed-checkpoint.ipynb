{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-Armed Testbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: TestBed\n",
    "\n",
    "Class to defint TestBed containing actions and true action values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBed:\n",
    "    def __init__(self, n_arms, av_mean, av_std):\n",
    "        self.n_arms = n_arms # Number of arms in the bandit\n",
    "        self.av_mean = av_mean # Mean value to use for all action values\n",
    "        self.av_std = av_std # Standard deviation to use for all action values\n",
    "        self.av = np.zeros(nArms) # Placeholder to store action values of all arms\n",
    "        self.opt_act = 0 # Placeholder to store index of optimal action\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self): # Initialize a new testbed (a new bandit problem)\n",
    "        # True action values for all actions sampled from a normal distribution\n",
    "        # with mean = av_mean and standard deviation = av_std\n",
    "        self.av = np.random.normal(self.av_mean, self.av_std, self.n_arms)\n",
    "        self.opt_act = np.argmax(self.av) # Action with highest (true) action value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, n_arms, eps = 0):\n",
    "        self.n_arms = n_arms # Number of arms in the bandit\n",
    "        self.eps = eps # Define probability of choosing non-greedy action in eps-greedy algorithm\n",
    "        self.timeStep = 0 # Placeholder to store current time step (T)\n",
    "        self.prev_action = None # Placeholder to store action taken at previous time step (T-1)\n",
    "        self.action_count = np.zeros(n_arms) # Count of actions taken till time step (T - 1)\n",
    "        self.cum_reward = np.zeros(n_arms) # Cumulative reward obtained for each action till \n",
    "                                           # time step (T - 1)\n",
    "        self.av_est = np.zeros(n_arms) # Estimated action value at time step T\n",
    "\n",
    "    def __str__(self): # Define string to use as legend while plotting\n",
    "        if self.eps == 0:\n",
    "            return \"Greedy\"\n",
    "        else:\n",
    "            return \"epsilon = \" + str(self.eps)\n",
    "\n",
    "    # Selects action based on a epsilon-greedy behaviour,\n",
    "    # if epsilon equals zero, then the agent performs a greedy selection\n",
    "    def action(self):\n",
    "        \n",
    "        # Choose action based on eps-greedy algorithm.\n",
    "        # eps = 0 => Greedy selection.\n",
    "\n",
    "        randProb = np.random.random()   # Pick random probability between 0-1\n",
    "        if randProb < self.eProb:\n",
    "            a = np.random.choice(len(self.valEstimates))    # Select random action\n",
    "\n",
    "        # Greedy Method\n",
    "        else:\n",
    "            maxAction = np.argmax(self.valEstimates)     # Find max value estimate\n",
    "            # identify the corresponding action, as array containing only actions with max\n",
    "            action = np.where(self.valEstimates == np.argmax(self.valEstimates))[0]\n",
    "\n",
    "            # If multiple actions contain the same value, randomly select an action\n",
    "            if len(action) == 0:\n",
    "                a = maxAction\n",
    "            else:\n",
    "                a = np.random.choice(action)\n",
    "\n",
    "        # save last action in variable, and return result\n",
    "        self.lastAction = a\n",
    "        return a\n",
    "\n",
    "\n",
    "    # Interpreter - updates the value extimates amounts based on the last action\n",
    "    def interpreter(self, reward):\n",
    "        # Add 1 to the number of action taken in step\n",
    "        At = self.lastAction\n",
    "\n",
    "        self.kAction[At] += 1       # Add 1 to action selection\n",
    "        self.rSum[At] += reward     # Add reward to sum array\n",
    "\n",
    "        # Calculate new action-value, sum(r)/ka\n",
    "        self.valEstimates[At] = self.rSum[At]/self.kAction[At]\n",
    "\n",
    "        # Increase time step\n",
    "        self.timeStep += 1\n",
    "\n",
    "\n",
    "    # Reset all variables for next iteration\n",
    "    def reset(self):\n",
    "        self.timeStep = 0                    # Time Step t\n",
    "        self.lastAction = None               # Store last action\n",
    "\n",
    "        self.kAction[:] = 0                  # count of actions taken at time t\n",
    "        self.rSum[:] = 0\n",
    "        self.valEstimates[:] = 0   # action value estimates Qt ~= Q*(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() # Start timer\n",
    "n_arms = 10 # Number of arms in bandit\n",
    "iterations = 2000 # Number of repeated and independent bandit problems.\n",
    "plays = 1000 # Number of timesteps to run for each problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23374298 -1.42394939  1.56231347 -2.36682809 -0.11530937 -1.66648644\n",
      " -0.12181757  0.15724511  0.48356595 -0.85652382]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "testbed = TestBed(n_arms, av_mean = 0, av_std = 1)\n",
    "print(testbed.av)\n",
    "print(testbed.opt_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = []\n",
    "for ind in range(1000):\n",
    "    out.append(np.random.choice((0, 1), size = 1, p = (0.1, 0.9))[0])\n",
    "sum(out)/len(out)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
