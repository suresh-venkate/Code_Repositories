{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Prediction - Blackjack\n",
    "\n",
    "* **Description:** Perform first-visit and every-visit MC prediction for the Blackjack example\n",
    "* **Reference:** Reinforcement Learning, An Introduction, Second Edition by Sutton, Barto\n",
    "* **Section:** Section 5.1, Example 5.1, Pg. 93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define classes and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: Agent - Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BJ_Agent(object):\n",
    "    '''\n",
    "    Defines the agent class for the Blackjack example\n",
    "    Arguments:\n",
    "        policy: Policy to use for the agent class\n",
    "        actions: List of actions that the agent can take\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, policy, actions):\n",
    "        self.policy = policy # Initial policy\n",
    "        self.actions = actions # List of actions\n",
    "        \n",
    "    def step(self, state):\n",
    "        '''\n",
    "        Arguments:\n",
    "            state: Dictionary containing current player sum, dealer shown card and ace type\n",
    "        '''\n",
    "        # Execute one step of agent based on current state\n",
    "        if isinstance(self.policy, str):\n",
    "            if(self.policy == 'stick_20_21_policy'): # stick when player sum = 20 or 21\n",
    "                if ((state['player_sum'] == 20) or (state['player_sum'] == 21)):\n",
    "                    action = self.actions[1] # stick\n",
    "                else:\n",
    "                    action = self.actions[0] # hit\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: Environment - Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BJ_Environment(object):\n",
    "    '''\n",
    "    Defines the environment class for a n x n gridworld problem\n",
    "    Arguments:\n",
    "        n: Defines the size of the gridworld. n x n gridworld is generated\n",
    "        reward: Reward value for each transition\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dealer_hid_card, dealer_shn_card, player_init_cards, cards_list,\\\n",
    "                 card_value_dict, dealer_thresh):\n",
    "        self.dealer_hid_card = dealer_hid_card\n",
    "        self.dealer_shn_card = dealer_shn_card\n",
    "        self.dealer_cards = np.append(self.dealer_hid_card, self.dealer_shn_card)\n",
    "        self.dealer_thresh = dealer_thresh\n",
    "        self.player_init_cards = player_init_cards\n",
    "        self.cards_list = cards_list\n",
    "        self.card_value_dict = card_value_dict\n",
    "        self.player_cards = player_init_cards\n",
    "        self.player_sum = sum([self.card_value_dict[card] for card in self.player_cards])\n",
    "        self.dealer_sum = sum([self.card_value_dict[card] for card in self.dealer_cards])        \n",
    "        self.game_over = 0\n",
    "        self.update_states()\n",
    "        \n",
    "    def respond(self, action): # Respond to a particular action\n",
    "        if (action == 'player_hit'): # Player Hit action\n",
    "            player_next_card = np.random.choice(self.cards_list)\n",
    "            self.player_cards = np.append(self.player_cards, player_next_card)\n",
    "            self.player_sum = sum([self.card_value_dict[card] for card in self.player_cards])\n",
    "        if (action == 'player_stick'): # Player stick action\n",
    "            # Dealer hits until his sum becomes equal to or greater than self.dealer_thresh             \n",
    "            while (self.dealer_sum <= self.dealer_thresh):\n",
    "                dealer_next_card = np.random.choice(self.cards_list) # Next card for dealer\n",
    "                self.dealer_cards = np.append(self.dealer_cards, dealer_next_card)\n",
    "                self.dealer_sum = sum([self.card_value_dict[card] for card in self.dealer_cards])\n",
    "        self.update_states()\n",
    "        return self.state_int, self.state_vis\n",
    "    \n",
    "    def update_states(self):\n",
    "        self.game_status_upd()\n",
    "        self.state_int = {'dealer_hid_card': self.dealer_hid_card,\n",
    "                          'dealer_shn_card': self.dealer_shn_card,\n",
    "                          'dealer_cards': self.dealer_cards,\n",
    "                          'player_init_cards': self.player_init_cards,\n",
    "                          'player_cards': self.player_cards,\n",
    "                          #'cards_list': self.cards_list,\n",
    "                          #'card_value_dict': self.card_value_dict,\n",
    "                          'game_over_flag': self.game_over,\n",
    "                          'game_status': self.game_status,\n",
    "                          'reward': self.reward\n",
    "                         }\n",
    "        self.state_vis = {'player_sum': self.player_sum,\n",
    "                          'dealer_shn_card': self.dealer_shn_card,\n",
    "                          'ace_type': 'usable',\n",
    "                          'reward': self.reward\n",
    "                         }\n",
    "        \n",
    "    def game_status_upd(self):\n",
    "        if (self.player_sum > 21):\n",
    "            self.game_over = 1\n",
    "            self.game_status = 'Player_Bust_Player_Lose'\n",
    "            self.reward = -1\n",
    "        elif (self.dealer_sum > 21):\n",
    "            self.game_over = 1\n",
    "            self.game_status = 'Dealer_Bust_Player_Win'\n",
    "            self.reward = 1\n",
    "        elif (self.player_sum == 21):\n",
    "            self.game_over = 1\n",
    "            if (self.dealer_sum == 21):\n",
    "                self.game_status = 'Draw'\n",
    "                self.reward = 0\n",
    "            else:\n",
    "                self.game_status = 'Player_Win'\n",
    "                self.reward = 1\n",
    "        elif (self.dealer_sum == 21):\n",
    "            self.game_over = 1\n",
    "            self.game_status = 'Player_Lose'\n",
    "            self.reward = -1\n",
    "        else:\n",
    "            self.game_over = 0\n",
    "            self.game_status = 'Ongoing'\n",
    "            self.reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: iter_pol_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_pol_eval(svf_init, actions_list, gw_envir, theta, plot_iter_ind, num_cols, plot = False):\n",
    "    '''\n",
    "    Run the iterative policy evaluation algorithm - Non in-place method\n",
    "    Arguments:\n",
    "        svf_init: Initial state-value function\n",
    "        actions_list: List of actions that agent can take.\n",
    "        gw_envir: Environment instance\n",
    "        theta: Accuracy threshold at which to stop iteration (Pg. 97 of RL_Sutton) \n",
    "        plot_iter_ind: Iteration indices at which value functions will be plotted\n",
    "        num_cols: Number of columns to use for plotting\n",
    "        plot: boolean. If True, plot value functions\n",
    "    '''\n",
    "\n",
    "    num_plots = len(plot_iter_ind) + 2 # Plot initial and final value functions also\n",
    "    num_rows = math.ceil(num_plots / num_cols) # Number of rows to use for plotting\n",
    "    fig = plt.figure(figsize = ((num_cols * 3), (num_rows * 3.2)))\n",
    "    fig.suptitle('State Value Functions at different iterations', fontsize = 30)\n",
    "    sns.set(font_scale = 1.15)\n",
    "    \n",
    "    ind = 1 # Initialize iteration index \n",
    "    gw_size = int(np.sqrt(len(svf_init)))\n",
    "    plot_ind = 1\n",
    "    while(1): # Run iterative policy evaluation till convergence\n",
    "        if (ind == 1):\n",
    "            svf_curr = svf_init # Initialize current state value function in first iteration\n",
    "        else:\n",
    "            svf_curr = svf_next\n",
    "        svf_next = np.zeros(len(svf_init)) # v_(k+1): Placeholder for next state value function.        \n",
    "        # Loop through all states (leave out terminal states)\n",
    "        for s in range(1, (len(svf_init) - 1)): \n",
    "            for act in actions_list: # Execute all actions for each state\n",
    "                gw_envir.set_state(s)\n",
    "                s_pr, r = gw_envir.respond(act) # Get next state and reward from environment\n",
    "                # Note: Only one possible next state, reward for each s,a pair\n",
    "                # So, p(s',r|s,a) = 1\n",
    "                svf_next[s] += r + svf_curr[s_pr] # Update next state value function\n",
    "            # For equiprobable random policy pi(a|s) = 1/(num_actions)\n",
    "            svf_next[s] = svf_next[s] / len(actions_list)\n",
    "        \n",
    "        # Plot value function\n",
    "        if (plot):\n",
    "            if (ind == 1):\n",
    "                ax = plt.subplot(num_rows, num_cols, (plot_ind))\n",
    "                ax.set_title(f\"Initial_Value_Function\", fontsize = 15)\n",
    "                svf_table_df = pd.DataFrame(svf_curr.reshape(gw_size, gw_size))\n",
    "                sns.heatmap(svf_table_df, annot = True, cbar = False, square = True,\\\n",
    "                            cmap = 'Greys', vmin = 0, fmt = \"0.1f\", linewidths = 1,\\\n",
    "                            linecolor = 'black', xticklabels = False, yticklabels = False, ax = ax)\n",
    "                plot_ind += 1\n",
    "            if (ind in plot_iter_ind):\n",
    "                ax = plt.subplot(num_rows, num_cols, (plot_ind))\n",
    "                ax.set_title(f\"Iteration: {ind}\", fontsize = 15)\n",
    "                svf_table_df = pd.DataFrame(svf_next.reshape(gw_size, gw_size))\n",
    "                sns.heatmap(svf_table_df, annot = True, cbar = False, square = True,\\\n",
    "                            cmap = 'Greys', vmin = 0, fmt = \"0.1f\", linewidths = 1,\\\n",
    "                            linecolor = 'black', xticklabels = False, yticklabels = False, ax = ax) \n",
    "                plot_ind += 1\n",
    "            \n",
    "        # Compute delta\n",
    "        svf_delta = svf_next - svf_curr\n",
    "        delta = np.dot(svf_delta, svf_delta.T)\n",
    "        if (delta < theta):\n",
    "            break\n",
    "        ind += 1\n",
    "    \n",
    "    if (plot):\n",
    "        ax = plt.subplot(num_rows, num_cols, (plot_ind))\n",
    "        ax.set_title(f\"Final_Value_Function\", fontsize = 15)\n",
    "        svf_table_df = pd.DataFrame(svf_next.reshape(gw_size, gw_size))\n",
    "        sns.heatmap(svf_table_df, annot = True, cbar = False, square = True,\\\n",
    "                    cmap = 'Greys', vmin = 0, fmt = \"0.1f\", linewidths = 1,\\\n",
    "                    linecolor = 'black', xticklabels = False, yticklabels = False, ax = ax) \n",
    "        \n",
    "    return ind, svf_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: gen_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_episode(envir, agent, init_state, term_states, verb = False):\n",
    "    \n",
    "    '''\n",
    "    Generate one episode of agent-environment interaction\n",
    "    Arguments:\n",
    "        envir: Instance of environment to use for generating episode\n",
    "        agent: Instance of agent to use for generating episode\n",
    "        init_state: Initial state from which episode will start\n",
    "        term_states: List of states that are considered terminal states\n",
    "        verb: Boolean, controls verbosity\n",
    "        \n",
    "    Returns:\n",
    "        states_list: List of states encountered in the episode (includes init_state and term_state)\n",
    "        actions_list: List of actions take by agent at each time step\n",
    "        rewards_list: List of rewards received by agent at each time step\n",
    "    '''\n",
    "    \n",
    "    states_list = [] # Placeholder to store list of all states encountered in episode\n",
    "    actions_list = [] # Placeholder to store list of all actions taken by agent\n",
    "    rewards_list = [] # Placeholder to store list of rewards received at each step\n",
    "        \n",
    "    envir.set_state(init_state) # Initialize environment state\n",
    "    states_list.append(envir.state) # Update states_list with initial state\n",
    "    while(1): # Run agent till terminal states are reached\n",
    "        s_t = envir.state # Retrieve current state of environment (State at time t)\n",
    "        a_t = agent.step(s_t) # Action taken by agent at time t\n",
    "        s_tplus1, rew_tplus1 = envir.respond(a_t) # Environment reponds to action a_t and moves\n",
    "                                                  # to state s_tplus1 and\n",
    "                                                  # returns a reward rew_tplus1\n",
    "        states_list.append(s_tplus1) # Update states_list\n",
    "        actions_list.append(a_t) # Update actions_list\n",
    "        rewards_list.append(rew_tplus1) # Update rewards list\n",
    "        \n",
    "        # Stop episode if terminal state has been reached\n",
    "        if (s_tplus1 in term_states): \n",
    "            if (verb):\n",
    "                print(\"Terminal state reached.\")\n",
    "            break\n",
    "            \n",
    "    return states_list, actions_list, rewards_list        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize RL system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_init = 'stick_20_21_policy' # Set initial policy to stick_20_21\n",
    "actions_list = ['player_hit', 'player_stick'] # List of actions of agent\n",
    "\n",
    "# Define card list and card-value mapping\n",
    "card_value_dict = {'A': 11, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9,\n",
    "                   '10': 10, 'J': 10, 'Q': 10, 'K': 10\n",
    "                  }\n",
    "cards_list = list(card_value_dict.keys())\n",
    "                   \n",
    "# Get initial cards dealt to the dealer and player\n",
    "dealer_hid_card = np.random.choice(cards_list) # Hidden card of dealer\n",
    "dealer_shn_card = np.random.choice(cards_list) # Shown card of dealer\n",
    "dealer_thresh = 17 # Threshold at which dealer sticks\n",
    "player_init_cards = np.random.choice(cards_list, size = 2)\n",
    "\n",
    "bj_agent = BJ_Agent(policy_init, actions_list) # Instantiate agent\n",
    "bj_envir = BJ_Environment(dealer_hid_card, dealer_shn_card, player_init_cards, cards_list,\\\n",
    "                          card_value_dict, dealer_thresh) # Instantiate environment\n",
    "\n",
    "game_over = bj_envir.state_int['game_over_flag']\n",
    "while (game_over == 0):\n",
    "    print(bj_envir.state_int)\n",
    "    action = bj_agent.step(bj_envir.state_vis)\n",
    "    print()\n",
    "    print(action)\n",
    "    bj_envir.respond(action)\n",
    "    print()\n",
    "    print(bj_envir.state_int)\n",
    "    print()\n",
    "    game_over = bj_envir.state_int['game_over_flag']\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(bj_envir.state_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svf_init = np.zeros(gw_size ** 2) # v_0: Initialize value function array to all zeros.\n",
    "theta = 1e-5 # Accuracy threshold at which to stop iteration\n",
    "num_cols = 4\n",
    "plot_iter_ind = [1, 2, 3, 4, 50, 100]\n",
    "\n",
    "# Run iterative policy evaluation\n",
    "ind, svf_final = iter_pol_eval(svf_init, actions_list, gw_envir, theta, plot_iter_ind,\\\n",
    "                               num_cols, True)\n",
    "print(f\"Policy evaluation converged in {ind} steps\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Monte-Carlo Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction(envir, agent, non_term_states, term_states, num_ep, visit_type):\n",
    "    \n",
    "    '''\n",
    "    Function to run MC prediction and esti\n",
    "    Arguments:\n",
    "        envir: Instance of environment to use for generating episode\n",
    "        agent: Instance of agent to use for generating episode\n",
    "        init_state: Initial state from which episode will start\n",
    "        term_states: List of states that are considered terminal states\n",
    "        verb: Boolean, controls verbosity\n",
    "        \n",
    "    Returns:\n",
    "        states_list: List of states encountered in the episode (includes init_state and term_state)\n",
    "        actions_list: List of actions take by agent at each time step\n",
    "        rewards_list: List of rewards received by agent at each time step\n",
    "    '''\n",
    "    \n",
    "    # Initialize value function and count for all states to zero\n",
    "    vpi = defaultdict(int) # State value function for give policy 'pi'\n",
    "    count = defaultdict(int) # Number of times each state is encountered across episodes\n",
    "    for state in non_term_states:\n",
    "        vpi[state] = 0\n",
    "        count[state] = 0\n",
    "    \n",
    "    # Loop through 'num_ep' episodes\n",
    "    for ep in tqdm(range(num_ep)):\n",
    "        init_state = np.random.choice(non_term_states)\n",
    "        ep_states, ep_actions, ep_rewards = gen_episode(envir, agent, init_state, term_states)\n",
    "        num_timesteps = len(ep_rewards) # Number of timesteps in current episode\n",
    "        G = 0 # Initialize return to 0\n",
    "        \n",
    "        # Loop through each timestep of current episode\n",
    "        for ind in range((num_timesteps - 1), -1, -1): \n",
    "            G = gamma * G + ep_rewards[ind] # Update return of current timestep\n",
    "            curr_state = ep_states[ind] # Retrive state of current timestep\n",
    "            if (visit_type == 'first'): # For first-visit MC prediction\n",
    "                if (curr_state not in ep_states[0:ind]):\n",
    "                    count[curr_state] += 1\n",
    "                    vpi[curr_state] += (G - vpi[curr_state]) / count[curr_state]\n",
    "        \n",
    "    return vpi, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize required variables\n",
    "term_states = [0, 15] # List of terminal states\n",
    "non_term_states = list(range(1, 15, 1)) # List of non-terminal states\n",
    "gamma = 1 # Discount factor for return calculation\n",
    "num_ep = 10000 # Number of episodes to run\n",
    "visit_type = 'first'\n",
    "\n",
    "# Run MC Prediction algorithm\n",
    "vpi, count = mc_prediction(gw_envir, gw_agent, non_term_states, term_states, num_ep, visit_type)\n",
    "\n",
    "# Print final value function\n",
    "for key in vpi.keys():\n",
    "    print(\"%0.1f\" %vpi[key], end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "#plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env.action_space: Discrete(2)\n",
      "Env.observation_space: Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Seed: [9256296792507305051]\n",
      "np_random: RandomState(MT19937)\n",
      "Natural: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Env.action_space:\", env.action_space)\n",
    "print(\"Env.observation_space:\", env.observation_space)\n",
    "print(\"Seed:\", env.seed())\n",
    "print(\"np_random:\", env.np_random)\n",
    "print(\"Natural:\", env.natural)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial_Cards_With_Dealer: [4, 10]\n",
      "Initial_Cards_With_Player: [1, 4]\n",
      "Initial_Player_Score: 15\n",
      "Dealer_Visible_Card: 4\n",
      "Usable_Ace: True\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(\"Initial_Cards_With_Dealer:\", env.dealer)\n",
    "print(\"Initial_Cards_With_Player:\", env.player)\n",
    "print(\"Initial_Player_Score:\", observation[0])\n",
    "print(\"Dealer_Visible_Card:\", observation[1])\n",
    "print(\"Usable_Ace:\", observation[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_policy(observation):\n",
    "    player_score, dealer_score, usable_ace = observation\n",
    "    if player_score >= 20: # stick\n",
    "        return 0\n",
    "    else: # hit\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: gen_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gen_episode(policy, env):\n",
    "    '''\n",
    "    Function to generate one episode give an environment and a policy\n",
    "    \n",
    "    Arguments:\n",
    "        policy: Policy to use for generating episode\n",
    "        env: Environment to use for generating episode\n",
    "    '''\n",
    "    \n",
    "    # Initialize lists for storing states, actions, and rewards\n",
    "    states, actions, rewards = [], [], []\n",
    "    # Initialize the gym environment\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while True: # Loop indefinitely\n",
    "        \n",
    "        states.append(observation) # Append current observation to states list\n",
    "        action = policy(observation) # Take action based on policy\n",
    "        actions.append(action) # Append current action to actions list\n",
    "        \n",
    "        # Take next step based on action and log reward obtained\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if done: # If terminal state, stop the episode\n",
    "             break\n",
    "                \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 8, False) -1.0 True\n"
     ]
    }
   ],
   "source": [
    "states, actions, rewards = gen_episode(sample_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 10, 7]\n",
      "[8, 3]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(env.player)\n",
    "print(env.dealer)\n",
    "print(done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
