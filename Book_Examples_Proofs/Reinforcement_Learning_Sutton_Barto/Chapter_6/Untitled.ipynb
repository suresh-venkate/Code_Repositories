{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bdc7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 0 is the left terminal state\n",
    "# 6 is the right terminal state\n",
    "# 1 ... 5 represents A ... E\n",
    "VALUES = np.zeros(7)\n",
    "VALUES[1:6] = 0.5\n",
    "# For convenience, we assume all rewards are 0\n",
    "# and the left terminal state has value 0, the right terminal state has value 1\n",
    "# This trick has been used in Gambler's Problem\n",
    "VALUES[6] = 1\n",
    "\n",
    "# set up true state values\n",
    "TRUE_VALUE = np.zeros(7)\n",
    "TRUE_VALUE[1:6] = np.arange(1, 6) / 6.0\n",
    "TRUE_VALUE[6] = 1\n",
    "\n",
    "ACTION_LEFT = 0\n",
    "ACTION_RIGHT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fbc10c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @values: current states value, will be updated if @batch is False\n",
    "# @alpha: step size\n",
    "# @batch: whether to update @values\n",
    "def temporal_difference(values, alpha=0.1, batch=False):\n",
    "    state = 3\n",
    "    trajectory = [state]\n",
    "    rewards = [0]\n",
    "    while True:\n",
    "        old_state = state\n",
    "        if np.random.binomial(1, 0.5) == ACTION_LEFT:\n",
    "            state -= 1\n",
    "        else:\n",
    "            state += 1\n",
    "        # Assume all rewards are 0\n",
    "        reward = 0\n",
    "        trajectory.append(state)\n",
    "        # TD update\n",
    "        if not batch:\n",
    "            values[old_state] += alpha * (reward + values[state] - values[old_state])\n",
    "        if state == 6 or state == 0:\n",
    "            break\n",
    "        rewards.append(reward)\n",
    "    return trajectory, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9acbc26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @values: current states value, will be updated if @batch is False\n",
    "# @alpha: step size\n",
    "# @batch: whether to update @values\n",
    "def monte_carlo(values, alpha=0.1, batch=False):\n",
    "    state = 3\n",
    "    trajectory = [state]\n",
    "\n",
    "    # if end up with left terminal state, all returns are 0\n",
    "    # if end up with right terminal state, all returns are 1\n",
    "    while True:\n",
    "        if np.random.binomial(1, 0.5) == ACTION_LEFT:\n",
    "            state -= 1\n",
    "        else:\n",
    "            state += 1\n",
    "        trajectory.append(state)\n",
    "        if state == 6:\n",
    "            returns = 1.0\n",
    "            break\n",
    "        elif state == 0:\n",
    "            returns = 0.0\n",
    "            break\n",
    "\n",
    "    if not batch:\n",
    "        for state_ in trajectory[:-1]:\n",
    "            # MC update\n",
    "            values[state_] += alpha * (returns - values[state_])\n",
    "    return trajectory, [returns] * (len(trajectory) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "745c2caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.5 0.5 0.5 0.5 0.5 1. ] [0.  0.5 0.5 0.5 0.5 0.5 1. ]\n"
     ]
    }
   ],
   "source": [
    "current_values = np.copy(VALUES)\n",
    "print(current_values, VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15129093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 2, 3, 2, 3, 4, 3, 2, 1, 2, 1, 2, 1, 2, 3, 4, 3, 2, 1, 0]] [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "episodes = 1\n",
    "trajectories = []\n",
    "rewards = []\n",
    "for ep in range(episodes):\n",
    "    trajectory_, rewards_ = monte_carlo(current_values, batch=True)\n",
    "    trajectories.append(trajectory_)\n",
    "    rewards.append(rewards_)\n",
    "    \n",
    "print(trajectories, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 0 is the left terminal state\n",
    "# 6 is the right terminal state\n",
    "# 1 ... 5 represents A ... E\n",
    "VALUES = np.zeros(7)\n",
    "VALUES[1:6] = 0.5\n",
    "# For convenience, we assume all rewards are 0\n",
    "# and the left terminal state has value 0, the right terminal state has value 1\n",
    "# This trick has been used in Gambler's Problem\n",
    "VALUES[6] = 1\n",
    "\n",
    "# set up true state values\n",
    "TRUE_VALUE = np.zeros(7)\n",
    "TRUE_VALUE[1:6] = np.arange(1, 6) / 6.0\n",
    "TRUE_VALUE[6] = 1\n",
    "\n",
    "ACTION_LEFT = 0\n",
    "ACTION_RIGHT = 1\n",
    "\n",
    "# @values: current states value, will be updated if @batch is False\n",
    "# @alpha: step size\n",
    "# @batch: whether to update @values\n",
    "def temporal_difference(values, alpha=0.1, batch=False):\n",
    "    state = 3\n",
    "    trajectory = [state]\n",
    "    rewards = [0]\n",
    "    while True:\n",
    "        old_state = state\n",
    "        if np.random.binomial(1, 0.5) == ACTION_LEFT:\n",
    "            state -= 1\n",
    "        else:\n",
    "            state += 1\n",
    "        # Assume all rewards are 0\n",
    "        reward = 0\n",
    "        trajectory.append(state)\n",
    "        # TD update\n",
    "        if not batch:\n",
    "            values[old_state] += alpha * (reward + values[state] - values[old_state])\n",
    "        if state == 6 or state == 0:\n",
    "            break\n",
    "        rewards.append(reward)\n",
    "    return trajectory, rewards\n",
    "\n",
    "# @values: current states value, will be updated if @batch is False\n",
    "# @alpha: step size\n",
    "# @batch: whether to update @values\n",
    "def monte_carlo(values, alpha=0.1, batch=False):\n",
    "    state = 3\n",
    "    trajectory = [state]\n",
    "\n",
    "    # if end up with left terminal state, all returns are 0\n",
    "    # if end up with right terminal state, all returns are 1\n",
    "    while True:\n",
    "        if np.random.binomial(1, 0.5) == ACTION_LEFT:\n",
    "            state -= 1\n",
    "        else:\n",
    "            state += 1\n",
    "        trajectory.append(state)\n",
    "        if state == 6:\n",
    "            returns = 1.0\n",
    "            break\n",
    "        elif state == 0:\n",
    "            returns = 0.0\n",
    "            break\n",
    "\n",
    "    if not batch:\n",
    "        for state_ in trajectory[:-1]:\n",
    "            # MC update\n",
    "            values[state_] += alpha * (returns - values[state_])\n",
    "    return trajectory, [returns] * (len(trajectory) - 1)\n",
    "\n",
    "# Example 6.2 left\n",
    "def compute_state_value():\n",
    "    episodes = [0, 1, 10, 100]\n",
    "    current_values = np.copy(VALUES)\n",
    "    plt.figure(1)\n",
    "    for i in range(episodes[-1] + 1):\n",
    "        if i in episodes:\n",
    "            plt.plot((\"A\", \"B\", \"C\", \"D\", \"E\"), current_values[1:6], label=str(i) + ' episodes')\n",
    "        temporal_difference(current_values)\n",
    "    plt.plot((\"A\", \"B\", \"C\", \"D\", \"E\"), TRUE_VALUE[1:6], label='true values')\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel('Estimated Value')\n",
    "    plt.legend()\n",
    "\n",
    "# Example 6.2 right\n",
    "def rms_error():\n",
    "    # Same alpha value can appear in both arrays\n",
    "    td_alphas = [0.15, 0.1, 0.05]\n",
    "    mc_alphas = [0.01, 0.02, 0.03, 0.04]\n",
    "    episodes = 100 + 1\n",
    "    runs = 100\n",
    "    for i, alpha in enumerate(td_alphas + mc_alphas):\n",
    "        total_errors = np.zeros(episodes)\n",
    "        if i < len(td_alphas):\n",
    "            method = 'TD'\n",
    "            linestyle = 'solid'\n",
    "        else:\n",
    "            method = 'MC'\n",
    "            linestyle = 'dashdot'\n",
    "        for r in tqdm(range(runs)):\n",
    "            errors = []\n",
    "            current_values = np.copy(VALUES)\n",
    "            for i in range(0, episodes):\n",
    "                errors.append(np.sqrt(np.sum(np.power(TRUE_VALUE - current_values, 2)) / 5.0))\n",
    "                if method == 'TD':\n",
    "                    temporal_difference(current_values, alpha=alpha)\n",
    "                else:\n",
    "                    monte_carlo(current_values, alpha=alpha)\n",
    "            total_errors += np.asarray(errors)\n",
    "        total_errors /= runs\n",
    "        plt.plot(total_errors, linestyle=linestyle, label=method + ', $\\\\alpha$ = %.02f' % (alpha))\n",
    "    plt.xlabel('Walks/Episodes')\n",
    "    plt.ylabel('Empirical RMS error, averaged over states')\n",
    "    plt.legend()\n",
    "\n",
    "# Figure 6.2\n",
    "# @method: 'TD' or 'MC'\n",
    "def batch_updating(method, episodes, alpha=0.001):\n",
    "    # perform 100 independent runs\n",
    "    runs = 100\n",
    "    total_errors = np.zeros(episodes)\n",
    "    for r in tqdm(range(0, runs)):\n",
    "        current_values = np.copy(VALUES)\n",
    "        current_values[1:6] = -1\n",
    "        errors = []\n",
    "        # track shown trajectories and reward/return sequences\n",
    "        trajectories = []\n",
    "        rewards = []\n",
    "        for ep in range(episodes):\n",
    "            if method == 'TD':\n",
    "                trajectory_, rewards_ = temporal_difference(current_values, batch=True)\n",
    "            else:\n",
    "                trajectory_, rewards_ = monte_carlo(current_values, batch=True)\n",
    "            trajectories.append(trajectory_)\n",
    "            rewards.append(rewards_)\n",
    "            while True:\n",
    "                # keep feeding our algorithm with trajectories seen so far until state value function converges\n",
    "                updates = np.zeros(7)\n",
    "                for trajectory_, rewards_ in zip(trajectories, rewards):\n",
    "                    for i in range(0, len(trajectory_) - 1):\n",
    "                        if method == 'TD':\n",
    "                            updates[trajectory_[i]] += rewards_[i] + current_values[trajectory_[i + 1]] - current_values[trajectory_[i]]\n",
    "                        else:\n",
    "                            updates[trajectory_[i]] += rewards_[i] - current_values[trajectory_[i]]\n",
    "                updates *= alpha\n",
    "                if np.sum(np.abs(updates)) < 1e-3:\n",
    "                    break\n",
    "                # perform batch updating\n",
    "                current_values += updates\n",
    "            # calculate rms error\n",
    "            errors.append(np.sqrt(np.sum(np.power(current_values - TRUE_VALUE, 2)) / 5.0))\n",
    "        total_errors += np.asarray(errors)\n",
    "    total_errors /= runs\n",
    "    return total_errors\n",
    "\n",
    "def example_6_2():\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    compute_state_value()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    rms_error()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('../images/example_6_2.png')\n",
    "    plt.close()\n",
    "\n",
    "def figure_6_2():\n",
    "    episodes = 100 + 1\n",
    "    td_errors = batch_updating('TD', episodes)\n",
    "    mc_errors = batch_updating('MC', episodes)\n",
    "\n",
    "    plt.plot(td_errors, label='TD')\n",
    "    plt.plot(mc_errors, label='MC')\n",
    "    plt.title(\"Batch Training\")\n",
    "    plt.xlabel('Walks/Episodes')\n",
    "    plt.ylabel('RMS error, averaged over states')\n",
    "    plt.xlim(0, 100)\n",
    "    plt.ylim(0, 0.25)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('../images/figure_6_2.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    example_6_2()\n",
    "    figure_6_2()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
