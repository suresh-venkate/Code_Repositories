{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Maze class as defined in Example 8.1: Dyna Maze, Section 8.2 of RL_Sutton\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.WORLD_WIDTH = 9 # maze width\n",
    "        self.WORLD_HEIGHT = 6 # maze height\n",
    "\n",
    "        # all possible actions\n",
    "        self.ACTION_UP = 0\n",
    "        self.ACTION_DOWN = 1\n",
    "        self.ACTION_LEFT = 2\n",
    "        self.ACTION_RIGHT = 3\n",
    "        self.actions = [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_LEFT, self.ACTION_RIGHT]\n",
    "\n",
    "        self.START_STATE = [2, 0] # start state\n",
    "        self.GOAL_STATES = [[0, 8]] # goal state\n",
    "\n",
    "        # all obstacles\n",
    "        self.obstacles = [[1, 2], [2, 2], [3, 2], [0, 7], [1, 7], [2, 7], [4, 5]]\n",
    "#         self.oldObstacles = None\n",
    "#         self.newObstacles = None\n",
    "\n",
    "#         # time to change obstacles\n",
    "#         self.changingPoint = None\n",
    "\n",
    "        # initial state-action pair values\n",
    "        self.stateActionValues = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions)))\n",
    "\n",
    "        # max steps\n",
    "        self.maxSteps = float('inf')\n",
    "\n",
    "#         # track the resolution for this maze\n",
    "#         self.resolution = 1\n",
    "\n",
    "    # take @action in @state\n",
    "    # @return: [new state, reward]\n",
    "    def takeAction(self, state, action):\n",
    "        x, y = state\n",
    "        if action == self.ACTION_UP:\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == self.ACTION_DOWN:\n",
    "            x = min(x + 1, self.WORLD_HEIGHT - 1)\n",
    "        elif action == self.ACTION_LEFT:\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == self.ACTION_RIGHT:\n",
    "            y = min(y + 1, self.WORLD_WIDTH - 1)\n",
    "        if [x, y] in self.obstacles:\n",
    "            x, y = state\n",
    "        if [x, y] in self.GOAL_STATES:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        return [x, y], reward\n",
    "    \n",
    "#     # extend a state to a higher resolution maze\n",
    "#     # @state: state in lower resoultion maze\n",
    "#     # @factor: extension factor, one state will become factor^2 states after extension\n",
    "#     def extendState(self, state, factor):\n",
    "#         newState = [state[0] * factor, state[1] * factor]\n",
    "#         newStates = []\n",
    "#         for i in range(0, factor):\n",
    "#             for j in range(0, factor):\n",
    "#                 newStates.append([newState[0] + i, newState[1] + j])\n",
    "#         return newStates\n",
    "\n",
    "#     # extend a state into higher resolution\n",
    "#     # one state in original maze will become @factor^2 states in @return new maze\n",
    "#     def extendMaze(self, factor):\n",
    "#         newMaze = Maze()\n",
    "#         newMaze.WORLD_WIDTH = self.WORLD_WIDTH * factor\n",
    "#         newMaze.WORLD_HEIGHT = self.WORLD_HEIGHT * factor\n",
    "#         newMaze.START_STATE = [self.START_STATE[0] * factor, self.START_STATE[1] * factor]\n",
    "#         newMaze.GOAL_STATES = self.extendState(self.GOAL_STATES[0], factor)\n",
    "#         newMaze.obstacles = []\n",
    "#         for state in self.obstacles:\n",
    "#             newMaze.obstacles.extend(self.extendState(state, factor))\n",
    "#         newMaze.stateActionValues = np.zeros((newMaze.WORLD_HEIGHT, newMaze.WORLD_WIDTH, len(newMaze.actions)))\n",
    "#         newMaze.resolution = factor\n",
    "#         return newMaze    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: DynaParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaParams:\n",
    "    \n",
    "    '''\n",
    "    Wrapper Class for parameters of dyna algorithms\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.gamma = 0.95 # discount\n",
    "        self.epsilon = 0.1 # probability for exploration\n",
    "        self.alpha = 0.1 # step size\n",
    "        self.planningSteps = 5 # n-step planning\n",
    "        self.runs = 10 # average over several independent runs\n",
    "        self.methods = ['Dyna-Q', 'Dyna-Q+'] # algorithm names\n",
    "        \n",
    "#         self.theta = 0 # threshold for priority queue\n",
    "#         self.timeWeight = 0 # weight for elapsed time     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: chooseAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseAction(state, stateActionValues, maze, dynaParams):\n",
    "    '''\n",
    "    # choose an action based on epsilon-greedy algorithm\n",
    "    '''\n",
    "    if np.random.binomial(1, dynaParams.epsilon) == 1:\n",
    "        return np.random.choice(maze.actions)\n",
    "    else:\n",
    "        values = stateActionValues[state[0], state[1], :]\n",
    "        return np.random.choice([action for action, value in enumerate(values) if value == np.max(values)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: TrivialModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialModel:\n",
    "    \n",
    "    '''\n",
    "    # Trivial model for planning in Dyna-Q\n",
    "    '''\n",
    "\n",
    "    # @rand: an instance of np.random.RandomState for sampling\n",
    "    def __init__(self, rand = np.random):\n",
    "        self.model = dict() # Initialize model as a dict\n",
    "        self.rand = rand # Initialize random state\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, currentState, action, newState, reward):\n",
    "        if tuple(currentState) not in self.model.keys():\n",
    "            self.model[tuple(currentState)] = dict()\n",
    "        self.model[tuple(currentState)][action] = [list(newState), reward]\n",
    "\n",
    "    # randomly sample from previous experience\n",
    "    def sample(self):\n",
    "        stateIndex = self.rand.choice(range(0, len(self.model.keys())))\n",
    "        state = list(self.model)[stateIndex]\n",
    "        actionIndex = self.rand.choice(range(0, len(self.model[state].keys())))\n",
    "        action = list(self.model[state])[actionIndex]\n",
    "        newState, reward = self.model[state][action]\n",
    "        return list(state), action, list(newState), reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: dynaQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play for an episode for Dyna-Q algorithm\n",
    "# @stateActionValues: state action pair values, will be updated\n",
    "# @model: model instance for planning\n",
    "# @planningSteps: steps for planning\n",
    "# @maze: a maze instance containing all information about the environment\n",
    "# @dynaParams: several params for the algorithm\n",
    "def dynaQ(stateActionValues, model, maze, dynaParams):\n",
    "    currentState = maze.START_STATE\n",
    "    steps = 0\n",
    "    while currentState not in maze.GOAL_STATES: # Loop until goal state is reached\n",
    "        steps += 1 # Update steps taken till now\n",
    "        action = chooseAction(currentState, stateActionValues, maze, dynaParams) # Choose action based on eps-greedy policy\n",
    "        newState, reward = maze.takeAction(currentState, action) # take action\n",
    "\n",
    "        # Q-Learning update\n",
    "        stateActionValues[currentState[0], currentState[1], action] += \\\n",
    "            dynaParams.alpha * (reward + dynaParams.gamma * np.max(stateActionValues[newState[0], newState[1], :]) -\n",
    "            stateActionValues[currentState[0], currentState[1], action])\n",
    "\n",
    "        # feed the model with experience\n",
    "        model.feed(currentState, action, newState, reward)\n",
    "\n",
    "        # sample experience from the model\n",
    "        for t in range(0, dynaParams.planningSteps):\n",
    "            stateSample, actionSample, newStateSample, rewardSample = model.sample()\n",
    "            stateActionValues[stateSample[0], stateSample[1], actionSample] += \\\n",
    "                dynaParams.alpha * (rewardSample + dynaParams.gamma * np.max(stateActionValues[newStateSample[0],\n",
    "                newStateSample[1], :]) - stateActionValues[stateSample[0], stateSample[1], actionSample])\n",
    "               \n",
    "\n",
    "        currentState = newState # Update currentState\n",
    "\n",
    "        # check whether it has exceeded the step limit\n",
    "        if steps > maze.maxSteps:\n",
    "            break\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.1: Dyna Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.05s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dynaMaze = Maze() # Instantiate Dyna Maze environment\n",
    "dynaParams = DynaParams() # Define parameters of Dyna algorithm\n",
    "\n",
    "planningSteps = [0, 5, 50] # Array of planning steps\n",
    "episodes = 50 # Number of episodes to run\n",
    "steps = np.zeros((len(planningSteps), episodes))\n",
    "runs = 10 # Number of repetitions of the experiment\n",
    "\n",
    "# this random seed is for sampling from model\n",
    "# we do need this separate random seed to make sure the first episodes for all planning steps are the same\n",
    "rand = np.random.RandomState(1)\n",
    "\n",
    "for run in tqdm(range(0, runs)):\n",
    "    for index, planningStep in enumerate(planningSteps):\n",
    "        dynaParams.planningSteps = planningStep # Set current value of planningStep\n",
    "        np.random.seed(run + 1) # set same random seed for each planning step\n",
    "        currentStateActionValues = np.copy(dynaMaze.stateActionValues) # Initialize state-action values\n",
    "        model = TrivialModel(rand) # generate an instance of Dyna-Q model\n",
    "        for ep in range(0, episodes): # Run dynaQ for multiple episodes\n",
    "            #print('run:', run, 'planning step:', planningStep, 'episode:', ep)\n",
    "            steps[index, ep] += dynaQ(currentStateActionValues, model, dynaMaze, dynaParams)\n",
    "\n",
    "steps /= runs # Averaged number of steps over runs\n",
    "\n",
    "plt.figure(0)\n",
    "for i in range(0, len(planningSteps)):\n",
    "    plt.plot(range(1, episodes), steps[i, 1:], label=str(planningSteps[i]) + ' planning steps')\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('steps per episode')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    episodes = 50\n",
    "    planningSteps = [0, 5, 50]\n",
    "    steps = np.zeros((len(planningSteps), episodes))\n",
    "\n",
    "    # this random seed is for sampling from model\n",
    "    # we do need this separate random seed to make sure the first episodes for all planning steps are the same\n",
    "    rand = np.random.RandomState(0)\n",
    "\n",
    "    for run in range(0, runs):\n",
    "        for index, planningStep in zip(range(0, len(planningSteps)), planningSteps):\n",
    "            dynaParams.planningSteps = planningStep\n",
    "\n",
    "            # set same random seed for each planning step\n",
    "            np.random.seed(run)\n",
    "\n",
    "            currentStateActionValues = np.copy(dynaMaze.stateActionValues)\n",
    "\n",
    "            # generate an instance of Dyna-Q model\n",
    "            model = TrivialModel(rand)\n",
    "            for ep in range(0, episodes):\n",
    "                print('run:', run, 'planning step:', planningStep, 'episode:', ep)\n",
    "                steps[index, ep] += dynaQ(currentStateActionValues, model, dynaMaze, dynaParams)\n",
    "\n",
    "    # averaging over runs\n",
    "    steps /= runs\n",
    "\n",
    "    plt.figure(0)\n",
    "    for i in range(0, len(planningSteps)):\n",
    "        plt.plot(range(0, episodes), steps[i, :], label=str(planningSteps[i]) + ' planning steps')\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('steps per episode')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze_Width:  9\n",
      "Maze_Height:  6\n",
      "Maze_Actions:  [0, 1, 2, 3]\n",
      "Maze_Start_State:  [2, 0]\n",
      "Maze_Goal_States:  [[0, 8]]\n",
      "Maze_obstacles:  [[1, 2], [2, 2], [3, 2], [0, 7], [1, 7], [2, 7], [4, 5]]\n",
      "Maze_stateActionValues_Shape:  (6, 9, 4)\n"
     ]
    }
   ],
   "source": [
    "dynaMaze = Maze()\n",
    "print(\"Maze_Width: \", dynaMaze.WORLD_WIDTH)\n",
    "print(\"Maze_Height: \", dynaMaze.WORLD_HEIGHT)\n",
    "print(\"Maze_Actions: \", dynaMaze.actions)\n",
    "print(\"Maze_Start_State: \", dynaMaze.START_STATE)\n",
    "print(\"Maze_Goal_States: \", dynaMaze.GOAL_STATES)\n",
    "print(\"Maze_obstacles: \", dynaMaze.obstacles)\n",
    "print(\"Maze_stateActionValues_Shape: \", dynaMaze.stateActionValues.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import heapq\n",
    "\n",
    "class PriorityQueue:\n",
    "    def __init__(self):\n",
    "        self.pq = []\n",
    "        self.entry_finder = {}\n",
    "        self.REMOVED = '<removed-task>'\n",
    "        self.counter = itertools.count()\n",
    "\n",
    "    def addItem(self, item, priority=0):\n",
    "        if item in self.entry_finder:\n",
    "            self.removeItem(item)\n",
    "        count = next(self.counter)\n",
    "        entry = [priority, count, item]\n",
    "        self.entry_finder[item] = entry\n",
    "        heapq.heappush(self.pq, entry)\n",
    "\n",
    "    def removeItem(self, item):\n",
    "        entry = self.entry_finder.pop(item)\n",
    "        entry[-1] = self.REMOVED\n",
    "\n",
    "    def popTask(self):\n",
    "        while self.pq:\n",
    "            priority, count, item = heapq.heappop(self.pq)\n",
    "            if item is not self.REMOVED:\n",
    "                del self.entry_finder[item]\n",
    "                return item, priority\n",
    "        raise KeyError('pop from an empty priority queue')\n",
    "\n",
    "    def empty(self):\n",
    "        return not self.entry_finder\n",
    "\n",
    "# choose an action based on epsilon-greedy algorithm\n",
    "def chooseAction(state, stateActionValues, maze, dynaParams):\n",
    "    if np.random.binomial(1, dynaParams.epsilon) == 1:\n",
    "        return np.random.choice(maze.actions)\n",
    "    else:\n",
    "        values = stateActionValues[state[0], state[1], :]\n",
    "        return np.random.choice([action for action, value in enumerate(values) if value == np.max(values)])\n",
    "\n",
    "# Time-based model for planning in Dyna-Q+\n",
    "class TimeModel:\n",
    "\n",
    "    # @maze: the maze instance. Indeed it's not very reasonable to give access to maze to the model.\n",
    "    # @timeWeight: also called kappa, the weight for elapsed time in sampling reward, it need to be small\n",
    "    # @rand: an instance of np.random.RandomState for sampling\n",
    "    def __init__(self, maze, timeWeight=1e-4, rand=np.random):\n",
    "        self.rand = rand\n",
    "        self.model = dict()\n",
    "\n",
    "        # track the total time\n",
    "        self.time = 0\n",
    "\n",
    "        self.timeWeight = timeWeight\n",
    "        self.maze = maze\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, currentState, action, newState, reward):\n",
    "        self.time += 1\n",
    "        if tuple(currentState) not in self.model.keys():\n",
    "            self.model[tuple(currentState)] = dict()\n",
    "\n",
    "            # Actions that had never been tried before from a state were allowed to be considered in the planning step\n",
    "            for action_ in self.maze.actions:\n",
    "                if action_ != action:\n",
    "                    # Such actions would lead back to the same state with a reward of zero\n",
    "                    # Notice that the minimum time stamp is 1 instead of 0\n",
    "                    self.model[tuple(currentState)][action_] = [list(currentState), 0, 1]\n",
    "\n",
    "        self.model[tuple(currentState)][action] = [list(newState), reward, self.time]\n",
    "\n",
    "    # randomly sample from previous experience\n",
    "    def sample(self):\n",
    "        stateIndex = self.rand.choice(range(0, len(self.model.keys())))\n",
    "        state = list(self.model)[stateIndex]\n",
    "        actionIndex = self.rand.choice(range(0, len(self.model[state].keys())))\n",
    "        action = list(self.model[state])[actionIndex]\n",
    "        newState, reward, time = self.model[state][action]\n",
    "\n",
    "        # adjust reward with elapsed time since last vist\n",
    "        reward += self.timeWeight * np.sqrt(self.time - time)\n",
    "\n",
    "        return list(state), action, list(newState), reward\n",
    "\n",
    "# Model containing a priority queue for Prioritized Sweeping\n",
    "class PriorityModel(TrivialModel):\n",
    "\n",
    "    def __init__(self, rand=np.random):\n",
    "        TrivialModel.__init__(self, rand)\n",
    "        # maintain a priority queue\n",
    "        self.priorityQueue = PriorityQueue()\n",
    "        # track predecessors for every state\n",
    "        self.predecessors = dict()\n",
    "\n",
    "    # add a @state-@action pair into the priority queue with priority @priority\n",
    "    def insert(self, priority, state, action):\n",
    "        # note the priority queue is a minimum heap, so we use -priority\n",
    "        self.priorityQueue.addItem((tuple(state), action), -priority)\n",
    "\n",
    "    # @return: whether the priority queue is empty\n",
    "    def empty(self):\n",
    "        return self.priorityQueue.empty()\n",
    "\n",
    "    # get the first item in the priority queue\n",
    "    def sample(self):\n",
    "        (state, action), priority = self.priorityQueue.popTask()\n",
    "        newState, reward = self.model[state][action]\n",
    "        return -priority, list(state), action, list(newState), reward\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, currentState, action, newState, reward):\n",
    "        TrivialModel.feed(self, currentState, action, newState, reward)\n",
    "        if tuple(newState) not in self.predecessors.keys():\n",
    "            self.predecessors[tuple(newState)] = set()\n",
    "        self.predecessors[tuple(newState)].add((tuple(currentState), action))\n",
    "\n",
    "    # get all seen predecessors of a state @state\n",
    "    def predecessor(self, state):\n",
    "        if tuple(state) not in self.predecessors.keys():\n",
    "            return []\n",
    "        predecessors = []\n",
    "        for statePre, actionPre in list(self.predecessors[tuple(state)]):\n",
    "            predecessors.append([list(statePre), actionPre, self.model[statePre][actionPre][1]])\n",
    "        return predecessors\n",
    "\n",
    "\n",
    "# play for an episode for Dyna-Q algorithm\n",
    "# @stateActionValues: state action pair values, will be updated\n",
    "# @model: model instance for planning\n",
    "# @planningSteps: steps for planning\n",
    "# @maze: a maze instance containing all information about the environment\n",
    "# @dynaParams: several params for the algorithm\n",
    "def dynaQ(stateActionValues, model, maze, dynaParams):\n",
    "    currentState = maze.START_STATE\n",
    "    steps = 0\n",
    "    while currentState not in maze.GOAL_STATES:\n",
    "        # track the steps\n",
    "        steps += 1\n",
    "\n",
    "        # get action\n",
    "        action = chooseAction(currentState, stateActionValues, maze, dynaParams)\n",
    "\n",
    "        # take action\n",
    "        newState, reward = maze.takeAction(currentState, action)\n",
    "\n",
    "        # Q-Learning update\n",
    "        stateActionValues[currentState[0], currentState[1], action] += \\\n",
    "            dynaParams.alpha * (reward + dynaParams.gamma * np.max(stateActionValues[newState[0], newState[1], :]) -\n",
    "            stateActionValues[currentState[0], currentState[1], action])\n",
    "\n",
    "        # feed the model with experience\n",
    "        model.feed(currentState, action, newState, reward)\n",
    "\n",
    "        # sample experience from the model\n",
    "        for t in range(0, dynaParams.planningSteps):\n",
    "            stateSample, actionSample, newStateSample, rewardSample = model.sample()\n",
    "            stateActionValues[stateSample[0], stateSample[1], actionSample] += \\\n",
    "                dynaParams.alpha * (rewardSample + dynaParams.gamma * np.max(stateActionValues[newStateSample[0], newStateSample[1], :]) -\n",
    "                stateActionValues[stateSample[0], stateSample[1], actionSample])\n",
    "\n",
    "        currentState = newState\n",
    "\n",
    "        # check whether it has exceeded the step limit\n",
    "        if steps > maze.maxSteps:\n",
    "            break\n",
    "\n",
    "    return steps\n",
    "\n",
    "# play for an episode for prioritized sweeping algorithm\n",
    "# @stateActionValues: state action pair values, will be updated\n",
    "# @model: model instance for planning\n",
    "# @maze: a maze instance containing all information about the environment\n",
    "# @dynaParams: several params for the algorithm\n",
    "# @return: # of backups happened in planning phase in this episode\n",
    "def prioritizedSweeping(stateActionValues, model, maze, dynaParams):\n",
    "    currentState = maze.START_STATE\n",
    "\n",
    "    # track the steps in this episode\n",
    "    steps = 0\n",
    "\n",
    "    # track the backups in planning phase\n",
    "    backups = 0\n",
    "\n",
    "    while currentState not in maze.GOAL_STATES:\n",
    "        steps += 1\n",
    "\n",
    "        # get action\n",
    "        action = chooseAction(currentState, stateActionValues, maze, dynaParams)\n",
    "\n",
    "        # take action\n",
    "        newState, reward = maze.takeAction(currentState, action)\n",
    "\n",
    "        # feed the model with experience\n",
    "        model.feed(currentState, action, newState, reward)\n",
    "\n",
    "        # get the priority for current state action pair\n",
    "        priority = np.abs(reward + dynaParams.gamma * np.max(stateActionValues[newState[0], newState[1], :]) -\n",
    "                          stateActionValues[currentState[0], currentState[1], action])\n",
    "\n",
    "        if priority > dynaParams.theta:\n",
    "            model.insert(priority, currentState, action)\n",
    "\n",
    "        # start planning\n",
    "        planningStep = 0\n",
    "\n",
    "        # planning for several steps,\n",
    "        # although keep planning until the priority queue becomes empty will converge much faster\n",
    "        while planningStep < dynaParams.planningSteps and not model.empty():\n",
    "            # get a sample with highest priority from the model\n",
    "            priority, sampleState, sampleAction, sampleNewState, sampleReward = model.sample()\n",
    "\n",
    "            # update the state action value for the sample\n",
    "            delta = sampleReward + dynaParams.gamma * np.max(stateActionValues[sampleNewState[0], sampleNewState[1], :]) - \\\n",
    "                    stateActionValues[sampleState[0], sampleState[1], sampleAction]\n",
    "            stateActionValues[sampleState[0], sampleState[1], sampleAction] += dynaParams.alpha * delta\n",
    "\n",
    "            # deal with all the predecessors of the sample state\n",
    "            for statePre, actionPre, rewardPre in model.predecessor(sampleState):\n",
    "                priority = np.abs(rewardPre + dynaParams.gamma * np.max(stateActionValues[sampleState[0], sampleState[1], :]) -\n",
    "                                  stateActionValues[statePre[0], statePre[1], actionPre])\n",
    "                if priority > dynaParams.theta:\n",
    "                    model.insert(priority, statePre, actionPre)\n",
    "            planningStep += 1\n",
    "\n",
    "        currentState = newState\n",
    "\n",
    "        # update the # of backups\n",
    "        backups += planningStep\n",
    "\n",
    "    return backups\n",
    "\n",
    "# Figure 8.3, DynaMaze, use 10 runs instead of 30 runs\n",
    "def figure8_3():\n",
    "\n",
    "    # set up an instance for DynaMaze\n",
    "    dynaMaze = Maze()\n",
    "    dynaParams = DynaParams()\n",
    "\n",
    "    runs = 10\n",
    "    episodes = 50\n",
    "    planningSteps = [0, 5, 50]\n",
    "    steps = np.zeros((len(planningSteps), episodes))\n",
    "\n",
    "    # this random seed is for sampling from model\n",
    "    # we do need this separate random seed to make sure the first episodes for all planning steps are the same\n",
    "    rand = np.random.RandomState(0)\n",
    "\n",
    "    for run in range(0, runs):\n",
    "        for index, planningStep in zip(range(0, len(planningSteps)), planningSteps):\n",
    "            dynaParams.planningSteps = planningStep\n",
    "\n",
    "            # set same random seed for each planning step\n",
    "            np.random.seed(run)\n",
    "\n",
    "            currentStateActionValues = np.copy(dynaMaze.stateActionValues)\n",
    "\n",
    "            # generate an instance of Dyna-Q model\n",
    "            model = TrivialModel(rand)\n",
    "            for ep in range(0, episodes):\n",
    "                print('run:', run, 'planning step:', planningStep, 'episode:', ep)\n",
    "                steps[index, ep] += dynaQ(currentStateActionValues, model, dynaMaze, dynaParams)\n",
    "\n",
    "    # averaging over runs\n",
    "    steps /= runs\n",
    "\n",
    "    plt.figure(0)\n",
    "    for i in range(0, len(planningSteps)):\n",
    "        plt.plot(range(0, episodes), steps[i, :], label=str(planningSteps[i]) + ' planning steps')\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('steps per episode')\n",
    "    plt.legend()\n",
    "\n",
    "# wrapper function for changing maze\n",
    "# @maze: a maze instance\n",
    "# @dynaParams: several parameters for dyna algorithms\n",
    "def changingMaze(maze, dynaParams):\n",
    "\n",
    "    # set up max steps\n",
    "    maxSteps = maze.maxSteps\n",
    "\n",
    "    # track the cumulative rewards\n",
    "    rewards = np.zeros((2, maxSteps))\n",
    "\n",
    "    for run in range(0, dynaParams.runs):\n",
    "        # set up models\n",
    "        models = [TrivialModel(), TimeModel(maze, timeWeight=dynaParams.timeWeight)]\n",
    "\n",
    "        # track cumulative reward in current run\n",
    "        rewards_ = np.zeros((2, maxSteps))\n",
    "\n",
    "        # initialize state action values\n",
    "        stateActionValues = [np.copy(maze.stateActionValues), np.copy(maze.stateActionValues)]\n",
    "\n",
    "        for i in range(0, len(dynaParams.methods)):\n",
    "            print('run:', run, dynaParams.methods[i])\n",
    "\n",
    "            # set old obstacles for the maze\n",
    "            maze.obstacles = maze.oldObstacles\n",
    "\n",
    "            steps = 0\n",
    "            lastSteps = steps\n",
    "            while steps < maxSteps:\n",
    "                # play for an episode\n",
    "                steps += dynaQ(stateActionValues[i], models[i], maze, dynaParams)\n",
    "\n",
    "                # update cumulative rewards\n",
    "                steps_ = min(steps, maxSteps - 1)\n",
    "                rewards_[i, lastSteps: steps_] = rewards_[i, lastSteps]\n",
    "                rewards_[i, steps_] = rewards_[i, lastSteps] + 1\n",
    "                lastSteps = steps\n",
    "\n",
    "                if steps > maze.changingPoint:\n",
    "                    # change the obstacles\n",
    "                    maze.obstacles = maze.newObstacles\n",
    "        rewards += rewards_\n",
    "\n",
    "    # averaging over runs\n",
    "    rewards /= dynaParams.runs\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Figure 8.5, BlockingMaze\n",
    "def figure8_5():\n",
    "    # set up a blocking maze instance\n",
    "    blockingMaze = Maze()\n",
    "    blockingMaze.START_STATE = [5, 3]\n",
    "    blockingMaze.GOAL_STATES = [[0, 8]]\n",
    "    blockingMaze.oldObstacles = [[3, i] for i in range(0, 8)]\n",
    "\n",
    "    # new obstalces will block the optimal path\n",
    "    blockingMaze.newObstacles = [[3, i] for i in range(1, 9)]\n",
    "\n",
    "    # step limit\n",
    "    blockingMaze.maxSteps = 3000\n",
    "\n",
    "    # obstacles will change after 1000 steps\n",
    "    # the exact step for changing will be different\n",
    "    # However given that 1000 steps is long enough for both algorithms to converge,\n",
    "    # the difference is guaranteed to be very small\n",
    "    blockingMaze.changingPoint = 1000\n",
    "\n",
    "    # set up parameters\n",
    "    dynaParams = DynaParams()\n",
    "\n",
    "    # it's a tricky alpha ...\n",
    "    dynaParams.alpha = 0.7\n",
    "\n",
    "    # 5-step planning\n",
    "    dynaParams.planningSteps = 5\n",
    "\n",
    "    # average over 20 runs\n",
    "    dynaParams.runs = 20\n",
    "\n",
    "    # kappa must be small, as the reward for getting the goal is only 1\n",
    "    dynaParams.timeWeight = 1e-4\n",
    "\n",
    "    # play\n",
    "    rewards = changingMaze(blockingMaze, dynaParams)\n",
    "\n",
    "    plt.figure(1)\n",
    "    for i in range(0, len(dynaParams.methods)):\n",
    "        plt.plot(range(0, blockingMaze.maxSteps), rewards[i, :], label=dynaParams.methods[i])\n",
    "    plt.xlabel('time steps')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    plt.legend()\n",
    "\n",
    "# Figure 8.6, ShortcutMaze\n",
    "def figure8_6():\n",
    "    # set up a shortcut maze instance\n",
    "    shortcutMaze = Maze()\n",
    "    shortcutMaze.START_STATE = [5, 3]\n",
    "    shortcutMaze.GOAL_STATES = [[0, 8]]\n",
    "    shortcutMaze.oldObstacles = [[3, i] for i in range(1, 9)]\n",
    "\n",
    "    # new obstacles will have a shorter path\n",
    "    shortcutMaze.newObstacles = [[3, i] for i in range(1, 8)]\n",
    "\n",
    "    # step limit\n",
    "    shortcutMaze.maxSteps = 6000\n",
    "\n",
    "    # obstacles will change after 3000 steps\n",
    "    # the exact step for changing will be different\n",
    "    # However given that 3000 steps is long enough for both algorithms to converge,\n",
    "    # the difference is guaranteed to be very small\n",
    "    shortcutMaze.changingPoint = 3000\n",
    "\n",
    "    # set up parameters\n",
    "    dynaParams = DynaParams()\n",
    "\n",
    "    # 50-step planning\n",
    "    dynaParams.planningSteps = 50\n",
    "\n",
    "    # average over 5 independent runs\n",
    "    dynaParams.runs = 5\n",
    "\n",
    "    # weight for elapsed time sine last visit\n",
    "    dynaParams.timeWeight = 1e-3\n",
    "\n",
    "    # also a tricky alpha ...\n",
    "    dynaParams.alpha = 0.7\n",
    "\n",
    "    # play\n",
    "    rewards = changingMaze(shortcutMaze, dynaParams)\n",
    "\n",
    "    plt.figure(2)\n",
    "    for i in range(0, len(dynaParams.methods)):\n",
    "        plt.plot(range(0, shortcutMaze.maxSteps), rewards[i, :], label=dynaParams.methods[i])\n",
    "    plt.xlabel('time steps')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    plt.legend()\n",
    "\n",
    "# Helper function to display best actions, just for debug\n",
    "def printActions(stateActionValues, maze):\n",
    "    bestActions = []\n",
    "    for i in range(0, maze.WORLD_HEIGHT):\n",
    "        bestActions.append([])\n",
    "        for j in range(0, maze.WORLD_WIDTH):\n",
    "            if [i, j] in maze.GOAL_STATES:\n",
    "                bestActions[-1].append('G')\n",
    "                continue\n",
    "            if [i, j] in maze.obstacles:\n",
    "                bestActions[-1].append('X')\n",
    "                continue\n",
    "            bestAction = np.argmax(stateActionValues[i, j, :])\n",
    "            if bestAction == maze.ACTION_UP:\n",
    "                bestActions[-1].append('U')\n",
    "            if bestAction == maze.ACTION_DOWN:\n",
    "                bestActions[-1].append('D')\n",
    "            if bestAction == maze.ACTION_LEFT:\n",
    "                bestActions[-1].append('L')\n",
    "            if bestAction == maze.ACTION_RIGHT:\n",
    "                bestActions[-1].append('R')\n",
    "    for row in bestActions:\n",
    "        print(row)\n",
    "    print('')\n",
    "\n",
    "# Check whether state-action values are already optimal\n",
    "def checkPath(stateActionValues, maze):\n",
    "    # get the length of optimal path\n",
    "    # 14 is the length of optimal path of the original maze\n",
    "    # 1.2 means it's a relaxed optifmal path\n",
    "    maxSteps = 14 * maze.resolution * 1.2\n",
    "    currentState = maze.START_STATE\n",
    "    steps = 0\n",
    "    while currentState not in maze.GOAL_STATES:\n",
    "        bestAction = np.argmax(stateActionValues[currentState[0], currentState[1], :])\n",
    "        currentState, _ = maze.takeAction(currentState, bestAction)\n",
    "        steps += 1\n",
    "        if steps > maxSteps:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Figure 8.7, mazes with different resolution\n",
    "def figure8_7():\n",
    "    # get the original 6 * 9 maze\n",
    "    originalMaze = Maze()\n",
    "\n",
    "    # set up the parameters for each algorithm\n",
    "    paramsDyna = DynaParams()\n",
    "    paramsDyna.planningSteps = 5\n",
    "    paramsDyna.alpha = 0.5\n",
    "    paramsDyna.gamma = 0.95\n",
    "\n",
    "    paramsPrioritized = DynaParams()\n",
    "    paramsPrioritized.theta = 0.0001\n",
    "    paramsPrioritized.planningSteps = 5\n",
    "    paramsPrioritized.alpha = 0.5\n",
    "    paramsPrioritized.gamma = 0.95\n",
    "\n",
    "    params = [paramsPrioritized, paramsDyna]\n",
    "\n",
    "    # set up models for planning\n",
    "    models = [PriorityModel, TrivialModel]\n",
    "    methodNames = ['Prioritized Sweeping', 'Dyna-Q']\n",
    "\n",
    "    # due to limitation of my machine, I can only perform experiments for 5 mazes\n",
    "    # say 1st maze has w * h states, then k-th maze has w * h * k * k states\n",
    "    numOfMazes = 5\n",
    "\n",
    "    # build all the mazes\n",
    "    mazes = [originalMaze.extendMaze(i) for i in range(1, numOfMazes + 1)]\n",
    "    methods = [prioritizedSweeping, dynaQ]\n",
    "\n",
    "    # track the # of backups\n",
    "    backups = np.zeros((2, numOfMazes))\n",
    "\n",
    "    # My machine cannot afford too many runs...\n",
    "    runs = 5\n",
    "    for run in range(0, runs):\n",
    "        for i in range(0, len(methodNames)):\n",
    "            for mazeIndex, maze in zip(range(0, len(mazes)), mazes):\n",
    "                print('run:', run, methodNames[i], 'maze size:', maze.WORLD_HEIGHT * maze.WORLD_WIDTH)\n",
    "\n",
    "                # initialize the state action values\n",
    "                currentStateActionValues = np.copy(maze.stateActionValues)\n",
    "\n",
    "                # track steps / backups for each episode\n",
    "                steps = []\n",
    "\n",
    "                # generate the model\n",
    "                model = models[i]()\n",
    "\n",
    "                # play for an episode\n",
    "                while True:\n",
    "                    steps.append(methods[i](currentStateActionValues, model, maze, params[i]))\n",
    "\n",
    "                    # print best actions w.r.t. current state-action values\n",
    "                    # printActions(currentStateActionValues, maze)\n",
    "\n",
    "                    # check whether the (relaxed) optimal path is found\n",
    "                    if checkPath(currentStateActionValues, maze):\n",
    "                        break\n",
    "\n",
    "                # update the total steps / backups for this maze\n",
    "                backups[i][mazeIndex] += np.sum(steps)\n",
    "\n",
    "    # Dyna-Q performs several backups per step\n",
    "    backups[1, :] *= paramsDyna.planningSteps\n",
    "\n",
    "    # average over independent runs\n",
    "    backups /= float(runs)\n",
    "\n",
    "    plt.figure(3)\n",
    "    for i in range(0, len(methodNames)):\n",
    "        plt.plot(np.arange(1, numOfMazes + 1), backups[i, :], label=methodNames[i])\n",
    "    plt.xlabel('maze resolution factor')\n",
    "    plt.ylabel('backups until optimal solution')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "figure8_3()\n",
    "figure8_5()\n",
    "figure8_6()\n",
    "figure8_7()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
