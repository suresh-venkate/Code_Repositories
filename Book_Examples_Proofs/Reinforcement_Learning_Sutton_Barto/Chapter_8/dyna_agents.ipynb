{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Maze class as defined in Example 8.1: Dyna Maze, Section 8.2 of RL_Sutton\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.WORLD_WIDTH = 9 # maze width\n",
    "        self.WORLD_HEIGHT = 6 # maze height\n",
    "\n",
    "        # all possible actions\n",
    "        self.ACTION_UP = 0\n",
    "        self.ACTION_DOWN = 1\n",
    "        self.ACTION_LEFT = 2\n",
    "        self.ACTION_RIGHT = 3\n",
    "        self.actions = [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_LEFT, self.ACTION_RIGHT]\n",
    "\n",
    "        self.START_STATE = [2, 0] # start state\n",
    "        self.GOAL_STATES = [[0, 8]] # goal state\n",
    "\n",
    "        # all obstacles\n",
    "        self.obstacles = [[1, 2], [2, 2], [3, 2], [0, 7], [1, 7], [2, 7], [4, 5]]\n",
    "#         self.oldObstacles = None\n",
    "#         self.newObstacles = None\n",
    "\n",
    "#         # time to change obstacles\n",
    "#         self.changingPoint = None\n",
    "\n",
    "        # initial state-action pair values\n",
    "        self.stateActionValues = np.zeros((self.WORLD_HEIGHT, self.WORLD_WIDTH, len(self.actions)))\n",
    "\n",
    "        # max steps\n",
    "        self.maxSteps = float('inf')\n",
    "\n",
    "#         # track the resolution for this maze\n",
    "#         self.resolution = 1\n",
    "\n",
    "    # take @action in @state\n",
    "    # @return: [new state, reward]\n",
    "    def takeAction(self, state, action):\n",
    "        x, y = state\n",
    "        if action == self.ACTION_UP:\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == self.ACTION_DOWN:\n",
    "            x = min(x + 1, self.WORLD_HEIGHT - 1)\n",
    "        elif action == self.ACTION_LEFT:\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == self.ACTION_RIGHT:\n",
    "            y = min(y + 1, self.WORLD_WIDTH - 1)\n",
    "        if [x, y] in self.obstacles:\n",
    "            x, y = state\n",
    "        if [x, y] in self.GOAL_STATES:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        return [x, y], reward\n",
    "    \n",
    "#     # extend a state to a higher resolution maze\n",
    "#     # @state: state in lower resoultion maze\n",
    "#     # @factor: extension factor, one state will become factor^2 states after extension\n",
    "#     def extendState(self, state, factor):\n",
    "#         newState = [state[0] * factor, state[1] * factor]\n",
    "#         newStates = []\n",
    "#         for i in range(0, factor):\n",
    "#             for j in range(0, factor):\n",
    "#                 newStates.append([newState[0] + i, newState[1] + j])\n",
    "#         return newStates\n",
    "\n",
    "#     # extend a state into higher resolution\n",
    "#     # one state in original maze will become @factor^2 states in @return new maze\n",
    "#     def extendMaze(self, factor):\n",
    "#         newMaze = Maze()\n",
    "#         newMaze.WORLD_WIDTH = self.WORLD_WIDTH * factor\n",
    "#         newMaze.WORLD_HEIGHT = self.WORLD_HEIGHT * factor\n",
    "#         newMaze.START_STATE = [self.START_STATE[0] * factor, self.START_STATE[1] * factor]\n",
    "#         newMaze.GOAL_STATES = self.extendState(self.GOAL_STATES[0], factor)\n",
    "#         newMaze.obstacles = []\n",
    "#         for state in self.obstacles:\n",
    "#             newMaze.obstacles.extend(self.extendState(state, factor))\n",
    "#         newMaze.stateActionValues = np.zeros((newMaze.WORLD_HEIGHT, newMaze.WORLD_WIDTH, len(newMaze.actions)))\n",
    "#         newMaze.resolution = factor\n",
    "#         return newMaze    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: DynaParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaParams:\n",
    "    \n",
    "    '''\n",
    "    Wrapper Class for parameters of dyna algorithms\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.gamma = 0.95 # discount\n",
    "        self.epsilon = 0.1 # probability for exploration\n",
    "        self.alpha = 0.1 # step size\n",
    "        self.planningSteps = 5 # n-step planning\n",
    "        self.runs = 10 # average over several independent runs\n",
    "        self.methods = ['Dyna-Q', 'Dyna-Q+'] # algorithm names\n",
    "        \n",
    "#         self.theta = 0 # threshold for priority queue\n",
    "#         self.timeWeight = 0 # weight for elapsed time     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: chooseAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseAction(state, stateActionValues, maze, dynaParams):\n",
    "    '''\n",
    "    # choose an action based on epsilon-greedy algorithm\n",
    "    '''\n",
    "    if np.random.binomial(1, dynaParams.epsilon) == 1:\n",
    "        return np.random.choice(maze.actions)\n",
    "    else:\n",
    "        values = stateActionValues[state[0], state[1], :]\n",
    "        return np.random.choice([action for action, value in enumerate(values) if value == np.max(values)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: TrivialModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrivialModel:\n",
    "    \n",
    "    '''\n",
    "    # Trivial model for planning in Dyna-Q\n",
    "    '''\n",
    "\n",
    "    # @rand: an instance of np.random.RandomState for sampling\n",
    "    def __init__(self, rand = np.random):\n",
    "        self.model = dict() # Initialize model as a dict\n",
    "        self.rand = rand # Initialize random state\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, currentState, action, newState, reward):\n",
    "        if tuple(currentState) not in self.model.keys():\n",
    "            self.model[tuple(currentState)] = dict()\n",
    "        self.model[tuple(currentState)][action] = [list(newState), reward]\n",
    "\n",
    "    # randomly sample from previous experience\n",
    "    def sample(self):\n",
    "        stateIndex = self.rand.choice(range(0, len(self.model.keys())))\n",
    "        state = list(self.model)[stateIndex]\n",
    "        actionIndex = self.rand.choice(range(0, len(self.model[state].keys())))\n",
    "        action = list(self.model[state])[actionIndex]\n",
    "        newState, reward = self.model[state][action]\n",
    "        return list(state), action, list(newState), reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class: dynaQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play for an episode for Dyna-Q algorithm\n",
    "# @stateActionValues: state action pair values, will be updated\n",
    "# @model: model instance for planning\n",
    "# @planningSteps: steps for planning\n",
    "# @maze: a maze instance containing all information about the environment\n",
    "# @dynaParams: several params for the algorithm\n",
    "def dynaQ(stateActionValues, model, maze, dynaParams):\n",
    "    currentState = maze.START_STATE\n",
    "    steps = 0\n",
    "    while currentState not in maze.GOAL_STATES: # Loop until goal state is reached\n",
    "        steps += 1 # Update steps taken till now\n",
    "        action = chooseAction(currentState, stateActionValues, maze, dynaParams) # Choose action based on eps-greedy policy\n",
    "        newState, reward = maze.takeAction(currentState, action) # take action\n",
    "\n",
    "        # Q-Learning update\n",
    "        stateActionValues[currentState[0], currentState[1], action] += \\\n",
    "            dynaParams.alpha * (reward + dynaParams.gamma * np.max(stateActionValues[newState[0], newState[1], :]) -\n",
    "            stateActionValues[currentState[0], currentState[1], action])\n",
    "\n",
    "        # feed the model with experience\n",
    "        model.feed(currentState, action, newState, reward)\n",
    "\n",
    "        # sample experience from the model\n",
    "        for t in range(0, dynaParams.planningSteps):\n",
    "            stateSample, actionSample, newStateSample, rewardSample = model.sample()\n",
    "            stateActionValues[stateSample[0], stateSample[1], actionSample] += \\\n",
    "                dynaParams.alpha * (rewardSample + dynaParams.gamma * np.max(stateActionValues[newStateSample[0],\n",
    "                newStateSample[1], :]) - stateActionValues[stateSample[0], stateSample[1], actionSample])\n",
    "               \n",
    "\n",
    "        currentState = newState # Update currentState\n",
    "\n",
    "        # check whether it has exceeded the step limit\n",
    "        if steps > maze.maxSteps:\n",
    "            break\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 8.1: Dyna Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [02:41<00:00,  5.37s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dynaMaze = Maze() # Instantiate Dyna Maze environment\n",
    "dynaParams = DynaParams() # Define parameters of Dyna algorithm\n",
    "\n",
    "planningSteps = [0, 5, 50] # Array of planning steps\n",
    "episodes = 50 # Number of episodes to run\n",
    "steps = np.zeros((len(planningSteps), episodes))\n",
    "runs = 30 # Number of repetitions of the experiment\n",
    "\n",
    "# this random seed is for sampling from model\n",
    "# we do need this separate random seed to make sure the first episodes for all planning steps are the same\n",
    "rand = np.random.RandomState(1)\n",
    "\n",
    "for run in tqdm(range(0, runs)):\n",
    "    for index, planningStep in enumerate(planningSteps):\n",
    "        dynaParams.planningSteps = planningStep # Set current value of planningStep\n",
    "        np.random.seed(run + 1) # set same random seed for each planning step\n",
    "        currentStateActionValues = np.copy(dynaMaze.stateActionValues) # Initialize state-action values\n",
    "        model = TrivialModel(rand) # generate an instance of Dyna-Q model\n",
    "        for ep in range(0, episodes): # Run dynaQ for multiple episodes\n",
    "            #print('run:', run, 'planning step:', planningStep, 'episode:', ep)\n",
    "            steps[index, ep] += dynaQ(currentStateActionValues, model, dynaMaze, dynaParams)\n",
    "\n",
    "steps /= runs # Averaged number of steps over runs\n",
    "\n",
    "plt.figure(0)\n",
    "for i in range(0, len(planningSteps)):\n",
    "    plt.plot(range(1, episodes), steps[i, 1:], label=str(planningSteps[i]) + ' planning steps')\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('steps per episode')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    episodes = 50\n",
    "    planningSteps = [0, 5, 50]\n",
    "    steps = np.zeros((len(planningSteps), episodes))\n",
    "\n",
    "    # this random seed is for sampling from model\n",
    "    # we do need this separate random seed to make sure the first episodes for all planning steps are the same\n",
    "    rand = np.random.RandomState(0)\n",
    "\n",
    "    for run in range(0, runs):\n",
    "        for index, planningStep in zip(range(0, len(planningSteps)), planningSteps):\n",
    "            dynaParams.planningSteps = planningStep\n",
    "\n",
    "            # set same random seed for each planning step\n",
    "            np.random.seed(run)\n",
    "\n",
    "            currentStateActionValues = np.copy(dynaMaze.stateActionValues)\n",
    "\n",
    "            # generate an instance of Dyna-Q model\n",
    "            model = TrivialModel(rand)\n",
    "            for ep in range(0, episodes):\n",
    "                print('run:', run, 'planning step:', planningStep, 'episode:', ep)\n",
    "                steps[index, ep] += dynaQ(currentStateActionValues, model, dynaMaze, dynaParams)\n",
    "\n",
    "    # averaging over runs\n",
    "    steps /= runs\n",
    "\n",
    "    plt.figure(0)\n",
    "    for i in range(0, len(planningSteps)):\n",
    "        plt.plot(range(0, episodes), steps[i, :], label=str(planningSteps[i]) + ' planning steps')\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('steps per episode')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze_Width:  9\n",
      "Maze_Height:  6\n",
      "Maze_Actions:  [0, 1, 2, 3]\n",
      "Maze_Start_State:  [2, 0]\n",
      "Maze_Goal_States:  [[0, 8]]\n",
      "Maze_obstacles:  [[1, 2], [2, 2], [3, 2], [0, 7], [1, 7], [2, 7], [4, 5]]\n",
      "Maze_stateActionValues_Shape:  (6, 9, 4)\n"
     ]
    }
   ],
   "source": [
    "dynaMaze = Maze()\n",
    "print(\"Maze_Width: \", dynaMaze.WORLD_WIDTH)\n",
    "print(\"Maze_Height: \", dynaMaze.WORLD_HEIGHT)\n",
    "print(\"Maze_Actions: \", dynaMaze.actions)\n",
    "print(\"Maze_Start_State: \", dynaMaze.START_STATE)\n",
    "print(\"Maze_Goal_States: \", dynaMaze.GOAL_STATES)\n",
    "print(\"Maze_obstacles: \", dynaMaze.obstacles)\n",
    "print(\"Maze_stateActionValues_Shape: \", dynaMaze.stateActionValues.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import heapq\n",
    "\n",
    "class PriorityQueue:\n",
    "    def __init__(self):\n",
    "        self.pq = []\n",
    "        self.entry_finder = {}\n",
    "        self.REMOVED = '<removed-task>'\n",
    "        self.counter = itertools.count()\n",
    "\n",
    "    def addItem(self, item, priority=0):\n",
    "        if item in self.entry_finder:\n",
    "            self.removeItem(item)\n",
    "        count = next(self.counter)\n",
    "        entry = [priority, count, item]\n",
    "        self.entry_finder[item] = entry\n",
    "        heapq.heappush(self.pq, entry)\n",
    "\n",
    "    def removeItem(self, item):\n",
    "        entry = self.entry_finder.pop(item)\n",
    "        entry[-1] = self.REMOVED\n",
    "\n",
    "    def popTask(self):\n",
    "        while self.pq:\n",
    "            priority, count, item = heapq.heappop(self.pq)\n",
    "            if item is not self.REMOVED:\n",
    "                del self.entry_finder[item]\n",
    "                return item, priority\n",
    "        raise KeyError('pop from an empty priority queue')\n",
    "\n",
    "    def empty(self):\n",
    "        return not self.entry_finder\n",
    "\n",
    "# choose an action based on epsilon-greedy algorithm\n",
    "def chooseAction(state, stateActionValues, maze, dynaParams):\n",
    "    if np.random.binomial(1, dynaParams.epsilon) == 1:\n",
    "        return np.random.choice(maze.actions)\n",
    "    else:\n",
    "        values = stateActionValues[state[0], state[1], :]\n",
    "        return np.random.choice([action for action, value in enumerate(values) if value == np.max(values)])\n",
    "\n",
    "# Time-based model for planning in Dyna-Q+\n",
    "class TimeModel:\n",
    "\n",
    "    # @maze: the maze instance. Indeed it's not very reasonable to give access to maze to the model.\n",
    "    # @timeWeight: also called kappa, the weight for elapsed time in sampling reward, it need to be small\n",
    "    # @rand: an instance of np.random.RandomState for sampling\n",
    "    def __init__(self, maze, timeWeight=1e-4, rand=np.random):\n",
    "        self.rand = rand\n",
    "        self.model = dict()\n",
    "\n",
    "        # track the total time\n",
    "        self.time = 0\n",
    "\n",
    "        self.timeWeight = timeWeight\n",
    "        self.maze = maze\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, currentState, action, newState, reward):\n",
    "        self.time += 1\n",
    "        if tuple(currentState) not in self.model.keys():\n",
    "            self.model[tuple(currentState)] = dict()\n",
    "\n",
    "            # Actions that had never been tried before from a state were allowed to be considered in the planning step\n",
    "            for action_ in self.maze.actions:\n",
    "                if action_ != action:\n",
    "                    # Such actions would lead back to the same state with a reward of zero\n",
    "                    # Notice that the minimum time stamp is 1 instead of 0\n",
    "                    self.model[tuple(currentState)][action_] = [list(currentState), 0, 1]\n",
    "\n",
    "        self.model[tuple(currentState)][action] = [list(newState), reward, self.time]\n",
    "\n",
    "    # randomly sample from previous experience\n",
    "    def sample(self):\n",
    "        stateIndex = self.rand.choice(range(0, len(self.model.keys())))\n",
    "        state = list(self.model)[stateIndex]\n",
    "        actionIndex = self.rand.choice(range(0, len(self.model[state].keys())))\n",
    "        action = list(self.model[state])[actionIndex]\n",
    "        newState, reward, time = self.model[state][action]\n",
    "\n",
    "        # adjust reward with elapsed time since last vist\n",
    "        reward += self.timeWeight * np.sqrt(self.time - time)\n",
    "\n",
    "        return list(state), action, list(newState), reward\n",
    "\n",
    "# Model containing a priority queue for Prioritized Sweeping\n",
    "class PriorityModel(TrivialModel):\n",
    "\n",
    "    def __init__(self, rand=np.random):\n",
    "        TrivialModel.__init__(self, rand)\n",
    "        # maintain a priority queue\n",
    "        self.priorityQueue = PriorityQueue()\n",
    "        # track predecessors for every state\n",
    "        self.predecessors = dict()\n",
    "\n",
    "    # add a @state-@action pair into the priority queue with priority @priority\n",
    "    def insert(self, priority, state, action):\n",
    "        # note the priority queue is a minimum heap, so we use -priority\n",
    "        self.priorityQueue.addItem((tuple(state), action), -priority)\n",
    "\n",
    "    # @return: whether the priority queue is empty\n",
    "    def empty(self):\n",
    "        return self.priorityQueue.empty()\n",
    "\n",
    "    # get the first item in the priority queue\n",
    "    def sample(self):\n",
    "        (state, action), priority = self.priorityQueue.popTask()\n",
    "        newState, reward = self.model[state][action]\n",
    "        return -priority, list(state), action, list(newState), reward\n",
    "\n",
    "    # feed the model with previous experience\n",
    "    def feed(self, currentState, action, newState, reward):\n",
    "        TrivialModel.feed(self, currentState, action, newState, reward)\n",
    "        if tuple(newState) not in self.predecessors.keys():\n",
    "            self.predecessors[tuple(newState)] = set()\n",
    "        self.predecessors[tuple(newState)].add((tuple(currentState), action))\n",
    "\n",
    "    # get all seen predecessors of a state @state\n",
    "    def predecessor(self, state):\n",
    "        if tuple(state) not in self.predecessors.keys():\n",
    "            return []\n",
    "        predecessors = []\n",
    "        for statePre, actionPre in list(self.predecessors[tuple(state)]):\n",
    "            predecessors.append([list(statePre), actionPre, self.model[statePre][actionPre][1]])\n",
    "        return predecessors\n",
    "\n",
    "\n",
    "# play for an episode for Dyna-Q algorithm\n",
    "# @stateActionValues: state action pair values, will be updated\n",
    "# @model: model instance for planning\n",
    "# @planningSteps: steps for planning\n",
    "# @maze: a maze instance containing all information about the environment\n",
    "# @dynaParams: several params for the algorithm\n",
    "def dynaQ(stateActionValues, model, maze, dynaParams):\n",
    "    currentState = maze.START_STATE\n",
    "    steps = 0\n",
    "    while currentState not in maze.GOAL_STATES:\n",
    "        # track the steps\n",
    "        steps += 1\n",
    "\n",
    "        # get action\n",
    "        action = chooseAction(currentState, stateActionValues, maze, dynaParams)\n",
    "\n",
    "        # take action\n",
    "        newState, reward = maze.takeAction(currentState, action)\n",
    "\n",
    "        # Q-Learning update\n",
    "        stateActionValues[currentState[0], currentState[1], action] += \\\n",
    "            dynaParams.alpha * (reward + dynaParams.gamma * np.max(stateActionValues[newState[0], newState[1], :]) -\n",
    "            stateActionValues[currentState[0], currentState[1], action])\n",
    "\n",
    "        # feed the model with experience\n",
    "        model.feed(currentState, action, newState, reward)\n",
    "\n",
    "        # sample experience from the model\n",
    "        for t in range(0, dynaParams.planningSteps):\n",
    "            stateSample, actionSample, newStateSample, rewardSample = model.sample()\n",
    "            stateActionValues[stateSample[0], stateSample[1], actionSample] += \\\n",
    "                dynaParams.alpha * (rewardSample + dynaParams.gamma * np.max(stateActionValues[newStateSample[0], newStateSample[1], :]) -\n",
    "                stateActionValues[stateSample[0], stateSample[1], actionSample])\n",
    "\n",
    "        currentState = newState\n",
    "\n",
    "        # check whether it has exceeded the step limit\n",
    "        if steps > maze.maxSteps:\n",
    "            break\n",
    "\n",
    "    return steps\n",
    "\n",
    "# play for an episode for prioritized sweeping algorithm\n",
    "# @stateActionValues: state action pair values, will be updated\n",
    "# @model: model instance for planning\n",
    "# @maze: a maze instance containing all information about the environment\n",
    "# @dynaParams: several params for the algorithm\n",
    "# @return: # of backups happened in planning phase in this episode\n",
    "def prioritizedSweeping(stateActionValues, model, maze, dynaParams):\n",
    "    currentState = maze.START_STATE\n",
    "\n",
    "    # track the steps in this episode\n",
    "    steps = 0\n",
    "\n",
    "    # track the backups in planning phase\n",
    "    backups = 0\n",
    "\n",
    "    while currentState not in maze.GOAL_STATES:\n",
    "        steps += 1\n",
    "\n",
    "        # get action\n",
    "        action = chooseAction(currentState, stateActionValues, maze, dynaParams)\n",
    "\n",
    "        # take action\n",
    "        newState, reward = maze.takeAction(currentState, action)\n",
    "\n",
    "        # feed the model with experience\n",
    "        model.feed(currentState, action, newState, reward)\n",
    "\n",
    "        # get the priority for current state action pair\n",
    "        priority = np.abs(reward + dynaParams.gamma * np.max(stateActionValues[newState[0], newState[1], :]) -\n",
    "                          stateActionValues[currentState[0], currentState[1], action])\n",
    "\n",
    "        if priority > dynaParams.theta:\n",
    "            model.insert(priority, currentState, action)\n",
    "\n",
    "        # start planning\n",
    "        planningStep = 0\n",
    "\n",
    "        # planning for several steps,\n",
    "        # although keep planning until the priority queue becomes empty will converge much faster\n",
    "        while planningStep < dynaParams.planningSteps and not model.empty():\n",
    "            # get a sample with highest priority from the model\n",
    "            priority, sampleState, sampleAction, sampleNewState, sampleReward = model.sample()\n",
    "\n",
    "            # update the state action value for the sample\n",
    "            delta = sampleReward + dynaParams.gamma * np.max(stateActionValues[sampleNewState[0], sampleNewState[1], :]) - \\\n",
    "                    stateActionValues[sampleState[0], sampleState[1], sampleAction]\n",
    "            stateActionValues[sampleState[0], sampleState[1], sampleAction] += dynaParams.alpha * delta\n",
    "\n",
    "            # deal with all the predecessors of the sample state\n",
    "            for statePre, actionPre, rewardPre in model.predecessor(sampleState):\n",
    "                priority = np.abs(rewardPre + dynaParams.gamma * np.max(stateActionValues[sampleState[0], sampleState[1], :]) -\n",
    "                                  stateActionValues[statePre[0], statePre[1], actionPre])\n",
    "                if priority > dynaParams.theta:\n",
    "                    model.insert(priority, statePre, actionPre)\n",
    "            planningStep += 1\n",
    "\n",
    "        currentState = newState\n",
    "\n",
    "        # update the # of backups\n",
    "        backups += planningStep\n",
    "\n",
    "    return backups\n",
    "\n",
    "# Figure 8.3, DynaMaze, use 10 runs instead of 30 runs\n",
    "def figure8_3():\n",
    "\n",
    "    # set up an instance for DynaMaze\n",
    "    dynaMaze = Maze()\n",
    "    dynaParams = DynaParams()\n",
    "\n",
    "    runs = 10\n",
    "    episodes = 50\n",
    "    planningSteps = [0, 5, 50]\n",
    "    steps = np.zeros((len(planningSteps), episodes))\n",
    "\n",
    "    # this random seed is for sampling from model\n",
    "    # we do need this separate random seed to make sure the first episodes for all planning steps are the same\n",
    "    rand = np.random.RandomState(0)\n",
    "\n",
    "    for run in range(0, runs):\n",
    "        for index, planningStep in zip(range(0, len(planningSteps)), planningSteps):\n",
    "            dynaParams.planningSteps = planningStep\n",
    "\n",
    "            # set same random seed for each planning step\n",
    "            np.random.seed(run)\n",
    "\n",
    "            currentStateActionValues = np.copy(dynaMaze.stateActionValues)\n",
    "\n",
    "            # generate an instance of Dyna-Q model\n",
    "            model = TrivialModel(rand)\n",
    "            for ep in range(0, episodes):\n",
    "                print('run:', run, 'planning step:', planningStep, 'episode:', ep)\n",
    "                steps[index, ep] += dynaQ(currentStateActionValues, model, dynaMaze, dynaParams)\n",
    "\n",
    "    # averaging over runs\n",
    "    steps /= runs\n",
    "\n",
    "    plt.figure(0)\n",
    "    for i in range(0, len(planningSteps)):\n",
    "        plt.plot(range(0, episodes), steps[i, :], label=str(planningSteps[i]) + ' planning steps')\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('steps per episode')\n",
    "    plt.legend()\n",
    "\n",
    "# wrapper function for changing maze\n",
    "# @maze: a maze instance\n",
    "# @dynaParams: several parameters for dyna algorithms\n",
    "def changingMaze(maze, dynaParams):\n",
    "\n",
    "    # set up max steps\n",
    "    maxSteps = maze.maxSteps\n",
    "\n",
    "    # track the cumulative rewards\n",
    "    rewards = np.zeros((2, maxSteps))\n",
    "\n",
    "    for run in range(0, dynaParams.runs):\n",
    "        # set up models\n",
    "        models = [TrivialModel(), TimeModel(maze, timeWeight=dynaParams.timeWeight)]\n",
    "\n",
    "        # track cumulative reward in current run\n",
    "        rewards_ = np.zeros((2, maxSteps))\n",
    "\n",
    "        # initialize state action values\n",
    "        stateActionValues = [np.copy(maze.stateActionValues), np.copy(maze.stateActionValues)]\n",
    "\n",
    "        for i in range(0, len(dynaParams.methods)):\n",
    "            print('run:', run, dynaParams.methods[i])\n",
    "\n",
    "            # set old obstacles for the maze\n",
    "            maze.obstacles = maze.oldObstacles\n",
    "\n",
    "            steps = 0\n",
    "            lastSteps = steps\n",
    "            while steps < maxSteps:\n",
    "                # play for an episode\n",
    "                steps += dynaQ(stateActionValues[i], models[i], maze, dynaParams)\n",
    "\n",
    "                # update cumulative rewards\n",
    "                steps_ = min(steps, maxSteps - 1)\n",
    "                rewards_[i, lastSteps: steps_] = rewards_[i, lastSteps]\n",
    "                rewards_[i, steps_] = rewards_[i, lastSteps] + 1\n",
    "                lastSteps = steps\n",
    "\n",
    "                if steps > maze.changingPoint:\n",
    "                    # change the obstacles\n",
    "                    maze.obstacles = maze.newObstacles\n",
    "        rewards += rewards_\n",
    "\n",
    "    # averaging over runs\n",
    "    rewards /= dynaParams.runs\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Figure 8.5, BlockingMaze\n",
    "def figure8_5():\n",
    "    # set up a blocking maze instance\n",
    "    blockingMaze = Maze()\n",
    "    blockingMaze.START_STATE = [5, 3]\n",
    "    blockingMaze.GOAL_STATES = [[0, 8]]\n",
    "    blockingMaze.oldObstacles = [[3, i] for i in range(0, 8)]\n",
    "\n",
    "    # new obstalces will block the optimal path\n",
    "    blockingMaze.newObstacles = [[3, i] for i in range(1, 9)]\n",
    "\n",
    "    # step limit\n",
    "    blockingMaze.maxSteps = 3000\n",
    "\n",
    "    # obstacles will change after 1000 steps\n",
    "    # the exact step for changing will be different\n",
    "    # However given that 1000 steps is long enough for both algorithms to converge,\n",
    "    # the difference is guaranteed to be very small\n",
    "    blockingMaze.changingPoint = 1000\n",
    "\n",
    "    # set up parameters\n",
    "    dynaParams = DynaParams()\n",
    "\n",
    "    # it's a tricky alpha ...\n",
    "    dynaParams.alpha = 0.7\n",
    "\n",
    "    # 5-step planning\n",
    "    dynaParams.planningSteps = 5\n",
    "\n",
    "    # average over 20 runs\n",
    "    dynaParams.runs = 20\n",
    "\n",
    "    # kappa must be small, as the reward for getting the goal is only 1\n",
    "    dynaParams.timeWeight = 1e-4\n",
    "\n",
    "    # play\n",
    "    rewards = changingMaze(blockingMaze, dynaParams)\n",
    "\n",
    "    plt.figure(1)\n",
    "    for i in range(0, len(dynaParams.methods)):\n",
    "        plt.plot(range(0, blockingMaze.maxSteps), rewards[i, :], label=dynaParams.methods[i])\n",
    "    plt.xlabel('time steps')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    plt.legend()\n",
    "\n",
    "# Figure 8.6, ShortcutMaze\n",
    "def figure8_6():\n",
    "    # set up a shortcut maze instance\n",
    "    shortcutMaze = Maze()\n",
    "    shortcutMaze.START_STATE = [5, 3]\n",
    "    shortcutMaze.GOAL_STATES = [[0, 8]]\n",
    "    shortcutMaze.oldObstacles = [[3, i] for i in range(1, 9)]\n",
    "\n",
    "    # new obstacles will have a shorter path\n",
    "    shortcutMaze.newObstacles = [[3, i] for i in range(1, 8)]\n",
    "\n",
    "    # step limit\n",
    "    shortcutMaze.maxSteps = 6000\n",
    "\n",
    "    # obstacles will change after 3000 steps\n",
    "    # the exact step for changing will be different\n",
    "    # However given that 3000 steps is long enough for both algorithms to converge,\n",
    "    # the difference is guaranteed to be very small\n",
    "    shortcutMaze.changingPoint = 3000\n",
    "\n",
    "    # set up parameters\n",
    "    dynaParams = DynaParams()\n",
    "\n",
    "    # 50-step planning\n",
    "    dynaParams.planningSteps = 50\n",
    "\n",
    "    # average over 5 independent runs\n",
    "    dynaParams.runs = 5\n",
    "\n",
    "    # weight for elapsed time sine last visit\n",
    "    dynaParams.timeWeight = 1e-3\n",
    "\n",
    "    # also a tricky alpha ...\n",
    "    dynaParams.alpha = 0.7\n",
    "\n",
    "    # play\n",
    "    rewards = changingMaze(shortcutMaze, dynaParams)\n",
    "\n",
    "    plt.figure(2)\n",
    "    for i in range(0, len(dynaParams.methods)):\n",
    "        plt.plot(range(0, shortcutMaze.maxSteps), rewards[i, :], label=dynaParams.methods[i])\n",
    "    plt.xlabel('time steps')\n",
    "    plt.ylabel('cumulative reward')\n",
    "    plt.legend()\n",
    "\n",
    "# Helper function to display best actions, just for debug\n",
    "def printActions(stateActionValues, maze):\n",
    "    bestActions = []\n",
    "    for i in range(0, maze.WORLD_HEIGHT):\n",
    "        bestActions.append([])\n",
    "        for j in range(0, maze.WORLD_WIDTH):\n",
    "            if [i, j] in maze.GOAL_STATES:\n",
    "                bestActions[-1].append('G')\n",
    "                continue\n",
    "            if [i, j] in maze.obstacles:\n",
    "                bestActions[-1].append('X')\n",
    "                continue\n",
    "            bestAction = np.argmax(stateActionValues[i, j, :])\n",
    "            if bestAction == maze.ACTION_UP:\n",
    "                bestActions[-1].append('U')\n",
    "            if bestAction == maze.ACTION_DOWN:\n",
    "                bestActions[-1].append('D')\n",
    "            if bestAction == maze.ACTION_LEFT:\n",
    "                bestActions[-1].append('L')\n",
    "            if bestAction == maze.ACTION_RIGHT:\n",
    "                bestActions[-1].append('R')\n",
    "    for row in bestActions:\n",
    "        print(row)\n",
    "    print('')\n",
    "\n",
    "# Check whether state-action values are already optimal\n",
    "def checkPath(stateActionValues, maze):\n",
    "    # get the length of optimal path\n",
    "    # 14 is the length of optimal path of the original maze\n",
    "    # 1.2 means it's a relaxed optifmal path\n",
    "    maxSteps = 14 * maze.resolution * 1.2\n",
    "    currentState = maze.START_STATE\n",
    "    steps = 0\n",
    "    while currentState not in maze.GOAL_STATES:\n",
    "        bestAction = np.argmax(stateActionValues[currentState[0], currentState[1], :])\n",
    "        currentState, _ = maze.takeAction(currentState, bestAction)\n",
    "        steps += 1\n",
    "        if steps > maxSteps:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Figure 8.7, mazes with different resolution\n",
    "def figure8_7():\n",
    "    # get the original 6 * 9 maze\n",
    "    originalMaze = Maze()\n",
    "\n",
    "    # set up the parameters for each algorithm\n",
    "    paramsDyna = DynaParams()\n",
    "    paramsDyna.planningSteps = 5\n",
    "    paramsDyna.alpha = 0.5\n",
    "    paramsDyna.gamma = 0.95\n",
    "\n",
    "    paramsPrioritized = DynaParams()\n",
    "    paramsPrioritized.theta = 0.0001\n",
    "    paramsPrioritized.planningSteps = 5\n",
    "    paramsPrioritized.alpha = 0.5\n",
    "    paramsPrioritized.gamma = 0.95\n",
    "\n",
    "    params = [paramsPrioritized, paramsDyna]\n",
    "\n",
    "    # set up models for planning\n",
    "    models = [PriorityModel, TrivialModel]\n",
    "    methodNames = ['Prioritized Sweeping', 'Dyna-Q']\n",
    "\n",
    "    # due to limitation of my machine, I can only perform experiments for 5 mazes\n",
    "    # say 1st maze has w * h states, then k-th maze has w * h * k * k states\n",
    "    numOfMazes = 5\n",
    "\n",
    "    # build all the mazes\n",
    "    mazes = [originalMaze.extendMaze(i) for i in range(1, numOfMazes + 1)]\n",
    "    methods = [prioritizedSweeping, dynaQ]\n",
    "\n",
    "    # track the # of backups\n",
    "    backups = np.zeros((2, numOfMazes))\n",
    "\n",
    "    # My machine cannot afford too many runs...\n",
    "    runs = 5\n",
    "    for run in range(0, runs):\n",
    "        for i in range(0, len(methodNames)):\n",
    "            for mazeIndex, maze in zip(range(0, len(mazes)), mazes):\n",
    "                print('run:', run, methodNames[i], 'maze size:', maze.WORLD_HEIGHT * maze.WORLD_WIDTH)\n",
    "\n",
    "                # initialize the state action values\n",
    "                currentStateActionValues = np.copy(maze.stateActionValues)\n",
    "\n",
    "                # track steps / backups for each episode\n",
    "                steps = []\n",
    "\n",
    "                # generate the model\n",
    "                model = models[i]()\n",
    "\n",
    "                # play for an episode\n",
    "                while True:\n",
    "                    steps.append(methods[i](currentStateActionValues, model, maze, params[i]))\n",
    "\n",
    "                    # print best actions w.r.t. current state-action values\n",
    "                    # printActions(currentStateActionValues, maze)\n",
    "\n",
    "                    # check whether the (relaxed) optimal path is found\n",
    "                    if checkPath(currentStateActionValues, maze):\n",
    "                        break\n",
    "\n",
    "                # update the total steps / backups for this maze\n",
    "                backups[i][mazeIndex] += np.sum(steps)\n",
    "\n",
    "    # Dyna-Q performs several backups per step\n",
    "    backups[1, :] *= paramsDyna.planningSteps\n",
    "\n",
    "    # average over independent runs\n",
    "    backups /= float(runs)\n",
    "\n",
    "    plt.figure(3)\n",
    "    for i in range(0, len(methodNames)):\n",
    "        plt.plot(np.arange(1, numOfMazes + 1), backups[i, :], label=methodNames[i])\n",
    "    plt.xlabel('maze resolution factor')\n",
    "    plt.ylabel('backups until optimal solution')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "\n",
    "figure8_3()\n",
    "figure8_5()\n",
    "figure8_6()\n",
    "figure8_7()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
