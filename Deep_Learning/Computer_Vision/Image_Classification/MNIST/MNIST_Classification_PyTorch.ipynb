{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Dataset_Eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "256px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suresh-venkate/Code_Repositories/blob/main/Deep_Learning/Computer_Vision/Image_Classification/MNIST/MNIST_Classification_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urg0J7uoSwE1"
      },
      "source": [
        "# MNIST Dataset Classification - PyTorch version\n",
        "\n",
        "**Author:** Suresh Venkatesan\n",
        "\n",
        "* Problem statement: Classify hand-written digits into their respective classes.\n",
        "* Dataset to be used: [MNIST](https://en.wikipedia.org/wiki/MNIST_database)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8x5K9y_BHgY"
      },
      "source": [
        "# Complete preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O0lJuOACzOb"
      },
      "source": [
        "## Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA_w3sQsdvoM"
      },
      "source": [
        "### Use this for Google Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U64TT4QGSwE3"
      },
      "source": [
        "## Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJvBuOZlIgRX"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import shutil\n",
        "import pytz\n",
        "import math\n",
        "import requests\n",
        "import pickle\n",
        "import gzip\n",
        "%matplotlib inline\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Ignore the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "#from torch.utils.data import DataLoader\n",
        "#from torchvision import datasets\n",
        "#from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "#from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP5VBxNPzlVH"
      },
      "source": [
        "## Define directory paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8awMYjozlVH"
      },
      "source": [
        "# Define base path for TensorBoard Logs directory\n",
        "tb_logs_base_path = \"/content/drive/MyDrive/AI_ML_Folder/Colab_Directory/Model_Outputs/MNIST_PyTorch/TB_Logs/\"\n",
        "os.makedirs(tb_logs_base_path, exist_ok = True) # Don't raise any exception if directory exists\n",
        "# Define base path for storing all outputs related to model / training\n",
        "out_base_path = \"/content/drive/MyDrive/AI_ML_Folder/Colab_Directory/Model_Outputs/MNIST_PyTorch/Training_Outputs/\"\n",
        "os.makedirs(out_base_path, exist_ok = True) # Don't raise any exception if directory exists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEus3VwROJH_"
      },
      "source": [
        "# Dataset - Import, EDA and pre-process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tr38D1dOLM-"
      },
      "source": [
        "## Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FioFzBK7OrKL"
      },
      "source": [
        "# Download mnist.pkl.gz and copy to /data/mnist folder\n",
        "\n",
        "data_path = Path(\"data\") / \"mnist\" # Define \"/data/mnist\" path as PosixPath object\n",
        "data_path.mkdir(parents = True, exist_ok = True) # Create missing parents of the path if required\n",
        "                                                 # # Don't raise any exception if directory exists\n",
        "mnist_url = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
        "mnist_fname = \"mnist.pkl.gz\"\n",
        "\n",
        "# Download zip file\n",
        "if not (data_path / mnist_fname).exists():\n",
        "  content = requests.get(mnist_url + mnist_fname).content\n",
        "  (data_path / mnist_fname).open(\"wb\").write(content)\n",
        "  print(\"mnist.pkl.gz created\")\n",
        "else:\n",
        "  print(\"mnist.pkl.gz already exists.\")\n",
        "print()\n",
        "\n",
        "# Unzip zip file and load pickle file contents\n",
        "with gzip.open((data_path / mnist_fname).as_posix(), \"rb\") as f:\n",
        "        ((X_train, y_train), (X_val, y_val), (X_test, y_test)) = pickle.load(f, encoding=\"latin-1\")\n",
        "\n",
        "print(f\"Type of X_train is {type(X_train)}\")\n",
        "print(f\"Type of X_val is {type(X_val)}\")\n",
        "print(f\"Type of X_test is {type(X_test)}\")\n",
        "print()\n",
        "print(f\"Data-Type of X_train is {X_train.dtype}\")\n",
        "print(f\"Data-Type of X_val is {X_val.dtype}\")\n",
        "print(f\"Data-Type of X_test is {X_test.dtype}\")\n",
        "print()\n",
        "print(f\"Shape of X_train is {X_train.shape}\")\n",
        "print(f\"Shape of X_val is {X_val.shape}\")\n",
        "print(f\"Shape of X_test is {X_test.shape}\")\n",
        "print()\n",
        "print(f\"Shape of y_train is {y_train.shape}\")\n",
        "print(f\"Shape of y_val is {y_val.shape}\")\n",
        "print(f\"Shape of y_test is {y_test.shape}\")   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UApn2GuBFZyD"
      },
      "source": [
        "## Reshape input image arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKHH_YBqFiov"
      },
      "source": [
        "X_train = X_train.reshape(-1, 28, 28)\n",
        "X_val = X_val.reshape(-1, 28, 28)\n",
        "X_test = X_test.reshape(-1, 28, 28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DGBy_UHGAWP"
      },
      "source": [
        "## Get information about train, val and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cCL_bmvGAWQ"
      },
      "source": [
        "print(\"Training_Set_Information:\")\n",
        "print(\"-------------------------\")\n",
        "print(\"Shape of training set data input is {}\" .format(X_train.shape))\n",
        "print(\"Number of training data samples is %d\" %(X_train.shape[0]))\n",
        "print(\"Shape of each training set sample is {}\" .format(X_train.shape[1:]))\n",
        "print(\"Data-Type of training set data input is {}\" .format(X_train.dtype))\n",
        "print(\"Shape of training set output is {}\" .format(y_train.shape))\n",
        "print()\n",
        "print(\"Validation_Set_Information:\")\n",
        "print(\"---------------------------\")\n",
        "print(\"Shape of validation set data input is {}\" .format(X_val.shape))\n",
        "print(\"Number of validation data samples is %d\" %(X_val.shape[0]))\n",
        "print(\"Shape of each validation set sample is {}\" .format(X_val.shape[1:]))\n",
        "print(\"Data-Type of validation set data input is {}\" .format(X_val.dtype))\n",
        "print(\"Shape of validation set output is {}\" .format(y_val.shape))\n",
        "print()\n",
        "print(\"Test_Set_Information:\")\n",
        "print(\"--------------------:\")\n",
        "print(\"Shape of test set data input is {}\" .format(X_test.shape))\n",
        "print(\"Number of test data samples is %d\" %(X_test.shape[0]))\n",
        "print(\"Shape of each test set sample is {}\" .format(X_test.shape[1:]))\n",
        "print(\"Data-Type of test set data input is {}\" .format(X_test.dtype))\n",
        "print(\"Shape of test set output is {}\" .format(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPwYbVEVzlVL"
      },
      "source": [
        "print(\"Minimum value of training data samples is %0.2f\" %(X_train.min()))\n",
        "print(\"Maximum value of training data samples is %0.2f\" %(X_train.max()))\n",
        "print()\n",
        "print(\"Minimum value of validation data samples is %0.2f\" %(X_val.min()))\n",
        "print(\"Maximum value of validation data samples is %0.2f\" %(X_val.max()))\n",
        "print()\n",
        "print(\"Minimum value of test data samples is %0.2f\" %(X_test.min()))\n",
        "print(\"Maximum value of training data samples is %0.2f\" %(X_test.max()))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEacLP2azlVL"
      },
      "source": [
        "<b>Above data indicates that the images are encoded as 28 x 28 grayscale images encoded in 0.0 to 1.0 floating point format.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boCnhuze5Vo8"
      },
      "source": [
        "## Get class distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG0RwemL5YpM"
      },
      "source": [
        "sns.set(color_codes = True)\n",
        "fig = plt.figure(figsize = (20, 8))\n",
        "fig.suptitle('Distribution of classes', fontsize = 30)\n",
        "\n",
        "ax1 = plt.subplot(1, 3, 1)\n",
        "sns.countplot(y_train)\n",
        "ax1.set_title('Training_Set', fontsize = 20)\n",
        "ax1.set_xlabel('Class Values', fontsize = 20)\n",
        "ax1.set_ylabel('Class Distribution', fontsize = 20)\n",
        "\n",
        "ax2 = plt.subplot(1, 3, 2)\n",
        "sns.countplot(y_val)\n",
        "ax2.set_title('Validation_Set', fontsize = 20)\n",
        "ax2.set_xlabel('Class Values', fontsize = 20)\n",
        "ax2.set_ylabel('Class Distribution', fontsize = 20)\n",
        "\n",
        "ax2 = plt.subplot(1, 3, 3)\n",
        "sns.countplot(y_test)\n",
        "ax2.set_title('Test_Set', fontsize = 20)\n",
        "ax2.set_xlabel('Class Values', fontsize = 20)\n",
        "ax2.set_ylabel('Class Distribution', fontsize = 20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmGjqK3JTDFp"
      },
      "source": [
        "# Collate class frequency counts into a dataframe\n",
        "class_dist_df = pd.DataFrame(np.bincount(y_train), columns=['Training_Set'])\n",
        "class_dist_df.index.name = 'Class_Label'\n",
        "class_dist_df['Val_Set'] = np.bincount(y_val)\n",
        "class_dist_df['Test_Set'] = np.bincount(y_test)\n",
        "\n",
        "display(class_dist_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44-5vtXe00mb"
      },
      "source": [
        "## Define Function: Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtu_--i302u-"
      },
      "source": [
        "def viz_data_from_array(name, X, y, X_dtype, mode, num_images, num_cols, col_size, row_size, bm_name = None):\n",
        "\n",
        "  '''\n",
        "  Function to plot random images from an input array along with corresponding labels\n",
        "\n",
        "  Arguments:\n",
        "    name: Name to print in title (Training_Set, Test_Set etc.)\n",
        "    X: Image array (should be in (batch, height, width, channel)) format\n",
        "    y: label array (Raw labels - should not be One-Hot encoded)\n",
        "    X_dtype: Data Type of image array. One of 'Int' or 'Float'\n",
        "    mode: One of 'grayscale' or 'color'\n",
        "    num_images: Number of images to plot from input array\n",
        "    num_cols: Number of columns to use for plotting\n",
        "    col_size: Size of columns to use for plotting\n",
        "    row_size: Size of rows to use for plotting\n",
        "    bm_name: Name of base model that will be used to undo pre-processing (if required)\n",
        "\n",
        "  '''\n",
        "\n",
        "  num_rows = math.ceil(num_images / num_cols) # Number of rows to use for plotting\n",
        "\n",
        "  fig = plt.figure(figsize = ((num_cols * col_size), (num_rows * row_size)))\n",
        "  fig.suptitle('Random sample images from ' + name, fontsize = 40)\n",
        "\n",
        "  # Generate random sample indices\n",
        "  samp_index = np.random.randint(low = 0, high = X.shape[0], size = num_images).tolist()\n",
        "\n",
        "  for ind, value in enumerate(samp_index): # Loop through samp_index\n",
        "    if (X_dtype == 'int'):\n",
        "      img = (X[value].squeeze()).astype('uint8') # Extract image and force type to uint8\n",
        "    elif (X_dtype == 'float') :\n",
        "      if (bm_name == None):\n",
        "        img = (X[value].squeeze()).astype('float32') # Extract image and force type to float32\n",
        "      else:\n",
        "        img = (X[value].squeeze()).astype('float32') # Extract image and force type to float32        \n",
        "        img = undo_preprocess_data(img, bm_name) # Undo any pre-processing done on image\n",
        "    label = y[value] # Extract label\n",
        "    ax = plt.subplot(num_rows, num_cols, (ind + 1))\n",
        "    if (mode == 'grayscale'):\n",
        "      ax.imshow(img, cmap = 'gray') # Plot image in grayscale\n",
        "    elif (mode == 'color'):\n",
        "      ax.imshow(img) # Plot image in color\n",
        "\n",
        "    ax.set_title(\"Class label is %d\" %label, fontsize = 25)\n",
        "    ax.grid(False)  \n",
        "\n",
        "  plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJywg4mN4joc"
      },
      "source": [
        "## Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnlDjYoNSwFU"
      },
      "source": [
        "viz_data_from_array('Training_Set', X_train, y_train, 'float', 'grayscale', num_images = 10, num_cols = 5,\\\n",
        "         col_size = 5, row_size = 6)\n",
        "print(\"\\n\\n\")\n",
        "viz_data_from_array('Validation_Set', X_val, y_val, 'float', 'grayscale', num_images = 10, num_cols = 5,\\\n",
        "         col_size = 5, row_size = 6)\n",
        "print(\"\\n\\n\")\n",
        "viz_data_from_array('Test_Set', X_test, y_test, 'float', 'grayscale', num_images = 10, num_cols = 5,\\\n",
        "         col_size = 5, row_size = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiPJ3Xx9wP6p"
      },
      "source": [
        "## Convert numpy arrays to tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PBT-QokwSHL"
      },
      "source": [
        "X_train, X_val, X_test = map(torch.tensor, (X_train, X_val, X_test))\n",
        "y_train, y_val, y_test = map(torch.tensor, (y_train, y_val, y_test)) \n",
        "print(f\"Type of X_train is {type(X_train)}\")\n",
        "print(f\"Type of X_val is {type(X_val)}\")\n",
        "print(f\"Type of X_test is {type(X_test)}\")\n",
        "print()\n",
        "print(f\"Data-Type of X_train is {X_train.dtype}\")\n",
        "print(f\"Data-Type of X_val is {X_val.dtype}\")\n",
        "print(f\"Data-Type of X_test is {X_test.dtype}\")\n",
        "print()\n",
        "print(f\"Shape of X_train is {X_train.shape}\")\n",
        "print(f\"Shape of X_val is {X_val.shape}\")\n",
        "print(f\"Shape of X_test is {X_test.shape}\")\n",
        "print()\n",
        "print(f\"Shape of y_train is {y_train.shape}\")\n",
        "print(f\"Shape of y_val is {y_val.shape}\")\n",
        "print(f\"Shape of y_test is {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS6xzSVYWIse"
      },
      "source": [
        "# Backup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAzj--3GNPv0"
      },
      "source": [
        "## NN from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O8s9mo3NYSV"
      },
      "source": [
        "# Define weights and bias \n",
        "weights = torch.randn(784, 10) / math.sqrt(784)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWTM1yJ5OEjf"
      },
      "source": [
        "def log_softmax(x):\n",
        "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "def model(xb):\n",
        "    return log_softmax(xb @ weights + bias)   \n",
        "def loss_func(input, target):\n",
        "    return -input[range(target.shape[0]), target].mean() \n",
        "def accuracy(out, yb):\n",
        "    preds = torch.argmax(out, dim = 1)\n",
        "    return (preds == yb).float().mean()     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGy14mrhQ8wX"
      },
      "source": [
        "bat_size = 64\n",
        "X_batch = X_train[0:bat_size]\n",
        "y_batch = y_train[0:bat_size]\n",
        "pred_batch = model(X_batch)\n",
        "\n",
        "print(loss_func(pred_batch, y_batch))\n",
        "print(accuracy(pred_batch, y_batch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U_Btr9rTQ68"
      },
      "source": [
        "n, c = X_train.shape\n",
        "print(n, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RobDXmQRUCa"
      },
      "source": [
        "lr = 0.5  # learning rate\n",
        "epochs = 2  # how many epochs to train for\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range((n - 1) // bat_size + 1):\n",
        "        #set_trace()\n",
        "        start_i = i * bat_size\n",
        "        end_i = start_i + bat_size\n",
        "        xb = X_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        #print(xb.shape, yb.shape)\n",
        "        pred = model(xb)\n",
        "        #print(pred.shape)\n",
        "        loss = loss_func(pred, yb)\n",
        "        #print(loss)\n",
        "        loss.backward()\n",
        "        #set_trace()\n",
        "        acc = 100 * accuracy(pred, yb)\n",
        "        print(f\"Epoch: {epoch}, Batch: {i}, Loss:{loss:0.4f}, Accuracy: {acc:0.2f}\")\n",
        "        with torch.no_grad():\n",
        "            weights -= weights.grad * lr\n",
        "            bias -= bias.grad * lr\n",
        "            weights.grad.zero_()\n",
        "            bias.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaWxa8r8X98V"
      },
      "source": [
        "## Using torch.nn.functionalm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTXIiO-UYp5j"
      },
      "source": [
        "loss_func = F.cross_entropy\n",
        "\n",
        "def model(xb):\n",
        "    return xb @ weights + bias\n",
        "\n",
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z25Cm4vg_RG3"
      },
      "source": [
        "<b>Class distribution is reasonably uniform for all three sets.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA7zX1kgSwFJ"
      },
      "source": [
        "## One-hot encode training, validation and testing labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMVE2h6BSwFO"
      },
      "source": [
        "num_classes = 10 # Digits 0 to 9\n",
        "y_train_ohe = to_categorical(y_train, num_classes = num_classes)\n",
        "y_val_ohe = to_categorical(y_val, num_classes = num_classes)\n",
        "y_test_ohe = to_categorical(y_test, num_classes = num_classes)\n",
        "print(\"Shape of one-hot encoded training set output is {}\" .format(y_train_ohe.shape))\n",
        "print(\"Shape of one-hot encoded validation set output is {}\" .format(y_val_ohe.shape))\n",
        "print(\"Shape of one-hot encoded test set output is {}\" .format(y_test_ohe.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCKDWMaqZqbA"
      },
      "source": [
        "# Define Image Data Generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpU3pa_gQhxR"
      },
      "source": [
        "## Define Function: plot_img_transf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J06bmCdZQl4j"
      },
      "source": [
        "def plot_img_transf(X, X_dtype, mode, rot, wid_shift, hgt_shift, shear, zx, zy,\\\n",
        "                    num_cols, col_size, row_size):\n",
        "  \n",
        "  '''\n",
        "  Function to plot various image transformations on an image before applying these transformations\n",
        "  for data augmentation\n",
        "\n",
        "  Arguments:\n",
        "    X: Image array (should be in (batch, height, width, channel)) format\n",
        "    X_dtype: Data Type of image array. One of 'Int' or 'Float'\n",
        "    mode: One of 'grayscale' or 'color'\n",
        "    rot: Extent of clockwise-rotation in degrees\n",
        "    wid_shift: Extent of wid_shift\n",
        "    hgt_shift: Extent of height shift\n",
        "    shear: Shear angle in degrees\n",
        "    zx: Extent of zoom in x-direction\n",
        "    zy: Extent of zoom in y-direction\n",
        "    num_cols: Number of columns to use for plotting\n",
        "    col_size: Size of columns to use for plotting\n",
        "    row_size: Size of rows to use for plotting\n",
        "\n",
        "  '''\n",
        "\n",
        "  # Generate random sample index\n",
        "  samp_index = np.random.randint(low = 0, high = X.shape[0], size = 1)[0] \n",
        "\n",
        "  img_transf = {} # Dict place-holder to store original and transformed images\n",
        "  img = X[samp_index] # Extract original image\n",
        "  img_transf['Original_Image'] = img # Append original image to img_transf dict\n",
        "\n",
        "  img_transf['Clockwise_Rotation'] = apply_affine_transform(img, theta = rot) # Clockwise rotn\n",
        "  img_transf['Anti-clockwise Rotation'] = apply_affine_transform(img, theta = -rot) # Anti-clockwise rotn\n",
        "  img_transf['Left_Shift'] = apply_affine_transform(img, ty = wid_shift) # Left Shift\n",
        "  img_transf['Right_Shift'] = apply_affine_transform(img, ty = -wid_shift) # Right Shift\n",
        "  img_transf['Upward_Shift'] = apply_affine_transform(img, tx = hgt_shift) # Upward Shift\n",
        "  img_transf['Downward_Shift'] = apply_affine_transform(img, tx = -hgt_shift) # Downward Shift\n",
        "  img_transf['Shear_Left'] = apply_affine_transform(img, shear = shear) # Left shear\n",
        "  img_transf['Shear_Right'] = apply_affine_transform(img, shear = -shear) # Righ shear  \n",
        "  img_transf['Zoom_Y'] = apply_affine_transform(img, zx = zx) # Zoom in x-direction  \n",
        "  img_transf['Zoom_X'] = apply_affine_transform(img, zy = zy) # Zoom in y-direction    \n",
        "\n",
        "  num_images = len(img_transf) # Total number of images to plot\n",
        "  num_rows = math.ceil(num_images / num_cols) # Number of rows to use for plotting\n",
        "  \n",
        "  fig = plt.figure(figsize = ((num_cols * col_size), (num_rows * row_size)))\n",
        "  fig.suptitle('Original image along with some affine transformations', fontsize = 40)\n",
        "\n",
        "  for ind, dict_entry in enumerate(img_transf.items()): # Loop through dictionary items\n",
        "    key, image = dict_entry[0], dict_entry[1]\n",
        "    if (X_dtype == 'int'):\n",
        "      img = (image.squeeze()).astype('uint8') # Extract image and force type to uint8\n",
        "    elif (X_dtype == 'float') :\n",
        "      img = (image.squeeze()).astype('float32') # Extract image and force type to float32\n",
        "    ax = plt.subplot(num_rows, num_cols, (ind + 1))\n",
        "    if (mode == 'grayscale'):\n",
        "      ax.imshow(img, cmap = 'gray') # Plot image in grayscale\n",
        "    elif (mode == 'color'):\n",
        "      ax.imshow(img) # Plot image in color\n",
        "    ax.set_title(key, fontsize = 25) # Set key as title\n",
        "    #ax.grid(False) # Turn off grid\n",
        "\n",
        "  plt.show()    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUaLpjWJ4lVC"
      },
      "source": [
        "## Visualize some image transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf-bKsQnZTsT"
      },
      "source": [
        "plot_img_transf(X_train, 'int', 'grayscale', rot = 30, wid_shift = 5, hgt_shift = 5, shear = 30,\\\n",
        "                zx = 0.8, zy = 0.8, num_cols = 5, col_size = 5, row_size = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXYYTbHxLYOk"
      },
      "source": [
        "## Define base model, target image shape and model batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cYnxiHJLaMb"
      },
      "source": [
        "### The base model defined here (bm_name) will be used to choose the appropriate pre-processing function\n",
        "### to apply to the input images in the Image Data Generators\n",
        "\n",
        "### If bm_name = 'VGG16' or 'ResNet50':\n",
        "###    Only mean shift is applied\n",
        "###    img_preprocessed = img - [103.939, 116.779, 123.68]\n",
        "### If bm_name = 'MobileNet' or 'InceptionV3'\n",
        "###    Image scaled to lie between -1 and +1\n",
        "###    img_preprocessed = (img / 127.5) - 1\n",
        "### If bm_name = 'grayscale_model'\n",
        "###    Image scaled to lie between 0 and +1\n",
        "###    img_preprocessed = (img / 225.)\n",
        "\n",
        "# bm_name = 'VGG16'\n",
        "# bm_name = 'ResNet50'\n",
        "# bm_name = 'MobileNet'\n",
        "# bm_name = 'InceptionV3'\\\n",
        "bm_name = 'grayscale_model'\n",
        "\n",
        "# Define target image size and batch size \n",
        "mod_inp_shape = (28, 28, 1) # Define target image size for model input\n",
        "mod_bat_size = 64 # Batch size to use while model fitting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM-gStAoK1ZD"
      },
      "source": [
        "## Define pre-processing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4dSOfKaLepq"
      },
      "source": [
        "# Define pre-processing and undo pre-processing function\n",
        "# Appropriate pre-processing functions from Keras are used.\n",
        "\n",
        "def preprocess_data(img):\n",
        "\n",
        "  if (bm_name == 'VGG16'):\n",
        "    return vgg16.preprocess_input(img)\n",
        "  elif (bm_name == 'ResNet50'):\n",
        "    return resnet50.preprocess_input(img)\n",
        "  elif (bm_name == 'MobileNet'):\n",
        "    return mobilenet.preprocess_input(img)\n",
        "  elif (bm_name == 'InceptionV3'):\n",
        "    return inception_v3.preprocess_input(img)\n",
        "  elif (bm_name == 'grayscale_model'):\n",
        "    return img/255.\n",
        "\n",
        "def undo_preprocess_data(img, bm_name):\n",
        "\n",
        "  if ((bm_name == 'VGG16') or (bm_name == 'ResNet50')):\n",
        "    mean = [103.939, 116.779, 123.68]\n",
        "    img[..., 0] += mean[0]\n",
        "    img[..., 1] += mean[1]\n",
        "    img[..., 2] += mean[2]\n",
        "    img = img[..., ::-1].astype('uint8')\n",
        "  elif ((bm_name == 'MobileNet') or (bm_name == 'InceptionV3')):\n",
        "    img = ((img + 1) * 127.5).astype('uint8')\n",
        "  elif (bm_name == 'grayscale_model'):\n",
        "    img = (img * 255.0).astype('uint8')\n",
        "\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUtkaDMwUVel"
      },
      "source": [
        "## Define Image Data Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67xmE0koq0Lm"
      },
      "source": [
        "# Define train, val and test ImageDataGenerator objects\n",
        "# Train ImageDataGenerator includes image augmentation\n",
        "# Val and Test ImageDataGenerators don't have any image augmentation\n",
        "\n",
        "train_datagen = ImageDataGenerator(rotation_range = 30, horizontal_flip = False, width_shift_range = 5,\\\n",
        "                                   height_shift_range = 5, shear_range = 20, zoom_range = 0.2,\\\n",
        "                                   channel_shift_range = 0.0, preprocessing_function = preprocess_data)\n",
        "val_datagen = ImageDataGenerator(preprocessing_function = preprocess_data)\n",
        "test_datagen = ImageDataGenerator(preprocessing_function = preprocess_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bi2-xvGYCLR"
      },
      "source": [
        "## Define Generator Objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um1rq2ZvYE0y"
      },
      "source": [
        "# Define train, val and test generator objects\n",
        "train_generator = train_datagen.flow(X_train, y_train_ohe, batch_size = mod_bat_size,\\\n",
        "                                     shuffle = True, seed = 1234)\n",
        "val_generator = val_datagen.flow(X_val, y_val_ohe, batch_size = mod_bat_size,\\\n",
        "                                 shuffle = True, seed = 1234)\n",
        "test_generator = test_datagen.flow(X_test, y_test_ohe, batch_size = mod_bat_size,\\\n",
        "                                   shuffle = True, seed = 1234)\n",
        "\n",
        "print(\"Number of batches in train_generator is %d\" %(len(train_generator)))\n",
        "print(\"Number of batches in val_generator is %d\" %(len(val_generator)))\n",
        "print(\"Number of batches in test_generator is %d\" %(len(test_generator)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNx9acQy_bV_"
      },
      "source": [
        "## Verify generator objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRiBUxqr_b8S"
      },
      "source": [
        "X_batch, y_batch = next(train_generator)\n",
        "print(f\"Shape of X_batch is {X_batch.shape}\")\n",
        "print(f\"Shape of y_batch is {y_batch.shape}\")\n",
        "print(f\"Minimum value of X_batch is {X_batch.min()}\")\n",
        "print(f\"Maximum value of X_batch is {X_batch.max()}\")\n",
        "print(f\"Data_Type of X_batch is {X_batch.dtype}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RdCo1kgVBPq"
      },
      "source": [
        "## Visualize the data from generator objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmwy-DEYA99X"
      },
      "source": [
        "# Generate a batch of data from train_generator\n",
        "X_batch, y_batch = next(train_generator)\n",
        "y_batch = np.argmax(y_batch, axis = 1)\n",
        "viz_data('Train_Generator', X_batch, y_batch, 'float', 'grayscale', num_images = 10, num_cols = 5,\\\n",
        "         col_size = 5, row_size = 6, bm_name = bm_name)\n",
        "print(\"\\n\\n\")\n",
        "# Generate a batch of data from validation_generator\n",
        "X_batch, y_batch = next(val_generator)\n",
        "y_batch = np.argmax(y_batch, axis = 1)\n",
        "viz_data('Val_Generator', X_batch, y_batch, 'float', 'grayscale', num_images = 10, num_cols = 5,\\\n",
        "         col_size = 5, row_size = 6, bm_name = bm_name)\n",
        "print(\"\\n\\n\")\n",
        "# Generate a batch of data from test_generator\n",
        "X_batch, y_batch = next(test_generator)\n",
        "y_batch = np.argmax(y_batch, axis = 1)\n",
        "viz_data('Test_Generator', X_batch, y_batch, 'float', 'grayscale', num_images = 10, num_cols = 5,\\\n",
        "         col_size = 5, row_size = 6, bm_name = bm_name)\n",
        "print(\"\\n\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzjNXDrrbSAh"
      },
      "source": [
        "# Models - Build, Train, Tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP4SXajyoTkW"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwXZtqIEpBbq"
      },
      "source": [
        "### Define Model Core"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CowJ1IHpGKz"
      },
      "source": [
        "def model_core(model_dict):\n",
        "    \n",
        "    \"\"\"\n",
        "    Function to define the model core.\n",
        "    \n",
        "    Arguments:\n",
        "      model_dict - Dictionary with list of keys / values needed to build the model\n",
        "\n",
        "    Returns:\n",
        "      model - Model with all layers instantiated\n",
        " \n",
        "    \"\"\"    \n",
        "    # Retrieve model dict parameters\n",
        "    model_arch = model_dict['model_arch'] # Model Architecture\n",
        "    use_bnorm = model_dict['use_bnorm'] # Boolean, whether to use Batch Norm or not\n",
        "    use_dropout = model_dict['use_dropout'] # Boolean, whether to use dropout or not\n",
        "    dropout_rate = model_dict['dropout_rate'] # List containing dropout values for each layer\n",
        "\n",
        "    ##### Start Model Architecture A\n",
        "    if (model_arch == 'A'): \n",
        "      model = Sequential()\n",
        "\n",
        "      # Stage-1: Conv2D -> BN -> ReLU -> Dropout -> MaxPool2D ->\n",
        "      # -> Conv2D -> BN -> ReLU -> Dropout -> MaxPool2D\n",
        "      model.add(Conv2D(32, (3, 3), input_shape = (28, 28, 1), padding = 'same', name = 'C1'))\n",
        "      if (use_bnorm): model.add(BatchNormalization(name = 'B1'))\n",
        "      model.add(Activation('relu', name = 'A1'))\n",
        "      if (use_dropout): model.add(Dropout(rate = dropout_rate[0], name = 'DR1')) \n",
        "      model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), name = 'M1'))      \n",
        "\n",
        "      model.add(Conv2D(32, (3, 3), padding = 'same', name = 'C2'))\n",
        "      if (use_bnorm): model.add(BatchNormalization(name = 'B2'))\n",
        "      model.add(Activation('relu', name = 'A2'))\n",
        "      if (use_dropout): model.add(Dropout(rate = dropout_rate[1], name = 'DR2'))\n",
        "      model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2), name = 'M2'))\n",
        "      \n",
        "      # Stage-2: Flatten -> Dense(128, ReLU) -> Dense(64,ReLU) -> Dense(10, Softmax)\n",
        "      model.add(Flatten(name = 'F1'))\n",
        "      model.add(Dense(128, activation = 'relu', name = \"D1\"))\n",
        "      model.add(Dense(64, activation = 'relu', name = \"D2\"))      \n",
        "      model.add(Dense(10, activation = 'softmax', name = \"OL\"))\n",
        "  \n",
        "    ##### End Model Architecture A     \n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93zleWqKvQ0q"
      },
      "source": [
        "### Verify Model Core"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zQilif0vXsm"
      },
      "source": [
        "# Verify model defined above\n",
        "model_dict = {'model_arch': 'A',\n",
        "              'use_bnorm': True,\n",
        "              'use_dropout': True, \n",
        "              'dropout_rate': [0.0, 0.0]\n",
        "              }\n",
        "temp_model = model_core(model_dict)\n",
        "temp_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBITB1rJIlsB"
      },
      "source": [
        "### Define Model_Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pusIvvIKIzNC"
      },
      "source": [
        "def model_compile(model, compile_dict):\n",
        "    \n",
        "    \"\"\"\n",
        "    Function to compile the model\n",
        "    \n",
        "    Arguments:\n",
        "      model - Model instance that needs to be compiled\n",
        "      compile_dict - Dictionary with list of keys / values needed to compile the model\n",
        " \n",
        "    \"\"\"    \n",
        "    # Retrieve compile_dict parameters\n",
        "    ilr = compile_dict['ilr'] # Initial learning rate to use for learning rate decay scheduler    \n",
        "    dr = compile_dict['dr'] # Decay rate to use for learning rate decay scheduler    \n",
        "    ds = compile_dict['ds'] # Decay step to use for learning rate decay scheduler\n",
        "    redlr_plat = compile_dict['redlr_plat'] # Boolean: If True, implement reduce LR on plateau     \n",
        "\n",
        "    lr_sch = InverseTimeDecay(ilr, ds, dr) # Inverse Time Decay LR scheduler\n",
        "    # Define Optimizer\n",
        "    if (redlr_plat):\n",
        "      opt = optimizers.Adam(learning_rate = ilr) \n",
        "    else:\n",
        "      opt = optimizers.Adam(learning_rate = lr_sch)\n",
        "    loss = losses.CategoricalCrossentropy() # Define loss\n",
        "    met = metrics.CategoricalAccuracy() # Define metric\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer = opt, loss = loss, metrics = met)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tAuBzjiI125"
      },
      "source": [
        "### Define Model_Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6dVbZawL7fm"
      },
      "source": [
        "def model_fit(model, train_dict):\n",
        "    \n",
        "    \"\"\"\n",
        "    Function to fit the model\n",
        "    \n",
        "    Arguments:\n",
        "      model - Model instance that needs to be trained\n",
        "      train_dict - Dictionary with list of keys / values needed to fit the model      \n",
        "\n",
        "    Returns:\n",
        "      model - Final trained model\n",
        "      hist - Model training history\n",
        "    \"\"\"   \n",
        "\n",
        "    # Retrieve path parameters\n",
        "    tb_path = train_dict['tb_path'] # Path to store Tensorboard callback information\n",
        "    mc_path = train_dict['mc_path'] # File name to use for storing model checkpoints\n",
        "    \n",
        "    # Retrieve callback parameters\n",
        "    mcp_freq = train_dict['mcp_freq'] # Number of batches after which model will be checkpointed    \n",
        "    early_stop = train_dict['early_stop'] # Boolean: If True, implement early stop\n",
        "    redlr_plat = train_dict['redlr_plat'] # Boolean: If True, implement reduce LR on plateau  \n",
        "    lrpl_fac = train_dict['lrpl_fac'] # Factor to use for Reduce LR on Plateau callback\n",
        "    lrpl_pat = train_dict['lrpl_pat'] # Patience to use for Reduce LR on Plateau callback    \n",
        "\n",
        "    # Retrieve training parameters\n",
        "    train_gen = train_dict['train_gen'] # Train Generator to use while fitting\n",
        "    val_data = train_dict['val_gen'] # Validation Generator to use while fitting\n",
        "    epochs = train_dict['epochs'] # Number of epochs to train for\n",
        "    initial_epoch = train_dict['initial_epoch'] # Initial epoch to re-start training from\n",
        "    train_steps_per_epoch = train_dict['train_steps_per_epoch'] # Number of steps per training epoch\n",
        "    val_steps = train_dict['val_steps'] # Number of steps before stopping validation\n",
        "    val_freq = train_dict['val_freq'] # Number of epochs to run before performing a validation run\n",
        "    verb = train_dict['verb'] # Controls verbosity level of model fit.\n",
        "\n",
        "    #### Start -  Define callbacks\n",
        "    # Define path for tensorboard logs\n",
        "    logdir = os.path.join(tb_path,\\\n",
        "                          datetime.datetime.now(pytz.timezone('Asia/Kolkata')).strftime(\"%d%m_%H%M\"))\n",
        "    # Define Tensorboard callback\n",
        "    tensorboard_callback = TensorBoard(logdir, histogram_freq = 0)\n",
        "    # Define Model Checkpoint callback\n",
        "    mcp_callback = ModelCheckpoint(filepath = mc_path, save_freq = mcp_freq, verbose = 0)\n",
        "    # Define Early Stopping callback\n",
        "    earlystopping_callback = EarlyStopping(monitor = \"loss\", min_delta = 1e-4, patience = 10,\\\n",
        "                                           mode = \"min\", verbose = 1)\n",
        "    # Define 'Reduce learning rate on plateau' callback\n",
        "    redlr_plat_callback = ReduceLROnPlateau(monitor = \"val_loss\", factor = lrpl_fac, patience = lrpl_pat,\\\n",
        "                                            verbose = 1, mode = \"min\", min_delta = 0.0001)\n",
        "    # Define list of all callbacks\n",
        "    callback_list = []\n",
        "    if (tb_path != None): callback_list.append(tensorboard_callback)\n",
        "    if (mc_path != None): callback_list.append(mcp_callback)\n",
        "    if (early_stop): callback_list.append(earlystopping_callback)    \n",
        "    if (redlr_plat): callback_list.append(redlr_plat_callback)\n",
        "    #### End -  Define callbacks \n",
        "    \n",
        "    #### Start - Model Fit\n",
        "    hist = model.fit(x = train_gen, validation_data = val_data, epochs = epochs,\\\n",
        "                     initial_epoch = initial_epoch, steps_per_epoch = train_steps_per_epoch,\\\n",
        "                     validation_steps = val_steps, validation_freq = val_freq,\n",
        "                     callbacks = callback_list, verbose = verb)\n",
        "    #### End - Model Fit   \n",
        "\n",
        "    return model, hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Kvqg5fL9QZ"
      },
      "source": [
        "### Define Model_Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68U0fp7zMBEY"
      },
      "source": [
        "def model_train(model_dict, compile_dict, train_dict, model = None):\n",
        "  \"\"\"\n",
        "  Function to instantiate (or load) model, compile and fit model.\n",
        "    \n",
        "  Arguments:\n",
        "    model_dict - Dictionary with list of keys / values needed to build the model\n",
        "    compile_dict - Dictionary with list of keys / values needed to compile the model\n",
        "    train_dict - Dictionary with list of keys / values needed to train the model\n",
        "    model - Pre-trained model (Pass this as input only if fit_resume = True and load_model = False)\n",
        "    \n",
        "  Returns:\n",
        "    model - Final trained model\n",
        "    hist - Model training history\n",
        " \n",
        "  \"\"\"   \n",
        "  # Retrieve train_dict parameters\n",
        "  fit_resume = train_dict['fit_resume'] # Boolean: If True, resume fit from initial epoch\n",
        "  load_model = train_dict['load_model'] # Boolean: If True, load model from 'fm_path' and resume fit  \n",
        "  recompile = train_dict['recompile'] # Boolean: If True, recompile model before resuming fit  \n",
        "  fm_path = train_dict['fm_path'] # File name to use for storing final trained model\n",
        "  hi_path = train_dict['hi_path'] # File name to use for storing training history\n",
        "\n",
        "  if (not(fit_resume)): # fit_resume = False => instantiating new model\n",
        "    print(\"Instantiating new model...\", end = ', ')\n",
        "    model = model_core(model_dict) # Instantiate new model\n",
        "    print(\"Compiling model...\", end = ', ')\n",
        "    model_compile(model, compile_dict) # Compile model\n",
        "    print(\"Model Fit started....\", end = ', ')    \n",
        "    model, hist = model_fit(model, train_dict) # Fit model\n",
        "  else: # fit_resume = True => Proceed with existing model in memory or load model from disk\n",
        "    if (load_model): # load_model = True => Load model from disk\n",
        "      print(\"Loading model from disk...\", end = ', ')\n",
        "      model = models.load_model(fm_path) # Reload model from disk\n",
        "      if (recompile): # Re-compile model if \"recompile\" = True\n",
        "        print(\"Re-Compiling model...\", end = ', ')\n",
        "        model_compile(model, compile_dict) \n",
        "      print(\"Resuming model fit....\", end = ', ')\n",
        "      model, hist = model_fit(model, train_dict) # Resume model fit\n",
        "    else: # load_model = False => Proceed with existing model in memory\n",
        "      if (recompile): # Re-compile model if \"recompile\" = True\n",
        "        print(\"Re-compiling model...\", end = ', ')\n",
        "        model_compile(model, compile_dict)\n",
        "      print(\"Resuming model fit....\", end = ', ')\n",
        "      model, hist = model_fit(model, train_dict) # Resume model fit\n",
        "\n",
        "  # Save final trained model and history to file\n",
        "  if (fm_path != None): model.save(fm_path, overwrite = True, save_format = 'h5') \n",
        "  if (hi_path != None): np.save(hi_path, hist.history)\n",
        "\n",
        "  return model, hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WSVV99eJAZ6"
      },
      "source": [
        "## Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RFWakvXIZ6l"
      },
      "source": [
        "### Define HyperModel Class (For Hyperparameter Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekCnKh7eBsgJ"
      },
      "source": [
        "class MyHyperModel(HyperModel):\n",
        "\n",
        "  def __init__(self, model_dict, compile_dict, hp_dict):\n",
        "    self.model_dict_tune = model_dict.copy() # Dictionary of default model parameters\n",
        "    self.compile_dict_tune = compile_dict.copy() # Dictionary of default compilation parameters\n",
        "    self.hp_dict = hp_dict # Dictionary of hyperparameters to tune\n",
        "\n",
        "  def build(self, hp):\n",
        "    hp_list = [] # List placeholder to define all hyperparameters\n",
        "    for ind in range(len(hp_dict)): # Update hp_list based on hp_dict\n",
        "      if (hp_dict[ind][0] == 'choice'): hp_list.append(hp.Choice(name = hp_dict[ind][1],\\\n",
        "                                                                 values = hp_dict[ind][2],\\\n",
        "                                                                 ordered = hp_dict[ind][3]))    \n",
        "    # Set hyperparameters in model_dict_tune\n",
        "    self.model_dict_tune['dropout_rate'] = [hp_list[0], hp_list[1]]\n",
        "\n",
        "    model = model_core(self.model_dict_tune) # Instantiate model\n",
        "    model_compile(model, self.compile_dict_tune) # Compile model    \n",
        "   \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTNB3C8zI7rP"
      },
      "source": [
        "## Set model_core, model_compile, model_train parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXFkcpkFlG78"
      },
      "source": [
        "### Define file paths\n",
        "mod_file_pref = \"MA\" # Prefix to use for naming files and paths\n",
        "tb_path = os.path.join(tb_logs_base_path, mod_file_pref) # Tensorboard base path\n",
        "fm_path = os.path.join(out_base_path, mod_file_pref + \"_finalmodel.h5\") # Final trained model path\n",
        "mc_path = os.path.join(out_base_path, mod_file_pref + \"_EP{epoch:04d}.h5\") # Model checkpoints path\n",
        "hi_path = os.path.join(out_base_path, mod_file_pref + \"_hist.npy\") # Training history path\n",
        "\n",
        "# Set model_dict values\n",
        "model_dict = {'model_arch': 'A',\n",
        "              'use_bnorm': True,\n",
        "              'use_dropout': True, \n",
        "              'dropout_rate': [0.0, 0.0]\n",
        "              }\n",
        "# Set compile_dict values\n",
        "compile_dict = {'ilr': 1e-4, # Initial learning rate to use for learning rate decay scheduler    \n",
        "                'dr': 1, # Decay rate to use for learning rate decay scheduler\n",
        "                'ds': (len(train_generator) * 10), # Decay rate to use for learning rate decay scheduler\n",
        "                'redlr_plat': False, # Boolean: If True, implement reduce LR on plateau  \n",
        "               }\n",
        "# Set train_dict values               \n",
        "train_dict = {'fit_resume': False, # Boolean: If True, resume fit from initial epoch\n",
        "              'load_model': False, # Boolean: If True, load model from 'fm_path' and resume fit\n",
        "              'recompile': False, # Boolean: If True, recompile model before resuming fit\n",
        "              'train_gen': train_generator, # Train generator to use while fitting\n",
        "              'val_gen': val_generator, # Validation generator to use while fitting\n",
        "              'epochs': 30, # Number of epochs to train for              \n",
        "              'initial_epoch': 0, # Initial epoch to start from              \n",
        "              'train_steps_per_epoch': len(train_generator), # No. of steps per epoch\n",
        "              'val_steps': len(val_generator), # No. of steps before stopping eval of val set\n",
        "              'val_freq': 1, # Number of epochs to run before performing a validation run\n",
        "              'verb': 0, # Controls verbosity level of model fit.\n",
        "              'mcp_freq': (len(train_generator) * 500), # Checkpoint model after mcp_freq batches\n",
        "              'early_stop': False, # Boolean: If True, implement early stop\n",
        "              'redlr_plat': compile_dict['redlr_plat'], # Boolean: If True, implement reduce LR on plateau\n",
        "              'lrpl_fac': 0.5, # Factor to use for Reduce LR on Plateau callback\n",
        "              'lrpl_pat': 10, # Patience to use for Reduce LR on Plateau callback              \n",
        "              'tb_path': tb_path, # Path to store Tensorboard callback information\n",
        "              'mc_path': mc_path, # File name to use for storing model checkpoints\n",
        "              'fm_path': fm_path, # File name to use for storing final trained model\n",
        "              'hi_path': hi_path, # File name to use for storing training history\n",
        "              }\n",
        "\n",
        "print(tb_path)\n",
        "print(mc_path)\n",
        "print(fm_path)\n",
        "print(hi_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szj1Zm_A_96z"
      },
      "source": [
        "## Launch Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgDf2PM3__xC"
      },
      "source": [
        "%tensorboard --logdir {tb_logs_base_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHx2QAnCJHA7"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZVDA5n7MFdq"
      },
      "source": [
        "# Get start time of run and display it\n",
        "start_time = datetime.datetime.now(pytz.timezone('Asia/Kolkata'))\n",
        "print(\"Started at %s\" %(start_time.strftime(\"%H:%M:%S\")), end = '; ')\n",
        "\n",
        "# Instantiate, compile and fit model\n",
        "if (train_dict['fit_resume'] and not(train_dict['load_model'])):\n",
        "  model, hist = model_train(model_dict, compile_dict, train_dict, model)\n",
        "else:\n",
        "  model, hist = model_train(model_dict, compile_dict, train_dict)\n",
        "\n",
        "# Get end time of run and display elapsed time\n",
        "end_time = datetime.datetime.now(pytz.timezone('Asia/Kolkata'))\n",
        "elap_time = ((end_time - start_time).total_seconds())/60\n",
        "print(\"\\nCompleted at %s. Elapsed time = %0.2f minutes.\" %(end_time.strftime(\"%H:%M:%S\"), elap_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmVh2Trkw2Yc"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrhhtKJKw5uA"
      },
      "source": [
        "project_name = \"MA_DR_Tune\" # Directory to store results of hyperparameter tuning\n",
        "\n",
        "# Define hyperparameters in hp_dict\n",
        "hp_dict = {0: ['choice', 'Dropout_0', [0.2, 0.3, 0.4, 0.5], True],\n",
        "           1: ['choice', 'Dropout_1', [0.2, 0.3, 0.4, 0.5], True],\n",
        "           }\n",
        "\n",
        "# Instantiate hypermodel           \n",
        "hypermodel = MyHyperModel(model_dict, compile_dict, hp_dict) \n",
        "\n",
        "# Instantiate Tuner\n",
        "tuner = kt.RandomSearch(hypermodel, objective = 'val_categorical_accuracy', max_trials = 2,\\\n",
        "                        seed = 1234, directory = kt_logs_base_path, project_name = project_name)\n",
        "\n",
        "# Display Hyperparameter Search Space\n",
        "tuner.search_space_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-LH0cQt2W75"
      },
      "source": [
        "# Run hyperparameter search\n",
        "tuner.search(x = train_dict['train_gen'], validation_data = train_dict['val_gen'],\\\n",
        "             epochs = 2, steps_per_epoch = train_dict['train_steps_per_epoch'],\\\n",
        "             validation_steps = train_dict['val_steps'], verbose = 1)\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0].get_config()['values']\n",
        "print()\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(\"---------------------\")\n",
        "print(best_hps)\n",
        "\n",
        "# best_hps_dict = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "# best_model = tuner.hypermodel.build(best_hps_dict)\n",
        "# best_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQZt9YRQo3p2"
      },
      "source": [
        "# Results Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_iswhhUZF0q"
      },
      "source": [
        "## Define Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMBiXqvTZKQ9"
      },
      "source": [
        "### Function: get_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0mvSDZIZOV-"
      },
      "source": [
        "def get_model(mod_file_prefix, print_summary = False):\n",
        "  '''\n",
        "  Function to return required paths, loaded model and print model summary\n",
        "\n",
        "  Arguments:\n",
        "    mod_file_prefix: # Prefix of model file name\n",
        "    print_summary: Boolean. If True, print model summary\n",
        "  '''\n",
        "  fm_path = os.path.join(out_base_path, mod_file_pref + \"_finalmodel.h5\") # Final model path\n",
        "  hi_path = os.path.join(out_base_path, mod_file_pref + \"_hist.npy\") # History file path\n",
        "  \n",
        "  # Load model with final trained weights\n",
        "  model = models.load_model(fm_path) \n",
        "  \n",
        "  # Print model summary\n",
        "  if (print_summary):\n",
        "    display(model.summary())\n",
        "\n",
        "  return fm_path, hi_path, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxPHQr8glq3H"
      },
      "source": [
        "### Function: plot_lc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgYAGz1qjzrA"
      },
      "source": [
        "def plot_lc(mod_file_pref, hi_path, hist_plot_dict, num_cols, col_size, row_size):\n",
        "  '''\n",
        "  Function to plot learning curves \n",
        "\n",
        "  Arguments:\n",
        "    hi_path: path of history file\n",
        "    hist_plot_dict: Dictionary containing items to plot\n",
        "    num_cols: Number of columns to use for plotting\n",
        "    col_size: Column width to use while plotting\n",
        "    row_size: Row width to use while plotting\n",
        "  '''\n",
        "  hist_model = np.load(hi_path, allow_pickle = 'TRUE').item()\n",
        "  \n",
        "  num_plots = len(hist_plot_dict)\n",
        "  num_rows = math.ceil(num_plots / num_cols) # Number of rows to use for plotting\n",
        "\n",
        "  fig = plt.figure(figsize = ((num_cols * col_size), (num_rows * row_size)))\n",
        "  fig.suptitle(mod_file_pref + \" Learning Curves\", fontsize = 20)\n",
        "\n",
        "  for ind, value in enumerate(hist_plot_dict.items()):\n",
        "    ax = plt.subplot(num_rows, num_cols, (ind + 1))\n",
        "    for key in value[1][0].keys():\n",
        "      ax.plot(hist_model[key], value[1][0][key][0], label = value[1][0][key][1])     \n",
        "      ax.set_title(value[1][1], fontsize = 20)\n",
        "      ax.set_ylabel(value[1][2], fontsize = 20)\n",
        "      ax.set_xlabel(value[1][3], fontsize = 20)\n",
        "      ax.grid(b = True)\n",
        "      ax.legend(fontsize = 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SniyJYtXjJ5"
      },
      "source": [
        "### Function: plot_cm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZxI4S_u02yr"
      },
      "source": [
        "def plot_cm(train_gen, val_gen, test_gen, y_train, y_val, y_test, row_size, col_size):\n",
        "\n",
        "  '''\n",
        "  Function to plot confusion matrices for train, val and test sets\n",
        "\n",
        "  Arguments:\n",
        "    train_gen: Train generator without augmentation and shuffling\n",
        "    val_gen: Validation generator without augmentation and shuffling\n",
        "    test_gen: Test generator without augmentation and shuffling\n",
        "    y_train: Ground-truth training set labels\n",
        "    y_val: Ground-truth validation set labels\n",
        "    y_test: Ground-truth testing set labels\n",
        "    row_size: Row size to use for plotting\n",
        "    col_size: Column size to use for plotting\n",
        "  '''\n",
        "\n",
        "  # Obtain predicted labels for training, validation and testing sets\n",
        "  y_train_pred = np.argmax(model.predict(x = train_gen, steps = len(train_gen), verbose = 0), axis = 1)\n",
        "  y_val_pred = np.argmax(model.predict(x = val_gen, steps = len(val_gen), verbose = 0), axis = 1)\n",
        "  y_test_pred = np.argmax(model.predict(x = test_gen, steps = len(test_gen), verbose = 0), axis = 1)    \n",
        "\n",
        "  # Generate confusion matrices\n",
        "  cm_train = tf.math.confusion_matrix(y_train, y_train_pred).numpy()\n",
        "  cm_val = tf.math.confusion_matrix(y_val, y_val_pred).numpy()  \n",
        "  cm_test = tf.math.confusion_matrix(y_test, y_test_pred).numpy()\n",
        "\n",
        "  # Convert confusion matrices to pandas DF\n",
        "  index = [('A' + str(a)) for a in list(np.unique(y_train).astype(np.int))]\n",
        "  columns = [('P' + str(a)) for a in list(np.unique(y_train).astype(np.int))]\n",
        "  cm_train_df = pd.DataFrame(cm_train, index = index, columns = columns)\n",
        "  cm_val_df = pd.DataFrame(cm_val, index = index, columns = columns)  \n",
        "  cm_test_df = pd.DataFrame(cm_test, index = index, columns = columns)\n",
        "\n",
        "  # Plot confusion matrices\n",
        "  fig = plt.figure(figsize = (col_size, (3 * row_size)))\n",
        "\n",
        "  ax1 = plt.subplot(3, 1, 1)\n",
        "  sns.heatmap(cm_train_df, annot_kws = {\"fontsize\": 15}, linewidths = 1,\\\n",
        "              linecolor = 'black', cmap = 'Blues', annot = True ,fmt = 'g',\\\n",
        "              cbar = False, ax = ax1)\n",
        "  ax1.set_title(\"Confusion Matrix for Training set\", fontsize = 25)\n",
        "  ax1.tick_params(labelsize = 20)\n",
        "\n",
        "  ax2 = plt.subplot(3, 1, 2)\n",
        "  sns.heatmap(cm_val_df, annot_kws = {\"fontsize\": 15}, linewidths = 1,\\\n",
        "              linecolor = 'black', cmap = 'Blues', annot = True ,fmt = 'g',\\\n",
        "              cbar = False, ax = ax2)\n",
        "  ax2.set_title(\"Confusion Matrix for Validation set\", fontsize = 25)\n",
        "  ax2.tick_params(labelsize = 20)\n",
        "\n",
        "  ax3 = plt.subplot(3, 1, 3)\n",
        "  sns.heatmap(cm_test_df, annot_kws = {\"fontsize\": 15}, linewidths = 1,\\\n",
        "              linecolor = 'black', cmap = 'Blues', annot = True ,fmt = 'g',\\\n",
        "              cbar = False, ax = ax3)\n",
        "  ax3.set_title(\"Confusion Matrix for Test set\", fontsize = 25)\n",
        "  ax3.tick_params(labelsize = 20)  \n",
        "\n",
        "  plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxGjt-5MwOsV"
      },
      "source": [
        "### Function: predict_and_plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78dEw14XnhUj"
      },
      "source": [
        "def predict_and_plot(X, y, samp_indices, model, num_cols, col_size, row_size, fig_title):\n",
        "\n",
        "  '''\n",
        "  Function to make predictions on a subset of data and plot images with actual\n",
        "  and predicted labels\n",
        "\n",
        "  Arguments:\n",
        "    X: Input image array\n",
        "    y: Input label array\n",
        "    samp_indices: Indices from X to be plotted\n",
        "    model: Model instance to use for making predictions\n",
        "    num_cols: Number of columns to use for plotting\n",
        "    col_size: Size of columns to use for plotting\n",
        "    row_size: Size of rows to use for plotting\n",
        "    fig_title: Title to use for overall figure\n",
        "  ''' \n",
        "\n",
        "  num_rows = math.ceil(len(samp_indices) / num_cols) # Number of rows to use for plotting\n",
        "  fig = plt.figure(figsize = ((num_cols * col_size), (num_rows * row_size)))\n",
        "  fig.suptitle(fig_title, fontsize = 40)\n",
        "\n",
        "  for ind, samp_ind in enumerate(samp_indices):\n",
        "    img = np.expand_dims(X[samp_ind].copy(), axis = 0)\n",
        "    img = preprocess_data(img)\n",
        "    y_pred = np.argmax(model.predict(img), axis = 1)[0]\n",
        "    ax = plt.subplot(num_rows, num_cols, (ind + 1))\n",
        "    ax.text(0.5, 1.35, f\"Sample index is {samp_ind}\", transform = ax.transAxes,\\\n",
        "            horizontalalignment = 'center', verticalalignment = 'center',\\\n",
        "            color = 'black', fontfamily = 'sans-serif', fontsize = '22')    \n",
        "    ax.text(0.5, 1.20, f\"Actual label is {y[samp_ind]}\", transform = ax.transAxes,\\\n",
        "            horizontalalignment = 'center', verticalalignment = 'center',\\\n",
        "            color = 'black', fontfamily = 'sans-serif', fontsize = '22')    \n",
        "    ax.text(0.5, 1.05, f\"Predicted label is {y_pred}\", transform = ax.transAxes,\\\n",
        "            horizontalalignment = 'center', verticalalignment = 'center',\\\n",
        "            color = 'black', fontfamily = 'sans-serif', fontsize = '22')    \n",
        "    ax.imshow(img[0].squeeze(), cmap = 'gray')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccXOB8IpbSAu"
      },
      "source": [
        "## Model_1: CNN_1\n",
        "\n",
        "* Input shape = (28 x 28 x 1)\n",
        "* Layers:\n",
        "\n",
        "    * Conv2D: Filters = 32, Kernel size = (3 x 3). Padding = 'valid', stride = (1 x 1). Output shape = (26 x 26 x 32). \n",
        "    * Activation = 'ReLU'\n",
        "    * Conv2D: Filters = 32, Kernel size = (3 x 3). Padding = 'valid', stride = (1 x 1). Output shape = (24 x 24 x 32)\n",
        "    * Activation = 'ReLU'\n",
        "    * Flatten: Output shape = (18432 x 1)\n",
        "    * Dense: Output shape = (128 x 1)\n",
        "    * Activation = 'ReLU'\n",
        "    * Dense: Output shape = (10 x 1)\n",
        "    * Activation = 'Softmax'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnfMDTb_Y5tD"
      },
      "source": [
        "### Define file paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQN2p3yZY6_6"
      },
      "source": [
        "mod_file_pref = \"MA\"\n",
        "fm_path, hi_path, model = get_model(\"MA\", False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3kZGuqDwjH1"
      },
      "source": [
        "### Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJHVf5aeds8o"
      },
      "source": [
        "hist_plot_dict = {0: [{'loss': ['r-', 'Training_Set_Loss'],\n",
        "                       'val_loss': ['b-', 'Validation_Set_Loss']},\n",
        "                      'Loss vs. #Epoch', 'Loss', '# Epochs'],\n",
        "                  1: [{'categorical_accuracy': ['r-', 'Training_Set_Accuracy'],\n",
        "                      'val_categorical_accuracy': ['b-', 'Validation_Set_Accuracy']},\n",
        "                      'Accuracy vs. #Epoch', 'Accuracy', '# Epochs']\n",
        "                  }\n",
        "plot_lc(mod_file_pref, hi_path, hist_plot_dict, 2, 7, 7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwaPm5YwtVt"
      },
      "source": [
        "### Evaluate model on training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmpQYEW3wtVx"
      },
      "source": [
        "# Evaluate model on training set\n",
        "res = model.evaluate(x = train_generator, steps = len(train_generator), verbose = 0, return_dict = True)\n",
        "print(\"Training set loss is %0.4f\" % res['loss'])\n",
        "print(\"Training set accuracy is %0.2f %%\" % (100 * res['categorical_accuracy']))\n",
        "print()\n",
        "# Evaluate model on validation set\n",
        "res = model.evaluate(x = val_generator, steps = len(val_generator), verbose = 0, return_dict = True)\n",
        "print(\"Validation set loss is %0.4f\" % res['loss'])\n",
        "print(\"Validation set accuracy is %0.2f %%\" % (100 * res['categorical_accuracy']))\n",
        "print()\n",
        "# Evaluate model on test set\n",
        "res = model.evaluate(x = test_generator, steps = len(test_generator), verbose = 0, return_dict = True)\n",
        "print(\"Test set loss is %0.4f\" % res['loss'])\n",
        "print(\"Test set accuracy is %0.2f %%\" % (100 * res['categorical_accuracy']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsN8doP3x2lG"
      },
      "source": [
        "### Plot Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pykY1SdXqFj"
      },
      "source": [
        "# Regenerate train, val and test generators without augmentation and shuffling\n",
        "train_datagen_cm = ImageDataGenerator(preprocessing_function = preprocess_data)\n",
        "val_datagen_cm = ImageDataGenerator(preprocessing_function = preprocess_data)\n",
        "test_datagen_cm = ImageDataGenerator(preprocessing_function = preprocess_data)\n",
        "\n",
        "train_gen_cm = train_datagen_cm.flow(X_train, y_train_ohe, batch_size = mod_bat_size,\\\n",
        "                                           shuffle = False)\n",
        "val_gen_cm = val_datagen_cm.flow(X_val, y_val_ohe, batch_size = mod_bat_size,\\\n",
        "                                           shuffle = False)\n",
        "test_gen_cm = test_datagen_cm.flow(X_test, y_test_ohe, batch_size = mod_bat_size,\\\n",
        "                                           shuffle = False)\n",
        "\n",
        "plot_cm(train_gen_cm, val_gen_cm, test_gen_cm, y_train, y_val, y_test, 8, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEKS2o2UKuWS"
      },
      "source": [
        "### Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyYPi_x0iRBJ"
      },
      "source": [
        "### Use this to plot random images from a particular set\n",
        "# samp_indices = np.random.randint(low = 0, high = X_train.shape[0], size = 10)\n",
        "# fig_title = 'Random Images from training set'\n",
        "\n",
        "### Use this to plot random images corresponding to a particular label\n",
        "# label = 5\n",
        "# full_indices = np.nonzero(y_train == label)[0]\n",
        "# samp_indices = list(np.random.choice(full_indices, size = 10))\n",
        "# fig_title = f'Images from training set with actual label = {label}'\n",
        "\n",
        "### Use this to plot random images corresponding to incorrect predictions from a particular set\n",
        "# y_train_pred = np.argmax(model.predict(x = train_gen_cm, steps = len(train_gen_cm), verbose = 0),\\\n",
        "#                          axis = 1)\n",
        "# full_indices = np.nonzero(y_train != y_train_pred)[0]\n",
        "# samp_indices = list(np.random.choice(full_indices, size = 10))\n",
        "# fig_title = 'Images from training set corresponding to incorrect predictions'\n",
        "\n",
        "### Use this to plot random images corresponding to incorrect predictions of a particular label\n",
        "y_train_pred = np.argmax(model.predict(x = train_gen_cm, steps = len(train_gen_cm), verbose = 0),\\\n",
        "                         axis = 1)\n",
        "label = 8\n",
        "full_indices = np.nonzero(y_train == label)[0]\n",
        "full_indices = full_indices[np.nonzero(y_train[full_indices] != y_train_pred[full_indices])]\n",
        "samp_indices = list(np.random.choice(full_indices, size = 10))\n",
        "fig_title = f'Images from training set with incorrect predictions of label = {label}'\n",
        "\n",
        "# Predict and plot\n",
        "predict_and_plot(X_train, y_train, samp_indices, model, 5, 5, 7, fig_title)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}