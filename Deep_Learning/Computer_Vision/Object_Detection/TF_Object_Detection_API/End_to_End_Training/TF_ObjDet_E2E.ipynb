{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Object_Detection_E2E_TF2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suresh-venkate/Code_Repositories/blob/main/Deep_Learning/Computer_Vision/Object_Detection/TF_Object_Detection_API/End_to_End_Training/TF_ObjDet_E2E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLrs0QG4OII_"
      },
      "source": [
        "# TensorFlow Object Detection API - End to End"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2zIuU2I70fY"
      },
      "source": [
        "# Complete all installations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj_ujSb4C_mB"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Clone TF models repository from GitHUb\n",
        "!git clone https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbe2su-APa7r"
      },
      "source": [
        "%%bash\n",
        "cd models/research\n",
        "# Compile protos.\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "# Install TensorFlow Object Detection API.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install --quiet ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6UoR-CtZKuW"
      },
      "source": [
        "# Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4YXi_wPZMnn"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx8_pjTsGorG"
      },
      "source": [
        "# Download Pascal VOC dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUGWdmsuIcz5"
      },
      "source": [
        "%%capture\n",
        "%cd /content/ \n",
        "!mkdir Dataset \n",
        "%cd Dataset \n",
        "\n",
        "# Download xml_to_csv.py and generate-tfrecord.py\n",
        "!wget https://raw.githubusercontent.com/suresh-venkate/Code_Repositories/main/Deep_Learning/Computer_Vision/Object_Detection/TF_Object_Detection_API/End_to_End_Training/xml_to_csv.py\n",
        "!wget https://raw.githubusercontent.com/suresh-venkate/Code_Repositories/main/Deep_Learning/Computer_Vision/Object_Detection/TF_Object_Detection_API/End_to_End_Training/generate_tfrecord.py\n",
        "\n",
        "#Get PASCAL VOC dataset\n",
        "!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar --quiet\n",
        "# Unzip tar file\n",
        "!tar -xf VOCtrainval_06-Nov-2007.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTTo8kvtJvDg"
      },
      "source": [
        "# Define images folder and XMLs folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFBOWStNJ1sK"
      },
      "source": [
        "img_path = '/content/Dataset/VOCdevkit/VOC2007/JPEGImages/'\n",
        "xml_path = '/content/Dataset/VOCdevkit/VOC2007/Annotations/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY5yXLB-K9Dl"
      },
      "source": [
        "# Data Pre-processing : Convert XML to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6cANEVgNhKc"
      },
      "source": [
        "# Convert XML to CSV using script\n",
        "!python xml_to_csv.py -i {xml_path} -o annot_data.csv\n",
        "print()\n",
        "\n",
        "# Load CSV file as dataframe\n",
        "annot_df = pd.read_csv('annot_data.csv')\n",
        "display(annot_df.head(5))\n",
        "print()\n",
        "\n",
        "# Print number of images with annotations\n",
        "print(\"Total number of images is %d\" %(annot_df.shape[0]))\n",
        "# Print number of unique classes in dataset\n",
        "print(\"Number of unique classes is %d\" %(len(annot_df['class'].unique())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15V6u830nPVk"
      },
      "source": [
        "# Label encode classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KfwMfHoUGKd"
      },
      "source": [
        "# Label Encode class and add a 'label' column to the dataframe\n",
        "le = preprocessing.LabelEncoder()\n",
        "annot_df['label'] = le.fit_transform(annot_df['class'])\n",
        "# Object detection API expects index to start from 1 (and not 0)\n",
        "annot_df['label'] = annot_df['label'] + 1\n",
        "\n",
        "# Print list of unique classes and unique class labels\n",
        "display(annot_df['class'].unique())\n",
        "print()\n",
        "display(annot_df['label'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wna0FoqMZw2h"
      },
      "source": [
        "# Create a dictionary mapping from Label to Class. \n",
        "label_class_dict = dict(zip(annot_df['label'], annot_df['class']))\n",
        "print(label_class_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xCyipZzYZr_"
      },
      "source": [
        "# Create Label Mapping File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxomQ0LyYeB5"
      },
      "source": [
        "# Build a pbtxt label file using label and class name\n",
        "\n",
        "pbtxt_file_str = '' # Initialize pbtxt file string\n",
        "\n",
        "for label in sorted(label_class_dict.keys()):\n",
        "  pbtxt_file_str += \"item {\\n  id: \" + str(label) + \"\\n  name: '\" +  label_class_dict[label] + \"'\\n}\\n\\n\"\n",
        "\n",
        "with open('label_map.txt','w') as pbfile:\n",
        "    pbfile.write(pbtxt_file_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyfANFw0Krpy"
      },
      "source": [
        "# Split data between training and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlgaYomMES2H"
      },
      "source": [
        "# Get information on all images\n",
        "all_images = annot_df['filename'].unique()\n",
        "# Create train / test split (80% of data for training)\n",
        "np.random.seed(0)\n",
        "mask = np.random.rand(all_images.shape[0]) < 0.8\n",
        "\n",
        "train_images = all_images[mask]\n",
        "test_images = all_images[~mask] \n",
        "\n",
        "print(\"Number of train_images is %d\" %(len(train_images)))\n",
        "print(\"Number of test_images is %d\" %(len(test_images)))\n",
        "print()\n",
        "\n",
        "# Split dataframe between training and test\n",
        "train_df = annot_df[annot_df['filename'].isin(train_images)]\n",
        "test_df = annot_df[annot_df['filename'].isin(test_images)]\n",
        "\n",
        "print(\"Size of train_df is %d\" %train_df.shape[0])\n",
        "print(\"Size of test_df is %d\" %test_df.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NJq-pBELXlX"
      },
      "source": [
        "# Visualizing some images with BBoxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6C56NWfq8ys"
      },
      "source": [
        "## Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5ANh3SOLTzZ"
      },
      "source": [
        "# Pickup a random image number\n",
        "samp_ind = np.random.randint(0, train_df.shape[0], 1)[0]\n",
        "\n",
        "# Read the image\n",
        "img_file = train_df.iloc[samp_ind,0]\n",
        "img = cv2.imread(img_path + '/' + img_file)\n",
        "\n",
        "# Find all rows which have same file name\n",
        "all_rows = train_df[train_df['filename'] == img_file].index.tolist()\n",
        "\n",
        "# Draw BBoxes along with labels\n",
        "for ind in all_rows:\n",
        "\n",
        "    # Get bounding box co-ords\n",
        "    xmin, ymin, xmax, ymax = train_df.loc[ind, ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "    # Get Class of object\n",
        "    obj_class = train_df.loc[ind, 'class']\n",
        "    # Draw BBox\n",
        "    cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
        "    # Add text\n",
        "    cv2.putText(img, obj_class, (xmin, ymin + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "# Convert BGR format (used by opencv to RGB format used by matplotlib)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Draw image using matplotlib\n",
        "plt.figure(figsize = (10,7))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gni_mk-6zVA6"
      },
      "source": [
        "## Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qigK56qzXA_"
      },
      "source": [
        "# Pickup a random image number\n",
        "samp_ind = np.random.randint(0, test_df.shape[0], 1)[0]\n",
        "\n",
        "# Read the image\n",
        "img_file = test_df.iloc[samp_ind, 0]\n",
        "img = cv2.imread(img_path + '/' + img_file)\n",
        "\n",
        "# Find all rows which have same file name\n",
        "all_rows = test_df[test_df['filename'] == img_file].index.tolist()\n",
        "\n",
        "# Draw BBoxes along with labels\n",
        "for ind in all_rows:\n",
        "\n",
        "    # Get bounding box co-ords\n",
        "    xmin, ymin, xmax, ymax = test_df.loc[ind, ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "    # Get Class of object\n",
        "    obj_class = test_df.loc[ind, 'class']\n",
        "    # Draw BBox\n",
        "    cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
        "    # Add text\n",
        "    cv2.putText(img, obj_class, (xmin, ymin + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "# Convert BGR format (used by opencv to RGB format used by matplotlib)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Draw image using matplotlib\n",
        "plt.figure(figsize = (10,7))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5_wHuLqVxb3"
      },
      "source": [
        "# Save training and test data as csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5mKamauV0Je"
      },
      "source": [
        "train_df.to_csv('train.csv', index = False)\n",
        "test_df.to_csv('test.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgYaNAGEQThJ"
      },
      "source": [
        "# Generate tfrecord from CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AhiyJ3BRZVv"
      },
      "source": [
        "%%capture\n",
        "\n",
        "#Generate tfrecord for training data\n",
        "!python generate_tfrecord.py --csv_input=train.csv --img_path={img_path} --output_path=train.tfrecord\n",
        "#Generate tfrecord for test data\n",
        "!python generate_tfrecord.py --csv_input=test.csv --img_path={img_path} --output_path=test.tfrecord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjxG56jeeFXk"
      },
      "source": [
        "# Download a pre-trained model\n",
        "\n",
        "A list of pre-trained models is available at [TensorFlow model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). We will use 'ssd_mobilenet_v1_coco' model for transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNE5_ol7eJCE"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Download the model from zoo\n",
        "!wget -q http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n",
        "# Extract tar file\n",
        "!tar -xf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n",
        "# Delete tar file\n",
        "!rm ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfu0k9k0e-CV"
      },
      "source": [
        "# Prepare Training configuration file\n",
        "\n",
        "1. Change num_classes parameter to 20 (as we have 20 categories in pascal voc dataset)\n",
        "2. For 'train_input_reader' change 'input_path' to filepath of train.record file. (Done)\n",
        "3. For 'train_input_reader' change 'label_map_path' to filepath of label_map.txt file. (Done)\n",
        "4. Repeat above two steps for 'eval_input_reader'. (Done)\n",
        "5. Change fine_tune_checkpoint to filepath where pre-trained model.ckpt file is available e.g ssd_mobilenet_v1_coco_2018_01_28/model.ckpt\n",
        "6. Change 'batch_size' accordingly to available memory.\n",
        "7. Change 'num_steps' to indicate how long the training will done e.g. 200000. For demo purpose, we are keeping it to 20 so that we can finish training quickly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd2oF4RATqCe"
      },
      "source": [
        "You can copy a sample configuration for the chosen pre-trained model (SSD MobileNet v2 320x320) in this case from [Configs](https://github.com/tensorflow/models/tree/master/research/object_detection/configs/tf2) folder. Here are things which need to be changed at a minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKX_yViAwXIw"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/suresh-venkate/Code_Repositories/main/Deep_Learning/Computer_Vision/Object_Detection/TF_Object_Detection_API/End_to_End_Training/ssd_mobilenet_v2_modified.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuPDh-FcNMJZ"
      },
      "source": [
        "config_file = 'ssd_mobilenet_v2_modified.config'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbCourM9jMGd"
      },
      "source": [
        "#### 8. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1k412IGjP_d"
      },
      "source": [
        "#Copy training file from 'models/research/object_detection' folder to current folder\n",
        "!cp /content/models/research/object_detection/model_main_tf2.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cNtnArWwKiZ"
      },
      "source": [
        "Start training \n",
        "\n",
        "- Please note that Object detection take long time to train. The training may take few days if run on single GPU machine (depending on num of steps indicated). Try to keep training the model till loss comes close to 1 (or goes below 1). The script takes 3 inputs\n",
        "\n",
        "1. --train_dir=<folder_name> : where model will be saved periodically as training progresses\n",
        "2. --pipeline_config_path=<config_file_path> :where is model training configuration file located."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYSZ3lU8jrlq"
      },
      "source": [
        "#Create a training folder to store model checkpoints/snapshots as training progresses\n",
        "!mkdir training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQLMnw-B43BR"
      },
      "source": [
        "#Check training folder\n",
        "!ls -l training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWU775b3cYsL"
      },
      "source": [
        "Start Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEc2xP2fbCCQ"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9tm1l5BasDb"
      },
      "source": [
        "%tensorboard --logdir training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sNQzqpxjnB4"
      },
      "source": [
        "#start training\n",
        "!python model_main_tf2.py --model_dir=training/ --pipeline_config_path={config_file} --checkpoint_every_n=5 --alsologtostderr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1bNNQ2pzxW8"
      },
      "source": [
        "Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiiARwj32t9y"
      },
      "source": [
        "#start Evaluation - THIS WILL KEEP RUNNING.\n",
        "!python model_main_tf2.py --model_dir=training/ --pipeline_config_path={config_file} --checkpoint_dir=training/ --alsologtostderr "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3CVJqnGWg7"
      },
      "source": [
        "#### Training and Evaluation in Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ust7kYHQ0mh6"
      },
      "source": [
        "If we want to evaluate our model on training data regularly, we have to run both training and evaluation script in parallel. Model evaluation on test data gets done everytime model checkpoint is saved during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYh6I60xCWLX"
      },
      "source": [
        "#Start training\n",
        "!nohup /usr/bin/python3 model_main_tf2.py --model_dir=training/ --pipeline_config_path={config_file} --checkpoint_every_n=500 --alsologtostderr --eval_training_data --sample_1_of_n_eval_on_train_examples=10  > train.txt 2>&1 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dczrQx0OC4Ki"
      },
      "source": [
        "!nohup /usr/bin/python3 model_main_tf2.py --model_dir=training/ --pipeline_config_path={config_file} --checkpoint_dir=training/ --alsologtostderr > eval.txt 2>&1 &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc444JgNDYzu"
      },
      "source": [
        "!cat train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvLi43NUDGX3"
      },
      "source": [
        "!cat eval.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aznLkMtLoGuF"
      },
      "source": [
        "#Check the training folder\n",
        "!ls -l training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxbue-P4kEvi"
      },
      "source": [
        "#### 9. Export trained model\n",
        "\n",
        "From the saved model checkpoints, we will create a frozen trained model. Frozen here means to remove model nodes which are no longer needed in prediction. This reduces model size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7MVmgSQkLov"
      },
      "source": [
        "#Copy export_inference_graph.py file from models/research/object_detection to current directory\n",
        "!cp /content/models/research/object_detection/exporter_main_v2.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_59ZQxnmkay5"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45hd528oMlpy"
      },
      "source": [
        "The export_inference_graph.py script file requires the following input:\n",
        "\n",
        "1. --input_type <input_node_name> : This will be used during prediction to set model input\n",
        "2. --pipeline_config_path <model_training_config_file_path> : where is model training config file located.\n",
        "3. --trained_checkpoint_prefix <file_path__model_checkpoint> : Which checkpoint should be used to create final model.\n",
        "4. --output_directory <frozen_model_directory> : where should the frozen model created by script should be stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwKWqRcwkHz7"
      },
      "source": [
        "#Provide input name, config file location, training folder\n",
        "!python exporter_main_v2.py --input_type \"image_tensor\" --pipeline_config_path {config_file} --trained_checkpoint_dir training/ --output_directory detection_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw8HOKCloQgp"
      },
      "source": [
        "#Check if model is saved in current directory\n",
        "!ls -l detection_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIlaUfPifCDD"
      },
      "source": [
        "#Check if model is saved in current directory\n",
        "!ls -l detection_model/saved_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdQK1xwqL__e"
      },
      "source": [
        "!ls -l detection_model/saved_model/variables/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkSf2y-vMEN_"
      },
      "source": [
        "!ls -l detection_model/saved_model/assets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G2ScbsvHrYT"
      },
      "source": [
        "#### Move the trained model to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtyngLYIHvL_"
      },
      "source": [
        "!cp -r detection_model '/gdrive/MyDrive/AI_ML_Folder/Colab_Directory/TF_ObjDet/pascal_voc_tf2/' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nuGe4twJ0iT"
      },
      "source": [
        "Save label dictionary as well for model prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6kF6U09J5dh"
      },
      "source": [
        "print(label_class_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D5PhdlGJ9cG"
      },
      "source": [
        "import pickle\n",
        "\n",
        "label_file_path = '/gdrive/MyDrive/AI_ML_Folder/Colab_Directory/TF_ObjDet/pascal_voc_label.pkl'\n",
        "\n",
        "with open(label_file_path,'wb') as file:\n",
        "    pickle.dump(label_class_dict, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtZnLR1ckdbS"
      },
      "source": [
        "#### Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk6AdwT7kfPE"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W9CfkGn9viy"
      },
      "source": [
        "Load Saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGcnbCBGklP2"
      },
      "source": [
        "saved_model_path = 'detection_model/saved_model'\n",
        "model = tf.saved_model.load(saved_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vhAv2id1xk9"
      },
      "source": [
        "Function to get model prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0i6SCaplWqP"
      },
      "source": [
        "#Function to get predictions from a Detection model\n",
        "def detector_prediction(image_file, confidence_threshold=0.5):\n",
        "\n",
        "    \"\"\"\n",
        "    image_file: File path of the image for which prediction needs to be done\n",
        "    confidence_threshold: Minimum confidence/probability for prediction to be considered\n",
        "    \"\"\"\n",
        "    #Load image\n",
        "    img = tf.keras.preprocessing.image.load_img(image_file)\n",
        "    \n",
        "    #Convert to numpy array\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img).astype('uint8')\n",
        "    #Make it a batch of one example\n",
        "    img_array = tf.expand_dims(img_array, axis=0)\n",
        "\n",
        "    #Prediction\n",
        "    output = model(img_array) #get list of tensors discussed above as output\n",
        "    #print(output)\n",
        "    detection_scores = output['detection_scores'].numpy()[0] #get detection scores\n",
        "    detection_classes = output['detection_classes'].numpy()[0]\n",
        "    detection_boxes = output['detection_boxes'].numpy()[0]\n",
        "\n",
        "    #Select predictions for which probability is higher than confidence_threshold\n",
        "    selected_predictions = detection_scores >= confidence_threshold\n",
        "\n",
        "    selected_prediction_scores = detection_scores[selected_predictions]\n",
        "    selected_prediction_classes = detection_classes[selected_predictions]\n",
        "    selected_prediction_boxes = detection_boxes[selected_predictions]\n",
        "\n",
        "    #De-normalize box co-ordinates (multiply x-coordinates by image width and y-coords by image height)\n",
        "    img_w, img_h = img.size\n",
        "\n",
        "    for i in range(selected_prediction_boxes.shape[0]):\n",
        "        \n",
        "        selected_prediction_boxes[i,0] *= img_h #ymin * img_w\n",
        "        selected_prediction_boxes[i,1] *= img_w #xmin * img_h\n",
        "        selected_prediction_boxes[i,2] *= img_h #ymax * img_w\n",
        "        selected_prediction_boxes[i,3] *= img_w #xmax * img_h\n",
        "\n",
        "    #Make all co-ordinates as integer\n",
        "    selected_prediction_boxes= selected_prediction_boxes.astype(int)\n",
        "\n",
        "    #Convert class indexes to actual class labels\n",
        "    predicted_classes = []\n",
        "    for i in range(selected_prediction_classes.shape[0]):\n",
        "        predicted_classes.append(label_class_dict[int(selected_prediction_classes[i])])\n",
        "\n",
        "    #Number of predictions\n",
        "    selected_num_predictions = selected_prediction_boxes.shape[0]\n",
        "\n",
        "    return {'Total Predictions': selected_num_predictions,\n",
        "            'Scores': selected_prediction_scores, \n",
        "            'Classes': predicted_classes, \n",
        "            'Box coordinates': selected_prediction_boxes}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULqXIGX512ri"
      },
      "source": [
        "Copy an image to predict on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdGHNALJ1pKo"
      },
      "source": [
        "!cp '/gdrive/MyDrive/AI_ML_Folder/Colab_Directory/TF_ObjDet/person_with_bike.jpg' ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jDTBVhc150o"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7MEewOAmLUb"
      },
      "source": [
        "#Model output\n",
        "detector_prediction('person_with_bike.jpg', confidence_threshold=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvZqpVaF1Tpo"
      },
      "source": [
        "Visualize model output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03U3uDUO1NDA"
      },
      "source": [
        "import cv2\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmInmPGz1WsW"
      },
      "source": [
        "def visualize_output(image_file, confidence_threshold=0.5):\n",
        "\n",
        "    #Call model prediction function above\n",
        "    output = detector_prediction(image_file, confidence_threshold=confidence_threshold)\n",
        "\n",
        "    #Read image\n",
        "    img = cv2.imread(image_file)\n",
        "\n",
        "    #Draw rectangle for predicted boxes, also add predicted classes\n",
        "    for i in range(output['Box coordinates'].shape[0]):\n",
        "\n",
        "        box = output['Box coordinates'][i]\n",
        "        \n",
        "        #Draw rectangle - (ymin, xmin, ymax, xmax)\n",
        "        img = cv2.rectangle(img, (box[1], box[0]), (box[3], box[2]), (0,255,0), 2)\n",
        "        \n",
        "        #Add Label - Class name and confidence level\n",
        "        label = output['Classes'][i] + ': ' + str(round(output['Scores'][i],2))\n",
        "        img = cv2.putText(img, label, (box[1], box[0]-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
        "    \n",
        "    #Conver BGR image to RGB to use with Matplotlib\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    #Display image\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKTH6Zys1YQ2"
      },
      "source": [
        "#Visualize on image\n",
        "visualize_output('person_with_bike.jpg', confidence_threshold=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}