{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Bounding Box Dataset Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define root directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.normpath(r'E:\\Sync_With_NAS_Ext\\Datasets\\Image_Datasets\\Stanford_Car_Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Check original data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_orig_data(dataset_path):\n",
    "    \n",
    "    ### Check original data first\n",
    "    names_orig_csv_path = os.path.join(dataset_path, 'names_orig.csv')\n",
    "    annot_train_orig_csv_path = os.path.join(dataset_path, 'annot_train_orig.csv')\n",
    "    annot_test_orig_csv_path = os.path.join(dataset_path, 'annot_test_orig.csv')\n",
    "    train_img_orig_path = os.path.join(dataset_path, 'car_data/train')\n",
    "    test_img_orig_path = os.path.join(dataset_path, 'car_data/test')\n",
    "    train_img_orig_num_fold = len(os.listdir(train_img_orig_path))\n",
    "    test_img_orig_num_fold = len(os.listdir(test_img_orig_path))\n",
    "\n",
    "    num_train_set_images = 0\n",
    "    num_test_set_images = 0\n",
    "    for _, _, files in os.walk(train_img_orig_path):\n",
    "        num_train_set_images += len(files)\n",
    "    for _, _, files in os.walk(test_img_orig_path):\n",
    "        num_test_set_images += len(files)\n",
    "\n",
    "    if (os.path.isfile(names_orig_csv_path)):\n",
    "        print(\"names_orig.csv file found in dataset_path...\")\n",
    "    if (os.path.isfile(annot_train_orig_csv_path)):\n",
    "        print(\"annot_train_orig.csv file found in dataset_path...\")\n",
    "    if (os.path.isfile(annot_test_orig_csv_path)):\n",
    "        print(\"annot_test_orig.csv file found in dataset_path...\")\n",
    "    if (os.path.isdir(train_img_orig_path)):\n",
    "        print(\"Training set images path found in dataset_path...\")\n",
    "    if (os.path.isdir(test_img_orig_path)):\n",
    "        print(\"Test set images path found in dataset_path...\")\n",
    "\n",
    "    print()\n",
    "    print(f\"{train_img_orig_num_fold} folders found in training set images path\")\n",
    "    print(f\"{test_img_orig_num_fold} folders found in test set images path\")\n",
    "    print()\n",
    "    print(f\"{num_train_set_images} images found in training set images path\")\n",
    "    print(f\"{num_test_set_images} images found in test set images path\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cons_dset_folders(dataset_path):\n",
    "    \n",
    "    # Delete previously created directories (if any)\n",
    "    cons_dir_path = os.path.join(dataset_path, \"Consolidated_Dataset\")\n",
    "    cons_train_img_dir_path = os.path.join(cons_dir_path, \"train_images\")\n",
    "    cons_test_img_dir_path = os.path.join(cons_dir_path, \"test_images\")\n",
    "    cons_zip_file_path = os.path.join(dataset_path, \"Consolidated_Dataset.zip\")\n",
    "    \n",
    "    if os.path.isdir(cons_dir_path):\n",
    "        print(\"Consolidated_Dataset folder found: Deleting...\")\n",
    "        shutil.rmtree(cons_dir_path)\n",
    "    if os.path.isfile(cons_zip_file_path):\n",
    "        print(\"Consolidated_Dataset.zip file found: Deleting...\")\n",
    "        os.remove(cons_zip_file_path)\n",
    "        \n",
    "    print()    \n",
    "    print(\"Creating Consolidated_Dataset folder...\")\n",
    "    os.mkdir(cons_dir_path)\n",
    "    print(\"Creating Consolidated_Dataset/train_images folder...\")\n",
    "    os.mkdir(cons_train_img_dir_path)    \n",
    "    print(\"Creating Consolidated_Dataset/test_images folder...\")\n",
    "    os.mkdir(cons_test_img_dir_path)\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_red_dset_folders(dataset_path):\n",
    "    \n",
    "    # Delete previously created directories (if any)\n",
    "    red_dir_path = os.path.join(dataset_path, \"Reduced_Dataset\")\n",
    "    red_train_img_dir_path = os.path.join(red_dir_path, \"train_images\")\n",
    "    red_test_img_dir_path = os.path.join(red_dir_path, \"test_images\")\n",
    "    red_zip_file_path = os.path.join(dataset_path, \"Reduced_Dataset.zip\")\n",
    "\n",
    "    if os.path.isdir(red_dir_path):\n",
    "        print(\"Reduced_Dataset folder found: Deleting...\")\n",
    "        shutil.rmtree(red_dir_path)\n",
    "    if os.path.isfile(red_zip_file_path):\n",
    "        print(\"Reduced_Dataset.zip file found: Deleting...\")\n",
    "        os.remove(red_zip_file_path)\n",
    "        \n",
    "    print()    \n",
    "    print(\"Creating Reduced_Dataset folder...\")\n",
    "    os.mkdir(red_dir_path)\n",
    "    print(\"Creating Reduced_Dataset/train_images folder...\")\n",
    "    os.mkdir(red_train_img_dir_path)    \n",
    "    print(\"Creating Reduced_Dataset/test_images folder...\")\n",
    "    os.mkdir(red_test_img_dir_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Consolidate all training and test set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons_img_files(dataset_path):\n",
    "    \n",
    "    # Consolidate training set images\n",
    "    train_img_root_path = os.path.join(dataset_path, 'car_data/train')\n",
    "    train_img_dest_path = os.path.join(dataset_path, 'Consolidated_Dataset/train_images/')\n",
    "    print(\"Consolidating training set images to Consolidated_Dataset/train_images... \",\\\n",
    "          end = '')\n",
    "    for folder in os.listdir(train_img_root_path):\n",
    "        curr_path = os.path.join(train_img_root_path, folder)\n",
    "        for file in os.listdir(curr_path):\n",
    "            file_path = os.path.join(curr_path, file)\n",
    "            shutil.copy(file_path, train_img_dest_path)   \n",
    "    print(\"%d images consolidated.\" %(len(os.listdir(train_img_dest_path))))\n",
    "    \n",
    "    # Consolidate test set images\n",
    "    test_img_root_path = os.path.join(dataset_path, 'car_data/test')\n",
    "    test_img_dest_path = os.path.join(dataset_path, 'Consolidated_Dataset/test_images/')\n",
    "    print(\"Consolidating test set images to Consolidated_Dataset/test_images... \", end = '')\n",
    "    for folder in os.listdir(test_img_root_path):\n",
    "        curr_path = os.path.join(test_img_root_path, folder)\n",
    "        for file in os.listdir(curr_path):\n",
    "            file_path = os.path.join(curr_path, file)\n",
    "            shutil.copy(file_path, test_img_dest_path) \n",
    "    print(\"%d images consolidated.\" %(len(os.listdir(test_img_dest_path))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create label-class dictionary mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lbl_cls_dict(dataset_path):\n",
    "    # Set path of class names csv file\n",
    "    classes_path = os.path.join(dataset_path, 'names_orig.csv') \n",
    "    # Load classes into a dataframe\n",
    "    classes = pd.read_csv(classes_path, header = None, names = ['class'])\n",
    "    # Define empty dictionary to store label_class dictionary mapping\n",
    "    label_class_dict = {}\n",
    "    # Iterate through classes DF and update label_class dictionary mapping\n",
    "    print(\"Creating label_class_dict...\")\n",
    "    for row in classes.iterrows():\n",
    "        label_class_dict[row[0] + 1] = row[1]['class']\n",
    "    \n",
    "    print()\n",
    "    return label_class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Update train annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def upd_train_ann(dataset_path):\n",
    "    \n",
    "    annot_train_orig_csv_path = os.path.join(dataset_path, 'annot_train_orig.csv')\n",
    "    df_cols = ['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'label']\n",
    "    annot_train_df = pd.read_csv(annot_train_orig_csv_path, header = None, names = df_cols,\\\n",
    "                                 index_col = False)\n",
    "    train_img_cons_path = os.path.join(dataset_path, 'Consolidated_Dataset/train_images/')\n",
    "    print(\"Adding image height, width and image class to annot_train_df...\")\n",
    "    for ind, row in annot_train_df.iterrows():\n",
    "        img_path = os.path.join(train_img_cons_path, row['filename'])\n",
    "        img_arr = mpimg.imread(img_path)\n",
    "        annot_train_df.loc[ind, 'img_h'] = img_arr.shape[0]    \n",
    "        annot_train_df.loc[ind, 'img_w'] = img_arr.shape[1]            \n",
    "        annot_train_df.loc[ind, 'class'] = label_class_dict[row['label']]\n",
    "    annot_train_cons_csv_path = os.path.join(dataset_path,\\\n",
    "                                             'Consolidated_Dataset/annot_train_cons.csv')\n",
    "    annot_train_df.to_csv(annot_train_cons_csv_path, index = False)\n",
    "    print(\"annot_train_df saved in path Consolidated_Dataset/annot_train_cons.csv...\")\n",
    "    print()\n",
    "    return annot_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Update test annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def upd_test_ann(dataset_path):\n",
    "    \n",
    "    annot_test_orig_csv_path = os.path.join(dataset_path, 'annot_test_orig.csv')\n",
    "    df_cols = ['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'label']\n",
    "    annot_test_df = pd.read_csv(annot_test_orig_csv_path, header = None, names = df_cols,\\\n",
    "                                 index_col = False)\n",
    "    test_img_cons_path = os.path.join(dataset_path, 'Consolidated_Dataset/test_images/')\n",
    "    print(\"Adding image height, width and image class to annot_test_df...\")\n",
    "    for ind, row in annot_test_df.iterrows():\n",
    "        img_path = os.path.join(test_img_cons_path, row['filename'])\n",
    "        img_arr = mpimg.imread(img_path)\n",
    "        annot_test_df.loc[ind, 'img_h'] = img_arr.shape[0]    \n",
    "        annot_test_df.loc[ind, 'img_w'] = img_arr.shape[1]            \n",
    "        annot_test_df.loc[ind, 'class'] = label_class_dict[row['label']]\n",
    "    annot_test_cons_csv_path = os.path.join(dataset_path,\\\n",
    "                                             'Consolidated_Dataset/annot_test_cons.csv')\n",
    "    annot_test_df.to_csv(annot_test_cons_csv_path, index = False)\n",
    "    print(\"annot_test_df saved in path Consolidated_Dataset/annot_test_cons.csv...\")\n",
    "    print()\n",
    "    return annot_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Copy image files to Reduced_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_img_red_dset(dataset_path, annot_train_df, annot_test_df, num_train_img_red,\\\n",
    "                      num_test_img_red):\n",
    "\n",
    "    train_img_root_path = os.path.join(dataset_path, 'Consolidated_Dataset/train_images')\n",
    "    train_img_dest_path = os.path.join(dataset_path, 'Reduced_Dataset/train_images/')\n",
    "    test_img_root_path = os.path.join(dataset_path, 'Consolidated_Dataset/test_images')\n",
    "    test_img_dest_path = os.path.join(dataset_path, 'Reduced_Dataset/test_images/')\n",
    "\n",
    "    print(\"Copying training set images to Reduced_Dataset/train_images...\")\n",
    "    for df_ind in range(num_train_img_red):\n",
    "        file_path = os.path.join(train_img_root_path, annot_train_df.loc[df_ind, 'filename'])\n",
    "        shutil.copy(file_path, train_img_dest_path)   \n",
    "\n",
    "    print(\"Copying test set images to Reduced_Dataset/test_images...\")\n",
    "    for df_ind in range(num_test_img_red):\n",
    "        file_path = os.path.join(test_img_root_path, annot_test_df.loc[df_ind, 'filename'])\n",
    "        shutil.copy(file_path, test_img_dest_path)    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create annotation files for reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ann_red_dset(dataset_path, num_train_img_per_class, num_test_img_per_class):\n",
    "    \n",
    "    # Define paths of consolidated annotation files for training and test sets\n",
    "    annot_train_cons_csv_path = os.path.join(dataset_path,\\\n",
    "                                             'Consolidated_Dataset/annot_train_cons.csv')\n",
    "    annot_test_cons_csv_path = os.path.join(dataset_path,\\\n",
    "                                             'Consolidated_Dataset/annot_test_cons.csv')\n",
    "    # Read consolidated train and test annotation files into a dataframe\n",
    "    annot_train_cons_df = pd.read_csv(annot_train_cons_csv_path)\n",
    "    annot_test_cons_df = pd.read_csv(annot_test_cons_csv_path)\n",
    "\n",
    "    # Get list of unique classes\n",
    "    class_list = annot_train_cons_df['class'].unique()\n",
    "\n",
    "    # Create place holder for reduced annotation dataframes for training and test sets\n",
    "    annot_train_red_df = pd.DataFrame(columns = annot_train_cons_df.columns)\n",
    "    annot_test_red_df = pd.DataFrame(columns = annot_test_cons_df.columns)\n",
    "\n",
    "    for class_val in class_list:\n",
    "        temp_train_df = annot_train_cons_df[annot_train_cons_df['class'] == class_val]\n",
    "        temp_test_df = annot_test_cons_df[annot_test_cons_df['class'] == class_val]    \n",
    "        annot_train_red_df = annot_train_red_df.append(temp_train_df[0:num_train_img_per_class])\n",
    "        annot_test_red_df = annot_test_red_df.append(temp_test_df[0:num_test_img_per_class])\n",
    "    \n",
    "    annot_train_red_csv_path = os.path.join(dataset_path,\\\n",
    "                                            'Consolidated_Dataset/annot_train_red.csv')\n",
    "    annot_test_red_csv_path = os.path.join(dataset_path,\\\n",
    "                                            'Consolidated_Dataset/annot_test_red.csv')\n",
    "    annot_train_red_df.to_csv(annot_train_red_csv_path, index = False)\n",
    "    print(\"annot_train_red_df saved in path Consolidated_Dataset/annot_train_red.csv...\")\n",
    "    annot_test_red_df.to_csv(annot_test_red_csv_path, index = False)\n",
    "    print(\"annot_test_red_df saved in path Consolidated_Dataset/annot_test_red.csv...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Copy names.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_names_files(dataset_path):\n",
    "    file_path = os.path.join(dataset_path, 'names_orig.csv')\n",
    "    con_dset_path = os.path.join(dataset_path, 'Consolidated_Dataset/class_names.csv')\n",
    "    print(\"Copying names.csv file to Consolidated_Dataset...\")\n",
    "    shutil.copy(file_path, con_dset_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zip_files(dataset_path):\n",
    "    con_dset_zip_file_name = os.path.join(dataset_path, \"Consolidated_Dataset\")\n",
    "    con_dset_dir_name = os.path.join(dataset_path, \"Consolidated_Dataset\")\n",
    "    \n",
    "    print(\"Creating Consolidated_Dataset.zip...\")\n",
    "    shutil.make_archive(con_dset_zip_file_name, 'zip', con_dset_dir_name)    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started preprocessing at 14:36:29. This process will take about 10 - 15 minutes...\n",
      "\n",
      "names_orig.csv file found in dataset_path...\n",
      "annot_train_orig.csv file found in dataset_path...\n",
      "annot_test_orig.csv file found in dataset_path...\n",
      "Training set images path found in dataset_path...\n",
      "Test set images path found in dataset_path...\n",
      "\n",
      "196 folders found in training set images path\n",
      "196 folders found in test set images path\n",
      "\n",
      "8144 images found in training set images path\n",
      "8041 images found in test set images path\n",
      "\n",
      "Consolidated_Dataset folder found: Deleting...\n",
      "\n",
      "Creating Consolidated_Dataset folder...\n",
      "Creating Consolidated_Dataset/train_images folder...\n",
      "Creating Consolidated_Dataset/test_images folder...\n",
      "\n",
      "Consolidating training set images to Consolidated_Dataset/train_images... 8144 images consolidated.\n",
      "Consolidating test set images to Consolidated_Dataset/test_images... 8041 images consolidated.\n",
      "\n",
      "Creating label_class_dict...\n",
      "\n",
      "Adding image height, width and image class to annot_train_df...\n",
      "annot_train_df saved in path Consolidated_Dataset/annot_train_cons.csv...\n",
      "\n",
      "Adding image height, width and image class to annot_test_df...\n",
      "annot_test_df saved in path Consolidated_Dataset/annot_test_cons.csv...\n",
      "\n",
      "annot_train_red_df saved in path Consolidated_Dataset/annot_train_red.csv...\n",
      "annot_test_red_df saved in path Consolidated_Dataset/annot_test_red.csv...\n",
      "\n",
      "Copying names.csv file to Consolidated_Dataset...\n",
      "\n",
      "Creating Consolidated_Dataset.zip...\n",
      "\n",
      "Completed preprocessing at 14:49:50. Elapsed time = 13.4 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Run complete-preprocessing\n",
    "\n",
    "# Define parameters\n",
    "num_train_img_per_class = 7 # #images per class to use for reduced train dataset annotation file\n",
    "num_test_img_per_class = 7 #images per class to use for reduced test dataset annotation file\n",
    "\n",
    "# Start run-timer\n",
    "start_time = datetime.datetime.now(pytz.timezone('Asia/Kolkata'))\n",
    "print(\"Started preprocessing at %s. This process will take about 10 - 15 minutes...\"\\\n",
    "      %(start_time.strftime(\"%H:%M:%S\")))\n",
    "print()\n",
    "\n",
    "# Get some details about original dataset\n",
    "check_orig_data(dataset_path) \n",
    "# Create Consolidated dataset folders\n",
    "create_cons_dset_folders(dataset_path) \n",
    "# Consolidate training and test set images into Consolidated dataset folder\n",
    "cons_img_files(dataset_path) \n",
    "# Create label-class dictionary\n",
    "label_class_dict = create_lbl_cls_dict(dataset_path) \n",
    "# Add image width, height and class to consolidated train_annotation xls\n",
    "annot_train_df = upd_train_ann(dataset_path) \n",
    "# Add image width, height and class to consolidated test_annotation xls\n",
    "annot_test_df = upd_test_ann(dataset_path) \n",
    "# Create reduced train and test annotation xls files\n",
    "create_ann_red_dset(dataset_path, num_train_img_per_class, num_test_img_per_class)\n",
    "# Copy names_files to Consolidate dataset folder\n",
    "copy_names_files(dataset_path)\n",
    "# Zip Consolidated dataset folder\n",
    "create_zip_files(dataset_path)\n",
    "\n",
    "end_time = datetime.datetime.now(pytz.timezone('Asia/Kolkata'))\n",
    "elap_time = ((end_time - start_time).total_seconds())/60\n",
    "print(\"Completed preprocessing at %s. Elapsed time = %0.1f minutes.\"\\\n",
    "      %(end_time.strftime(\"%H:%M:%S\"), elap_time)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
