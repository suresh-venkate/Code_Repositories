{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Bounding Box Dataset Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define root directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.normpath(r'E:\\Sync_With_NAS_Ext\\Datasets\\Image_Datasets\\Stanford_Car_Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Check original data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_orig_data(dataset_path):\n",
    "    \n",
    "    ### Check original data first\n",
    "    names_orig_csv_path = os.path.join(dataset_path, 'names_orig.csv')\n",
    "    annot_train_orig_csv_path = os.path.join(dataset_path, 'annot_train_orig.csv')\n",
    "    annot_test_orig_csv_path = os.path.join(dataset_path, 'annot_test_orig.csv')\n",
    "    train_img_orig_path = os.path.join(dataset_path, 'car_data/train')\n",
    "    test_img_orig_path = os.path.join(dataset_path, 'car_data/test')\n",
    "    train_img_orig_num_fold = len(os.listdir(train_img_orig_path))\n",
    "    test_img_orig_num_fold = len(os.listdir(test_img_orig_path))\n",
    "\n",
    "    num_train_set_images = 0\n",
    "    num_test_set_images = 0\n",
    "    for _, _, files in os.walk(train_img_orig_path):\n",
    "        num_train_set_images += len(files)\n",
    "    for _, _, files in os.walk(test_img_orig_path):\n",
    "        num_test_set_images += len(files)\n",
    "\n",
    "    if (os.path.isfile(names_orig_csv_path)):\n",
    "        print(\"names_orig.csv file found in dataset_path...\")\n",
    "    if (os.path.isfile(annot_train_orig_csv_path)):\n",
    "        print(\"annot_train_orig.csv file found in dataset_path...\")\n",
    "    if (os.path.isfile(annot_test_orig_csv_path)):\n",
    "        print(\"annot_test_orig.csv file found in dataset_path...\")\n",
    "    if (os.path.isdir(train_img_orig_path)):\n",
    "        print(\"Training set images path found in dataset_path...\")\n",
    "    if (os.path.isdir(test_img_orig_path)):\n",
    "        print(\"Test set images path found in dataset_path...\")\n",
    "\n",
    "    print()\n",
    "    print(f\"{train_img_orig_num_fold} folders found in training set images path\")\n",
    "    print(f\"{test_img_orig_num_fold} folders found in test set images path\")\n",
    "    print()\n",
    "    print(f\"{num_train_set_images} images found in training set images path\")\n",
    "    print(f\"{num_test_set_images} images found in test set images path\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders(dataset_path):\n",
    "    \n",
    "    # Delete previously created directories (if any)\n",
    "    cons_dir_path = os.path.join(dataset_path, \"Consolidated_Dataset\")\n",
    "    cons_train_img_dir_path = os.path.join(cons_dir_path, \"train_images\")\n",
    "    cons_test_img_dir_path = os.path.join(cons_dir_path, \"test_images\")\n",
    "    cons_zip_file_path = os.path.join(dataset_path, \"Consolidated_Dataset.zip\")\n",
    "    \n",
    "    red_dir_path = os.path.join(dataset_path, \"Reduced_Dataset\")\n",
    "    red_train_img_dir_path = os.path.join(red_dir_path, \"train_images\")\n",
    "    red_test_img_dir_path = os.path.join(red_dir_path, \"test_images\")\n",
    "    red_zip_file_path = os.path.join(dataset_path, \"Reduced_Dataset.zip\")\n",
    "    if os.path.isdir(cons_dir_path):\n",
    "        print(\"Consolidated_Dataset folder found: Deleting...\")\n",
    "        shutil.rmtree(cons_dir_path)\n",
    "    if os.path.isdir(red_dir_path):\n",
    "        print(\"Reduced_Dataset folder found: Deleting...\")\n",
    "        shutil.rmtree(red_dir_path)\n",
    "    if os.path.isfile(cons_zip_file_path):\n",
    "        print(\"Consolidated_Dataset.zip file found: Deleting...\")\n",
    "        os.remove(cons_zip_file_path)\n",
    "    if os.path.isfile(red_zip_file_path):\n",
    "        print(\"Reduced_Dataset.zip file found: Deleting...\")\n",
    "        os.remove(red_zip_file_path)\n",
    "        \n",
    "    print()    \n",
    "    print(\"Creating Consolidated_Dataset folder...\")\n",
    "    os.mkdir(cons_dir_path)\n",
    "    print(\"Creating Consolidated_Dataset/train_images folder...\")\n",
    "    os.mkdir(cons_train_img_dir_path)    \n",
    "    print(\"Creating Consolidated_Dataset/test_images folder...\")\n",
    "    os.mkdir(cons_test_img_dir_path)\n",
    "    print()    \n",
    "    print(\"Creating Reduced_Dataset folder...\")\n",
    "    os.mkdir(red_dir_path)\n",
    "    print(\"Creating Reduced_Dataset/train_images folder...\")\n",
    "    os.mkdir(red_train_img_dir_path)    \n",
    "    print(\"Creating Reduced_Dataset/test_images folder...\")\n",
    "    os.mkdir(red_test_img_dir_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Consolidate all training and test set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons_img_files(dataset_path):\n",
    "    \n",
    "    # Consolidate training set images\n",
    "    train_img_root_path = os.path.join(dataset_path, 'car_data/train')\n",
    "    train_img_dest_path = os.path.join(dataset_path, 'Consolidated_Dataset/train_images/')\n",
    "    print(\"Consolidating training set images to Consolidated_Dataset/train_images... \",\\\n",
    "          end = '')\n",
    "    for folder in os.listdir(train_img_root_path):\n",
    "        curr_path = os.path.join(train_img_root_path, folder)\n",
    "        for file in os.listdir(curr_path):\n",
    "            file_path = os.path.join(curr_path, file)\n",
    "            shutil.copy(file_path, train_img_dest_path)   \n",
    "    print(\"%d images consolidated.\" %(len(os.listdir(train_img_dest_path))))\n",
    "    \n",
    "    # Consolidate test set images\n",
    "    test_img_root_path = os.path.join(dataset_path, 'car_data/test')\n",
    "    test_img_dest_path = os.path.join(dataset_path, 'Consolidated_Dataset/test_images/')\n",
    "    print(\"Consolidating test set images to Consolidated_Dataset/test_images... \", end = '')\n",
    "    for folder in os.listdir(test_img_root_path):\n",
    "        curr_path = os.path.join(test_img_root_path, folder)\n",
    "        for file in os.listdir(curr_path):\n",
    "            file_path = os.path.join(curr_path, file)\n",
    "            shutil.copy(file_path, test_img_dest_path) \n",
    "    print(\"%d images consolidated.\" %(len(os.listdir(test_img_dest_path))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create label-class dictionary mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lbl_cls_dict(dataset_path):\n",
    "    # Set path of class names csv file\n",
    "    classes_path = os.path.join(dataset_path, 'names_orig.csv') \n",
    "    # Load classes into a dataframe\n",
    "    classes = pd.read_csv(classes_path, header = None, names = ['class'])\n",
    "    # Define empty dictionary to store label_class dictionary mapping\n",
    "    label_class_dict = {}\n",
    "    # Iterate through classes DF and update label_class dictionary mapping\n",
    "    print(\"Creating label_class_dict...\")\n",
    "    for row in classes.iterrows():\n",
    "        label_class_dict[row[0] + 1] = row[1]['class']\n",
    "    \n",
    "    print()\n",
    "    return label_class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Update train annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def upd_train_ann(dataset_path):\n",
    "    \n",
    "    annot_train_orig_csv_path = os.path.join(dataset_path, 'annot_train_orig.csv')\n",
    "    df_cols = ['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'label']\n",
    "    annot_train_df = pd.read_csv(annot_train_orig_csv_path, header = None, names = df_cols,\\\n",
    "                                 index_col = False)\n",
    "    train_img_cons_path = os.path.join(dataset_path, 'Consolidated_Dataset/train_images/')\n",
    "    print(\"Adding image height, width and image class to annot_train_df...\")\n",
    "    for ind, row in annot_train_df.iterrows():\n",
    "        img_path = os.path.join(train_img_cons_path, row['filename'])\n",
    "        img_arr = mpimg.imread(img_path)\n",
    "        annot_train_df.loc[ind, 'img_h'] = img_arr.shape[0]    \n",
    "        annot_train_df.loc[ind, 'img_w'] = img_arr.shape[1]            \n",
    "        annot_train_df.loc[ind, 'class'] = label_class_dict[row['label']]\n",
    "    annot_train_cons_csv_path = os.path.join(dataset_path,\\\n",
    "                                             'Consolidated_Dataset/annot_train_cons.csv')\n",
    "    annot_train_df.to_csv(annot_train_cons_csv_path, index = False)\n",
    "    print(\"annot_train_df saved in path Consolidated_Dataset/annot_train_cons.csv...\")\n",
    "    print()\n",
    "    return annot_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Update test annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def upd_test_ann(dataset_path):\n",
    "    \n",
    "    annot_test_orig_csv_path = os.path.join(dataset_path, 'annot_test_orig.csv')\n",
    "    df_cols = ['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'label']\n",
    "    annot_test_df = pd.read_csv(annot_test_orig_csv_path, header = None, names = df_cols,\\\n",
    "                                 index_col = False)\n",
    "    test_img_cons_path = os.path.join(dataset_path, 'Consolidated_Dataset/test_images/')\n",
    "    print(\"Adding image height, width and image class to annot_test_df...\")\n",
    "    for ind, row in annot_test_df.iterrows():\n",
    "        img_path = os.path.join(test_img_cons_path, row['filename'])\n",
    "        img_arr = mpimg.imread(img_path)\n",
    "        annot_test_df.loc[ind, 'img_h'] = img_arr.shape[0]    \n",
    "        annot_test_df.loc[ind, 'img_w'] = img_arr.shape[1]            \n",
    "        annot_test_df.loc[ind, 'class'] = label_class_dict[row['label']]\n",
    "    annot_test_cons_csv_path = os.path.join(dataset_path,\\\n",
    "                                             'Consolidated_Dataset/annot_test_cons.csv')\n",
    "    annot_test_df.to_csv(annot_test_cons_csv_path, index = False)\n",
    "    print(\"annot_test_df saved in path Consolidated_Dataset/annot_test_cons.csv...\")\n",
    "    print()\n",
    "    return annot_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Copy image files to Reduced_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_img_red_dset(dataset_path, annot_train_df, annot_test_df, num_train_img_red,\\\n",
    "                      num_test_img_red):\n",
    "\n",
    "    train_img_root_path = os.path.join(dataset_path, 'Consolidated_Dataset/train_images')\n",
    "    train_img_dest_path = os.path.join(dataset_path, 'Reduced_Dataset/train_images/')\n",
    "    test_img_root_path = os.path.join(dataset_path, 'Consolidated_Dataset/test_images')\n",
    "    test_img_dest_path = os.path.join(dataset_path, 'Reduced_Dataset/test_images/')\n",
    "\n",
    "    print(\"Copying training set images to Reduced_Dataset/train_images...\")\n",
    "    for df_ind in range(num_train_img_red):\n",
    "        file_path = os.path.join(train_img_root_path, annot_train_df.loc[df_ind, 'filename'])\n",
    "        shutil.copy(file_path, train_img_dest_path)   \n",
    "\n",
    "    print(\"Copying test set images to Reduced_Dataset/test_images...\")\n",
    "    for df_ind in range(num_test_img_red):\n",
    "        file_path = os.path.join(test_img_root_path, annot_test_df.loc[df_ind, 'filename'])\n",
    "        shutil.copy(file_path, test_img_dest_path)    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create annotation files for reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ann_red_dset(dataset_path, annot_train_df, annot_test_df, num_train_img_red,\\\n",
    "                        num_test_img_red):\n",
    "    print(\"creating train and test annotation csv files for reduced dataset...\")\n",
    "    annot_train_df_red = annot_train_df[0:num_train_img_red].copy()\n",
    "    annot_test_df_red = annot_test_df[0:num_test_img_red].copy()    \n",
    "    annot_train_red_csv_path = os.path.join(dataset_path, \"Reduced_Dataset/annot_train_red.csv\")\n",
    "    annot_test_red_csv_path = os.path.join(dataset_path, \"Reduced_Dataset/annot_test_red.csv\")\n",
    "    annot_train_df_red.to_csv(annot_train_red_csv_path, index = False)\n",
    "    annot_test_df_red.to_csv(annot_test_red_csv_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Copy names.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_names_files(dataset_path):\n",
    "    file_path = os.path.join(dataset_path, 'names_orig.csv')\n",
    "    con_dset_path = os.path.join(dataset_path, 'Consolidated_Dataset/class_names.csv')\n",
    "    red_dset_path = os.path.join(dataset_path, 'Reduced_Dataset/class_names.csv')\n",
    "    print(\"Copying names.csv file to Consolidated_Dataset and Reduced Dataset...\")\n",
    "    shutil.copy(file_path, con_dset_path)\n",
    "    shutil.copy(file_path, red_dset_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Create zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zip_files(dataset_path):\n",
    "    con_dset_zip_file_name = os.path.join(dataset_path, \"Consolidated_Dataset\")\n",
    "    con_dset_dir_name = os.path.join(dataset_path, \"Consolidated_Dataset\")\n",
    "    \n",
    "    red_dset_zip_file_name = os.path.join(dataset_path, \"Reduced_Dataset\")\n",
    "    red_dset_dir_name = os.path.join(dataset_path, \"Reduced_Dataset\")\n",
    "    \n",
    "    print(\"Creating Consolidated_Dataset.zip and Reduced_Dataset.zip...\")\n",
    "    #shutil.make_archive(con_dset_zip_file_name, 'zip', con_dset_dir_name)    \n",
    "    shutil.make_archive(red_dset_zip_file_name, 'zip', red_dset_dir_name)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started preprocessing at 20:35:12. This process will take about 10 - 15 minutes...\n",
      "\n",
      "names_orig.csv file found in dataset_path...\n",
      "annot_train_orig.csv file found in dataset_path...\n",
      "annot_test_orig.csv file found in dataset_path...\n",
      "Training set images path found in dataset_path...\n",
      "Test set images path found in dataset_path...\n",
      "\n",
      "196 folders found in training set images path\n",
      "196 folders found in test set images path\n",
      "\n",
      "8144 images found in training set images path\n",
      "8041 images found in test set images path\n",
      "\n",
      "Consolidated_Dataset folder found: Deleting...\n",
      "Reduced_Dataset folder found: Deleting...\n",
      "\n",
      "Creating Consolidated_Dataset folder...\n",
      "Creating Consolidated_Dataset/train_images folder...\n",
      "Creating Consolidated_Dataset/test_images folder...\n",
      "\n",
      "Creating Reduced_Dataset folder...\n",
      "Creating Reduced_Dataset/train_images folder...\n",
      "Creating Reduced_Dataset/test_images folder...\n",
      "\n",
      "Consolidating training set images to Consolidated_Dataset/train_images... 8144 images consolidated.\n",
      "Consolidating test set images to Consolidated_Dataset/test_images... 8041 images consolidated.\n",
      "\n",
      "Creating label_class_dict...\n",
      "\n",
      "Adding image height, width and image class to annot_train_df...\n",
      "annot_train_df saved in path Consolidated_Dataset/annot_train_cons.csv...\n",
      "\n",
      "Adding image height, width and image class to annot_test_df...\n",
      "annot_test_df saved in path Consolidated_Dataset/annot_test_cons.csv...\n",
      "\n",
      "Copying training set images to Reduced_Dataset/train_images...\n",
      "Copying test set images to Reduced_Dataset/test_images...\n",
      "\n",
      "creating train and test annotation csv files for reduced dataset...\n",
      "Completed preprocessing at 20:46:19. Elapsed time = 11.1 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Run complete-preprocessing\n",
    "\n",
    "# Define parameters\n",
    "create_red_dset_flag = True\n",
    "num_train_img_red = 1500\n",
    "num_test_img_red = 100\n",
    "\n",
    "start_time = datetime.datetime.now(pytz.timezone('Asia/Kolkata'))\n",
    "print(\"Started preprocessing at %s. This process will take about 10 - 15 minutes...\"\\\n",
    "      %(start_time.strftime(\"%H:%M:%S\")))\n",
    "print()\n",
    "\n",
    "check_orig_data(dataset_path)\n",
    "create_folders(dataset_path)\n",
    "cons_img_files(dataset_path)\n",
    "label_class_dict = create_lbl_cls_dict(dataset_path)\n",
    "annot_train_df = upd_train_ann(dataset_path)\n",
    "annot_test_df = upd_test_ann(dataset_path)\n",
    "if (create_red_dset_flag):\n",
    "    copy_img_red_dset(dataset_path, annot_train_df, annot_test_df, num_train_img_red,\\\n",
    "                      num_test_img_red)\n",
    "    create_ann_red_dset(dataset_path, annot_train_df, annot_test_df, num_train_img_red,\\\n",
    "                        num_test_img_red)\n",
    "copy_names_files(dataset_path)\n",
    "create_zip_files(dataset_path)\n",
    "\n",
    "end_time = datetime.datetime.now(pytz.timezone('Asia/Kolkata'))\n",
    "elap_time = ((end_time - start_time).total_seconds())/60\n",
    "print(\"Completed preprocessing at %s. Elapsed time = %0.1f minutes.\"\\\n",
    "      %(end_time.strftime(\"%H:%M:%S\"), elap_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of training set images is %d\" %(annot_train_df.shape[0]))\n",
    "print(\"Number of unique labels in training set is %d\" %(len(annot_train_df.label.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annot_train_df_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with simpler version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.normpath(r'E:\\Sync_With_NAS_Ext\\Datasets\\Image_Datasets\\Stanford_Car_Dataset\\Temp_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_train_csv_path = os.path.join(dataset_path, 'anno_train.csv')\n",
    "df_cols = ['filename', 'xmin', 'xmax', 'ymin', 'ymax', 'label']\n",
    "ann_train_df = pd.read_csv(ann_train_csv_path, header = None, names = df_cols, index_col = False)\n",
    "ann_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, row in ann_train_df.iterrows():\n",
    "    ann_train_df.loc[ind, 'class'] = label_class_dict[row['label']]\n",
    "\n",
    "ann_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_train_df.to_csv('ann_train.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = os.path.join(dataset_path, 'train_img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now(pytz.timezone('Asia/Kolkata'))\n",
    "print(\"Started train.tfrecord creation at %s\" %(start_time.strftime(\"%H:%M:%S\")), end = '; ')\n",
    "\n",
    "!python generate_tfrecord.py --csv_input=ann_train.csv --img_path={train_img_path} --output_path=train.tfrecord\n",
    "\n",
    "end_time = datetime.datetime.now(pytz.timezone('Asia/Kolkata'))\n",
    "elap_time = ((end_time - start_time).total_seconds())/60\n",
    "print(\"Completed at %s. Elapsed time = %0.2f minutes.\" %(end_time.strftime(\"%H:%M:%S\"), elap_time)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(dataset_path, 'Car_data/car_data/train/Audi TTS Coupe 2012/00001.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path)\n",
    "xmin = ann_train_df.iloc[0, 1]\n",
    "ymin = ann_train_df.iloc[0, 2]\n",
    "xmax = ann_train_df.iloc[0, 3]\n",
    "ymax = ann_train_df.iloc[0, 4]\n",
    "cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color = (0, 255, 0), thickness = 4)\n",
    "\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.imshow(img_rgb);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
