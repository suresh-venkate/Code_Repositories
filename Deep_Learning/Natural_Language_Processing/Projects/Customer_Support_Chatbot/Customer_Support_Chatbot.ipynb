{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Interactive-chatbot\" data-toc-modified-id=\"Interactive-chatbot-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Interactive chatbot</a></span></li><li><span><a href=\"#Import-required-libraries\" data-toc-modified-id=\"Import-required-libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import required libraries</a></span></li><li><span><a href=\"#Load-corpus\" data-toc-modified-id=\"Load-corpus-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load corpus</a></span></li><li><span><a href=\"#Study-corpus\" data-toc-modified-id=\"Study-corpus-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Study corpus</a></span></li><li><span><a href=\"#Define-functions\" data-toc-modified-id=\"Define-functions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Define functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function:-Get-list-of-all-characters-in-a-corpus\" data-toc-modified-id=\"Function:-Get-list-of-all-characters-in-a-corpus-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Function: Get list of all characters in a corpus</a></span></li><li><span><a href=\"#Function:-Pre-process-features\" data-toc-modified-id=\"Function:-Pre-process-features-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Function: Pre-process features</a></span></li></ul></li><li><span><a href=\"#Generate-training-set---Features-and-labels\" data-toc-modified-id=\"Generate-training-set---Features-and-labels-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Generate training set - Features and labels</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-corpus_docs-and-corpus_tags\" data-toc-modified-id=\"Create-corpus_docs-and-corpus_tags-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Create corpus_docs and corpus_tags</a></span></li><li><span><a href=\"#Pre-process-features\" data-toc-modified-id=\"Pre-process-features-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Pre-process features</a></span></li><li><span><a href=\"#Generate-BBOW\" data-toc-modified-id=\"Generate-BBOW-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Generate BBOW</a></span></li><li><span><a href=\"#Create-X_train-and-y_train\" data-toc-modified-id=\"Create-X_train-and-y_train-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Create X_train and y_train</a></span></li></ul></li><li><span><a href=\"#Model---Define,-compile-and-train\" data-toc-modified-id=\"Model---Define,-compile-and-train-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Model - Define, compile and train</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-model\" data-toc-modified-id=\"Define-model-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Define model</a></span></li><li><span><a href=\"#Compile-model\" data-toc-modified-id=\"Compile-model-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Compile model</a></span></li><li><span><a href=\"#Fit-and-evaluate-model\" data-toc-modified-id=\"Fit-and-evaluate-model-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Fit and evaluate model</a></span></li></ul></li><li><span><a href=\"#Define-chat-function\" data-toc-modified-id=\"Define-chat-function-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Define chat function</a></span></li><li><span><a href=\"#Execute-Chat\" data-toc-modified-id=\"Execute-Chat-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Execute Chat</a></span><ul class=\"toc-item\"><li><span><a href=\"#Start-->-Intro-->-Quit\" data-toc-modified-id=\"Start-->-Intro-->-Quit-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Start -&gt; Intro -&gt; Quit</a></span></li><li><span><a href=\"#Multiple-unrecognized-queries\" data-toc-modified-id=\"Multiple-unrecognized-queries-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Multiple unrecognized queries</a></span></li><li><span><a href=\"#Normal-query-flow\" data-toc-modified-id=\"Normal-query-flow-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Normal query flow</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive chatbot\n",
    "\n",
    "Problem Statement: Build a python based interactive semi-rule based chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"GL_Bot.json\") as file:\n",
    "    corpus = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tag', 'patterns', 'responses', 'context_set'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of keys in corpus['intents']\n",
    "corpus['intents'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag:\n",
      "====\n",
      "Intro\n",
      "\n",
      "Patterns:\n",
      "=========\n",
      "['hi', 'how are you', 'is anyone there', 'hello', 'whats up', 'hey', 'yo', 'listen', 'please help me', 'i am learner from', 'i belong to', 'aiml batch', 'aifl batch', 'i am from', 'my pm is', 'blended', 'online', 'i am from', 'hey ya', 'talking to you for first time', 'i need help']\n",
      "\n",
      "Responses:\n",
      "==========\n",
      "['Hello! how can i help you?']\n"
     ]
    }
   ],
   "source": [
    "# Look at one entry of corpus['intents']\n",
    "print(\"Tag:\")\n",
    "print(\"====\")\n",
    "print(corpus['intents'][0]['tag'])\n",
    "print()\n",
    "print(\"Patterns:\")\n",
    "print(\"=========\")\n",
    "print(corpus['intents'][0]['patterns'])\n",
    "print()\n",
    "print(\"Responses:\")\n",
    "print(\"==========\")\n",
    "print(corpus['intents'][0]['responses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tags in corpus is 8\n",
      "\n",
      "List of unique tags in corpus:\n",
      "==============================\n",
      "['Intro', 'Exit', 'Olympus', 'SL', 'NN', 'Bot', 'Profane', 'Ticket']\n",
      "\n",
      "Total number of unique patterns in corpus is 168\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique tags in corpus is %d\" %(len(corpus['intents'])))\n",
    "print()\n",
    "print(\"List of unique tags in corpus:\")\n",
    "print(\"==============================\")\n",
    "print([intent['tag'] for intent in corpus['intents']])\n",
    "print()\n",
    "print(\"Total number of unique patterns in corpus is %d\" \\\n",
    "      %(sum([len(intent['patterns']) for intent in corpus['intents']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>Number of unique patterns</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intro</td>\n",
       "      <td>21</td>\n",
       "      <td>Hello! how can i help you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exit</td>\n",
       "      <td>20</td>\n",
       "      <td>I hope I was able to assist you, Good Bye!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Olympus</td>\n",
       "      <td>14</td>\n",
       "      <td>Please refer to this link: https://www.greatle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SL</td>\n",
       "      <td>36</td>\n",
       "      <td>Please refer to this link: https://en.wikipedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NN</td>\n",
       "      <td>44</td>\n",
       "      <td>Please refer to this link: https://en.wikipedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bot</td>\n",
       "      <td>10</td>\n",
       "      <td>I am your virtual learning assistant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Profane</td>\n",
       "      <td>9</td>\n",
       "      <td>Please use respectful words!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ticket</td>\n",
       "      <td>14</td>\n",
       "      <td>A ticket has been filed. Please expect a respo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tag Number of unique patterns  \\\n",
       "0    Intro                        21   \n",
       "1     Exit                        20   \n",
       "2  Olympus                        14   \n",
       "3       SL                        36   \n",
       "4       NN                        44   \n",
       "5      Bot                        10   \n",
       "6  Profane                         9   \n",
       "7   Ticket                        14   \n",
       "\n",
       "                                            Response  \n",
       "0                         Hello! how can i help you?  \n",
       "1         I hope I was able to assist you, Good Bye!  \n",
       "2  Please refer to this link: https://www.greatle...  \n",
       "3  Please refer to this link: https://en.wikipedi...  \n",
       "4  Please refer to this link: https://en.wikipedi...  \n",
       "5               I am your virtual learning assistant  \n",
       "6                       Please use respectful words!  \n",
       "7  A ticket has been filed. Please expect a respo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get some statistics for each tag\n",
    "cols_list = ['Tag', 'Number of unique patterns', 'Response']\n",
    "corpus_df = pd.DataFrame(columns = cols_list)\n",
    "for intent in corpus['intents']:\n",
    "    entry_dict = {'Tag': intent['tag'],\n",
    "                  'Number of unique patterns': len(intent['patterns']),\n",
    "                  'Response': intent['responses'][0]\n",
    "                 }\n",
    "    corpus_df = corpus_df.append(entry_dict, ignore_index = True)\n",
    "display(corpus_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Get list of all characters in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_list(corpus):\n",
    "    \n",
    "    '''\n",
    "    Get a list of lexicographically sorted characters present in corpus. Each document in corpus\n",
    "    will be converted to lower-case before generating this list\n",
    "    \n",
    "    Arguments:\n",
    "        corpus - Input corpus from which list of characters has to be extracted.\n",
    "    \n",
    "    Returns: \n",
    "        List of all characters in corpus sorted lexicographically.\n",
    "    '''\n",
    "    \n",
    "    all_char_pattern = re.compile(r'.') # Match all characters in corpus except newline\n",
    "    all_char_list = [] # Placeholder to store all characters in corpus\n",
    "\n",
    "    for doc in corpus: # Loop through all documents in corpus and extract all characters\n",
    "        doc = doc.lower() # Convert document to lower-case\n",
    "        matches = all_char_pattern.finditer(doc)\n",
    "        for mat in matches:\n",
    "            all_char_list.append(mat.group(0)) # Append characters from current document\n",
    "\n",
    "    # Get unique characters, convert to list and sort.\n",
    "    uniq_char_list = sorted(list(set(all_char_list)))\n",
    "    \n",
    "    print()\n",
    "    print(\"Number of unique characters is %d\" %len(uniq_char_list))\n",
    "    print()\n",
    "    print(\"List of unique characters:\")\n",
    "    print(\"==========================\")\n",
    "    print(\"{}\".format(uniq_char_list))\n",
    "\n",
    "    return uniq_char_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Pre-process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_feat(corpus, stemmer):\n",
    "    \n",
    "    '''\n",
    "    Pre-process each document in corpus. Includes the following:\n",
    "        1. Remove html tags\n",
    "        2. Delete single-quote characters\n",
    "        3. Replace miscellaneous characters with whitespace\n",
    "        4. Split document into words, perform stemming and merge stemmed words back into document\n",
    "    \n",
    "    Arguments:\n",
    "        corpus - Input corpus.\n",
    "        stemmer - Stemmer object to use for performing stemming\n",
    "    \n",
    "    Returns: \n",
    "        Pre-processed corpus.    \n",
    "    '''\n",
    "\n",
    "    corpus_docs_processed = [] # Placeholder to store processed documents from corpus\n",
    "    for doc in corpus: # Loop through all documents in corpus\n",
    "        # Convert document to all lower-case\n",
    "        doc_proc = doc.lower()\n",
    "\n",
    "        # Search for html tags in the current document (if any) and replace them with space\n",
    "        html_tag_patt = re.compile('<.*?>')\n",
    "        doc_proc = re.sub(html_tag_patt, ' ', doc_proc)\n",
    "\n",
    "        # Search for quote character (if any) and delete it\n",
    "        # This will make words like don't = dont and won't = wont\n",
    "        pattern_1 = re.compile(r'[\\']')\n",
    "        doc_proc = re.sub(pattern_1, '', doc_proc)\n",
    "\n",
    "        # Search for miscellaneous characters (if any) and replace them with whitespace\n",
    "        # Standard symbols in QWErtY keyboard to be replaced with whitespace\n",
    "        pattern_2 = re.compile(r'[`|~|!|@|#|$|%|^|&|*|(|)|\\-|_|=|+|\\[|\\]|{|}|\\|;|:|\"|,|.|/|<|>|?]')\n",
    "        doc_proc = re.sub(pattern_2, ' ', doc_proc)\n",
    "\n",
    "        # Split document into list of words, perform stemming on each word\n",
    "        words = [stemmer.stem(word) for word in doc_proc.split()]  \n",
    "        # Concatenate words back into a document\n",
    "        doc_proc = ''\n",
    "        for word in words:\n",
    "            doc_proc = doc_proc + word + ' '\n",
    "\n",
    "        # Append current processed document to corpus_docs_processed\n",
    "        corpus_docs_processed.append(doc_proc)\n",
    "\n",
    "    return corpus_docs_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate training set - Features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create corpus_docs and corpus_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus is 168\n",
      "Number of unique tags in corpus is 8\n",
      "\n",
      "List of tags:\n",
      "=============\n",
      "['Bot', 'Exit', 'Intro', 'NN', 'Olympus', 'Profane', 'SL', 'Ticket']\n",
      "\n",
      "Number of unique characters is 27\n",
      "\n",
      "List of unique characters:\n",
      "==========================\n",
      "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "corpus_docs = [] # Placeholder to store all documents of the corpus for training set\n",
    "corpus_tags = [] # Placeholder to store all tags of the corpus for training set\n",
    "for intent in corpus['intents']: # Loop through all intents in corpus\n",
    "    for pattern in intent['patterns']: # Loop through all patterns in each intent\n",
    "        corpus_docs.append(pattern) # Append current pattern to corpus_docs\n",
    "        corpus_tags.append(intent['tag']) # Append current tag to corpus_tags\n",
    "unique_tags = sorted(list(set(corpus_tags))) # Get list of unique tags\n",
    "\n",
    "print(\"Number of documents in corpus is %d\" %len(corpus_docs))\n",
    "print(\"Number of unique tags in corpus is %d\" %len(unique_tags))\n",
    "print()\n",
    "print(\"List of tags:\")\n",
    "print(\"=============\")\n",
    "print(\"{}\".format(unique_tags))\n",
    "\n",
    "# Get list of unique characters in corpus_docs\n",
    "uniq_char_list = get_char_list(corpus_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() # Instantiate Porter Stemmer\n",
    "corpus_docs_processed = preprocess_feat(corpus_docs, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'how are you', 'is anyone there', 'hello', 'whats up', 'hey', 'yo', 'listen', 'please help me', 'i am learner from']\n",
      "\n",
      "['hi ', 'how are you ', 'is anyon there ', 'hello ', 'what up ', 'hey ', 'yo ', 'listen ', 'pleas help me ', 'i am learner from ']\n"
     ]
    }
   ],
   "source": [
    "# Check corpus_docs_processed\n",
    "print(corpus_docs[0:10]) # First 10 documents of original corpus\n",
    "print()\n",
    "print(corpus_docs_processed[0:10]) # First 10 documents of preprocessed corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate BBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 201\n",
      "\n",
      "First 20 features:\n",
      "['abl', 'about', 'access', 'activ', 'ada', 'adam', 'ai', 'aifl', 'aiml', 'am', 'an', 'ann', 'anyon', 'are', 'artifici', 'back', 'backprop', 'backward', 'bad', 'bag']\n",
      "\n",
      "Shape of X_bbow is (168, 201)\n"
     ]
    }
   ],
   "source": [
    "# Binary BOW count vectorizer\n",
    "vect_bbow = CountVectorizer(binary = True) \n",
    "X_bbow = vect_bbow.fit_transform(corpus_docs_processed)\n",
    "feat_names_bbow = vect_bbow.get_feature_names()\n",
    "\n",
    "print(\"Number of features: {}\".format(len(feat_names_bbow)))\n",
    "print()\n",
    "print(\"First 20 features:\\n{}\".format(feat_names_bbow[:20]))\n",
    "print()\n",
    "print(\"Shape of X_bbow is {}\".format(X_bbow.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of X_train is <class 'numpy.ndarray'>\n",
      "Type of y_train is <class 'numpy.ndarray'>\n",
      "\n",
      "Length of X_train is (168, 201)\n",
      "Length of y_train is (168, 8)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_bbow.toarray() # Convert sparse bbow array to normal numpy array\n",
    "y_train = np.array(pd.get_dummies(corpus_tags)) # Peform One-hot encoding on corpus_tags\n",
    "print(\"Type of X_train is {}\".format(type(X_train)))\n",
    "print(\"Type of y_train is {}\".format(type(y_train)))\n",
    "print()\n",
    "print(\"Length of X_train is {}\".format(X_train.shape))\n",
    "print(\"Length of y_train is {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Define, compile and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                12928     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 15,272\n",
      "Trainable params: 15,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a simple NN model to fit the above training set\n",
    "# Input -> Dense(64, ReLu) -> Dropout(0.5) -> Dense(32, ReLu) -> Dropout(0.5) -> Dense(8, softmax)\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim = X_train.shape[1], activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer\n",
    "opt = optimizers.Adam() \n",
    "# Define Loss = CategoricalCrossEntropy\n",
    "cce = keras.losses.CategoricalCrossentropy() \n",
    "# Define Metric = Categorical Accuracy\n",
    "met = keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "model.compile(optimizer = opt, loss = cce, metrics = met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved accuracy on training set is 100.00 %\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, epochs = 200, verbose = 0)\n",
    "acc = model.evaluate(X_train, y_train, return_dict = True, verbose = 0)['categorical_accuracy']\n",
    "print(\"Achieved accuracy on training set is %0.2f %%\" %(100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define chat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    \n",
    "    '''\n",
    "    Function to execute chat\n",
    "    '''\n",
    "\n",
    "    # Define a few response strings (not present in the corpus)\n",
    "    stmt_1 = \"Bot: Chat started: To exit the chat, type quit\"\n",
    "    stmt_2 = \"Bot: Thank you. Have a good day!\"\n",
    "    stmt_3 = \"\\nBot: Is there anything else I can help you with? If not, type quit to exit.\"\n",
    "    stmt_4 = \"Bot: Sorry, I could not understand your question.\"\n",
    "    stmt_5 = \" Please rephrase your question.\"    \n",
    "    stmt_6 = \"Bot: You have exceeded the maximum number of attempts.\"\n",
    "    stmt_7 = \" A ticket has been filed. Please expect a response within 24 hours.\"    \n",
    "    \n",
    "    print(stmt_1) # Statment initializing chat\n",
    "    print()\n",
    "    \n",
    "    # Initialize counter to keep track of number of questions that are not understood\n",
    "    not_understood_count = 0\n",
    "    \n",
    "    while True: # Loop infinitely unless user requests to exit\n",
    "        inp = input(\"\\nYou: \") # Get input from user\n",
    "        if (inp.lower() == \"quit\"): # Exit chat if user types 'quit'\n",
    "            print(stmt_2)\n",
    "            break \n",
    "        else: # If user input is not 'quit'\n",
    "            # Pre-process and vectorize input           \n",
    "            inp_arr = vect_bbow.transform(preprocess_feat([inp], stemmer)).toarray()\n",
    "            # Check if input has any words from the trained vocabulary\n",
    "            num_words_vocab = np.sum(inp_arr[0] == 1)\n",
    "            # If input has no words from vocab, ask user to repeat question\n",
    "            # If this happens thrice in a row, file a ticket\n",
    "            if (num_words_vocab == 0):\n",
    "                not_understood_count += 1\n",
    "                if (not_understood_count < 3):\n",
    "                    print(stmt_4 + stmt_5)\n",
    "                else: # If question has not been understood three times, file a ticket\n",
    "                    not_understood_count = 0 # Reset not_understood_count\n",
    "                    print(stmt_6 + stmt_7)\n",
    "                    print(stmt_3)\n",
    "            else: # Input has some words from vocab\n",
    "                # Reset not_understood_count if input has some word from vocabulary\n",
    "                not_understood_count = 0 \n",
    "                \n",
    "                model_out = model.predict(inp_arr) # Get model prediction\n",
    "                tag_index = np.argmax(model_out) # Get index of predicted tag\n",
    "                tag = unique_tags[tag_index] # Extract predicted tag\n",
    "                \n",
    "                # Extract responses corresponding to predicted tag from corpus\n",
    "                for intent in corpus['intents']:\n",
    "                    if intent['tag'] == tag:\n",
    "                        responses = intent['responses']\n",
    "                print(\"Bot: \" + np.random.choice(responses))\n",
    "                if (tag.lower() in [\"olympus\", 'nn', 'sl', 'ticket']):\n",
    "                    print(stmt_3)\n",
    "                if (tag.lower() == 'exit'):\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start -> Intro -> Quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Chat started: To exit the chat, type quit\n",
      "\n",
      "\n",
      "You: hi\n",
      "Bot: Hello! how can i help you?\n",
      "\n",
      "You: quit\n",
      "Bot: Thank you. Have a good day!\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple unrecognized queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Chat started: To exit the chat, type quit\n",
      "\n",
      "\n",
      "You: hello\n",
      "Bot: Hello! how can i help you?\n",
      "\n",
      "You: saldkfjalsj\n",
      "Bot: Sorry, I could not understand your question. Please rephrase your question.\n",
      "\n",
      "You: oiqwupiue\n",
      "Bot: Sorry, I could not understand your question. Please rephrase your question.\n",
      "\n",
      "You: lkjelw;rkjq\n",
      "Bot: You have exceeded the maximum number of attempts. A ticket has been filed. Please expect a response within 24 hours.\n",
      "\n",
      "Bot: Is there anything else I can help you with? If not, type quit to exit.\n",
      "\n",
      "You: quit\n",
      "Bot: Thank you. Have a good day!\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal query flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Chat started: To exit the chat, type quit\n",
      "\n",
      "\n",
      "You: hello\n",
      "Bot: Hello! how can i help you?\n",
      "\n",
      "You: please tell me about yourselves\n",
      "Bot: I am your virtual learning assistant\n",
      "\n",
      "You: I want to know about clustering\n",
      "Bot: Please refer to this link: https://en.wikipedia.org/wiki/Machine_learning\n",
      "\n",
      "Bot: Is there anything else I can help you with? If not, type quit to exit.\n",
      "\n",
      "You: Can you also tell me about neural networks?\n",
      "Bot: Please refer to this link: https://en.wikipedia.org/wiki/Neural_network\n",
      "\n",
      "Bot: Is there anything else I can help you with? If not, type quit to exit.\n",
      "\n",
      "You: thank you\n",
      "Bot: I hope I was able to assist you, Good Bye!\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
