{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Transformer_From_Scratch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suresh-venkate/Code_Repositories/blob/main/Deep_Learning/Natural_Language_Processing/Transformer_Models/Transformer_From_Scratch/Transformer_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI075CzzZS35"
      },
      "source": [
        "# Transformer from scratch\n",
        "\n",
        "* Reference: Attention is all you need, NIPS 2017\n",
        "* Authors: Ashish Vaswani et.al.\n",
        "* Link to paper: [Link](https://arxiv.org/abs/1706.03762)\n",
        "* All references in the code below refer to this paper unless stated otherwise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWbH-n-mYZzA"
      },
      "source": [
        "# Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lDHMlknmKxK"
      },
      "source": [
        "%%capture \n",
        "!wget https://raw.githubusercontent.com/suresh-venkate/Code_Repositories/main/Deep_Learning/Natural_Language_Processing/Transformer_Models/Transformer_From_Scratch/transformer_utils.py\n",
        "!pip install torchsummaryX"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MThcSvYYcig"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math, copy, time\n",
        "import seaborn\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary as ts_summary\n",
        "from torchsummaryX import summary as summaryx\n",
        "from transformer_utils import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ru3h5Q-5hN"
      },
      "source": [
        "# Backup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwJnfYQeIQGp"
      },
      "source": [
        "## Verify Function: clones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXw8VvRl9Mxm",
        "outputId": "3950e566-5d6c-480a-bad5-d7c4c350aa21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class TempModel(nn.Module):\n",
        "  def __init__(self, layer, N):\n",
        "    super(TempModel, self).__init__()\n",
        "    self.layers = clones(layer, N)\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lin_layer = nn.Linear(128, 128)\n",
        "temp_model = TempModel(lin_layer, 4).to(device) # Clone linear layer 4 times\n",
        "ts_summary(temp_model, input_size = (128,))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 128]          16,512\n",
            "            Linear-2                  [-1, 128]          16,512\n",
            "            Linear-3                  [-1, 128]          16,512\n",
            "            Linear-4                  [-1, 128]          16,512\n",
            "================================================================\n",
            "Total params: 66,048\n",
            "Trainable params: 66,048\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.25\n",
            "Estimated Total Size (MB): 0.26\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrzNUSB33Z6S"
      },
      "source": [
        "## Verify Class: Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvcLFj463cZC",
        "outputId": "ed343223-9763-465a-c7d9-0dae740c86ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "src_vocab = 11 # 0 to 10\n",
        "d_model = 512\n",
        "nb = 2 # Batch Size\n",
        "nw = 5 # Number of words (or positions) in the input\n",
        "temp_model = Embeddings(d_model, src_vocab).to(device)\n",
        "\n",
        "# Generate nw input samples of length 10 each\n",
        "input = torch.randint(0, 10, (nb, nw)).to(device)\n",
        "output = temp_model(input)\n",
        "\n",
        "print(input.shape, output.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5]) torch.Size([2, 5, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (2, 5)\n",
        "x_sample = torch.zeros(input_size, dtype=torch.long, device=torch.device('cuda'))\n",
        "summaryx(temp_model, x_sample)"
      ],
      "metadata": {
        "id": "WShSz0_6jFA6",
        "outputId": "3adb01ad-d2d8-4b7d-b807-3bdc0756416a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "      Kernel Shape Output Shape  Params  Mult-Adds\n",
            "Layer                                             \n",
            "0_emb    [512, 11]  [2, 5, 512]    5632       5632\n",
            "--------------------------------------------------\n",
            "                      Totals\n",
            "Total params            5632\n",
            "Trainable params        5632\n",
            "Non-trainable params       0\n",
            "Mult-Adds               5632\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Kernel Shape Output Shape  Params  Mult-Adds\n",
              "Layer                                             \n",
              "0_emb    [512, 11]  [2, 5, 512]    5632       5632"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fea58e20-64e2-4ff9-b99e-e68c3934f498\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_emb</th>\n",
              "      <td>[512, 11]</td>\n",
              "      <td>[2, 5, 512]</td>\n",
              "      <td>5632</td>\n",
              "      <td>5632</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fea58e20-64e2-4ff9-b99e-e68c3934f498')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fea58e20-64e2-4ff9-b99e-e68c3934f498 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fea58e20-64e2-4ff9-b99e-e68c3934f498');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV4dQw7_6RDw"
      },
      "source": [
        "## Verify Class: PositionalEncoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RR2dxTMAgmO",
        "outputId": "625d681d-9f94-43a9-c215-6098334edc1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "d_model = 20 # Embedding Size\n",
        "nb = 1 # Batch Size\n",
        "nw = 100 # Number of words (or positions) in the input\n",
        "pe = PositionalEncoding(d_model, dropout = 0)\n",
        "input = torch.zeros(nb, nw, d_model)\n",
        "output = pe.forward(input)\n",
        "\n",
        "print(input.shape, output.shape)\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(np.arange(100), output[0, :, 4:8].numpy())\n",
        "plt.legend([\"dim %d\" %p for p in [4, 5, 6, 7]])\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 100, 20]) torch.Size([1, 100, 20])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R8ANPCHRyy9"
      },
      "source": [
        "## Verify Class: ScaledDotProductAttention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMYg-_GduO1R",
        "outputId": "54484458-8fd3-4d22-8228-27d6ad87f2d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "nb = 2 # Batch Size\n",
        "h = 8 # Number of attention heads\n",
        "nw = 5 # Number of words (or positions) in the input\n",
        "dk = 64 # Dimension of Q, K and V vectors\n",
        "scaling = dk ** 0.5\n",
        "\n",
        "attn_dropout = nn.Dropout(p = 0) # Define dropout layer\n",
        "\n",
        "temp_model = ScaledDotProductAttention(scaling).to(device) # SDPA layer\n",
        "\n",
        "Q = torch.randint(low = 0, high = 10, size = (nb, h, nw, dk), dtype = torch.float32)\n",
        "K = torch.clone(Q)\n",
        "V = torch.clone(Q)\n",
        "\n",
        "attn, probs = temp_model(Q, K, V, None, attn_dropout)\n",
        "attn = torch.round(attn)\n",
        "\n",
        "print(Q.shape, attn.shape, probs.shape)\n",
        "print()\n",
        "print(Q[0, 0, :, :5])\n",
        "print()\n",
        "print(probs[0, 0])\n",
        "print()\n",
        "print(attn[0, 0, :, :5])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 8, 5, 64]) torch.Size([2, 8, 5, 64]) torch.Size([2, 8, 5, 5])\n",
            "\n",
            "tensor([[4., 5., 8., 1., 2.],\n",
            "        [7., 2., 9., 2., 3.],\n",
            "        [2., 3., 5., 6., 9.],\n",
            "        [1., 8., 0., 6., 3.],\n",
            "        [8., 2., 8., 6., 5.]])\n",
            "\n",
            "tensor([[1.0000e+00, 6.8196e-27, 3.2213e-27, 5.5211e-42, 3.3953e-28],\n",
            "        [2.1556e-36, 1.0000e+00, 1.5928e-35, 5.6317e-40, 8.5846e-29],\n",
            "        [1.7362e-20, 2.7159e-19, 1.0000e+00, 7.8281e-32, 1.7021e-22],\n",
            "        [5.3802e-32, 1.7362e-20, 1.4154e-28, 1.0000e+00, 3.4188e-21],\n",
            "        [1.4056e-35, 1.1244e-26, 1.3074e-36, 1.4524e-38, 1.0000e+00]])\n",
            "\n",
            "tensor([[4., 5., 8., 1., 2.],\n",
            "        [7., 2., 9., 2., 3.],\n",
            "        [2., 3., 5., 6., 9.],\n",
            "        [1., 8., 0., 6., 3.],\n",
            "        [8., 2., 8., 6., 5.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTxaYcNQc6xN"
      },
      "source": [
        "## Verify Class: MultiHeadAttention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67X-WPO3dBl_",
        "outputId": "d4767bae-d590-4231-a420-b8a5f57ed528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "nb = 2 # Batch Size\n",
        "h = 8 # Number of attention heads\n",
        "nw = 5 # Number of words (or positions) in the input\n",
        "dk = 64 # Dimension of Q, K and V vectors\n",
        "d_model = h * dk\n",
        "\n",
        "temp_model = MultiHeadAttention(h = h, d_model = 512, attn_dropout = 0.0).to(device)\n",
        "\n",
        "Q = torch.randint(0, 10, (nb, nw, (h * dk)), dtype = torch.float32).to(device)\n",
        "K = torch.clone(Q)\n",
        "V = torch.clone(Q)\n",
        "\n",
        "x = temp_model(Q, K, V, mask = None)\n",
        "print(Q.shape, x.shape, temp_model.attn.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 512]) torch.Size([2, 5, 512]) torch.Size([2, 8, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS06XXofFMd5"
      },
      "source": [
        "## Verify Class: (MultiHeadAttention + AddAndNorm)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QKTNMa9uWcE",
        "outputId": "35f16f2f-6c77-4b32-ae5c-6f1cfae92983",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "nb = 2 # Batch Size\n",
        "h = 8 # Number of attention heads\n",
        "nw = 5 # Number of words (or positions) in the input\n",
        "dk = 64 # Dimension of Q, K and V vectors\n",
        "d_model = h * dk\n",
        "\n",
        "attn = MultiHeadAttention(h = h, d_model = d_model, attn_dropout = 0.0).to(device)\n",
        "addandnorm = AddAndNorm(d_model).to(device)\n",
        "\n",
        "x = torch.randint(0, 10, (nb, nw, (h * dk)), dtype = torch.float32).to(device)\n",
        "print(x.shape)\n",
        "x = addandnorm(x, lambda x: attn(x, x, x))\n",
        "print(x.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 512])\n",
            "torch.Size([2, 5, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2e5fxRFFUTw"
      },
      "source": [
        "## Verify Class: Position-Wise Feed-Forward"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "d_model = 512 # Embedding Size\n",
        "d_ff = 2048 # Intermediate Layer Size\n",
        "PWFFN = PositionwiseFeedForward(d_model, d_ff, 0.0).to(device)\n",
        "ts_summary(PWFFN, input_size = (5, d_model,))"
      ],
      "metadata": {
        "id": "ZmF9o8dRFZHB",
        "outputId": "c46447c8-1edb-432b-9a85-28564076a960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1              [-1, 5, 2048]       1,050,624\n",
            "              ReLU-2              [-1, 5, 2048]               0\n",
            "            Linear-3               [-1, 5, 512]       1,049,088\n",
            "           Dropout-4               [-1, 5, 512]               0\n",
            "================================================================\n",
            "Total params: 2,099,712\n",
            "Trainable params: 2,099,712\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.20\n",
            "Params size (MB): 8.01\n",
            "Estimated Total Size (MB): 8.21\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb = 2 # Batch Size\n",
        "nw = 5 # Number of words (or positions) in the input\n",
        "d_model = 512\n",
        "input = torch.randint(0, 10, (nb, nw, d_model), dtype = torch.float32).to(device)\n",
        "output = PWFFN(input)\n",
        "print(input.shape, output.shape)"
      ],
      "metadata": {
        "id": "NjJS2SNcIhwe",
        "outputId": "0da3e752-9681-400d-b914-e939f490fa3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 512]) torch.Size([2, 5, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBlXPXGjLXpe"
      },
      "source": [
        "## Verify Class: EncoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512 # Embedding Size\n",
        "h = 8 # Number of attention heads\n",
        "d_ff = 2048 # Intermediate Layer Size\n",
        "enc_layer = EncoderLayer(d_model, h, d_ff, 0, 0).to(device)\n",
        "ts_summary(enc_layer, input_size = (5, d_model,))"
      ],
      "metadata": {
        "id": "DdB4jKRBLaZZ",
        "outputId": "cc74ee06-ce26-4592-e617-65a2e94c7763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1               [-1, 5, 512]         262,144\n",
            "            Linear-2               [-1, 5, 512]         262,144\n",
            "            Linear-3               [-1, 5, 512]         262,144\n",
            "           Softmax-4              [-1, 8, 5, 5]               0\n",
            "           Dropout-5              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-6  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "            Linear-7               [-1, 5, 512]         262,144\n",
            "           Dropout-8               [-1, 5, 512]               0\n",
            "MultiHeadAttention-9               [-1, 5, 512]               0\n",
            "        LayerNorm-10               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-11               [-1, 5, 512]               0\n",
            "           Linear-12              [-1, 5, 2048]       1,050,624\n",
            "             ReLU-13              [-1, 5, 2048]               0\n",
            "           Linear-14               [-1, 5, 512]       1,049,088\n",
            "          Dropout-15               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-16               [-1, 5, 512]               0\n",
            "        LayerNorm-17               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-18               [-1, 5, 512]               0\n",
            "================================================================\n",
            "Total params: 3,150,336\n",
            "Trainable params: 3,150,336\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 3.49\n",
            "Params size (MB): 12.02\n",
            "Estimated Total Size (MB): 15.52\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BtpmrSz6LOw"
      },
      "source": [
        "## Verify Class: Encoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512 # Embedding Size\n",
        "h = 8 # Number of attention heads\n",
        "d_ff = 2048 # Intermediate Layer Size\n",
        "N = 6 # Number of encoder layers\n",
        "enc = Encoder(d_model, h, d_ff, 0, 0, 6).to(device)\n",
        "ts_summary(enc, input_size = (5, d_model,))"
      ],
      "metadata": {
        "id": "uRt1MRI86MlK",
        "outputId": "bea11799-f764-4528-bf40-152ad3a7b25a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "         LayerNorm-1               [-1, 5, 512]           1,024\n",
            "            Linear-2               [-1, 5, 512]         262,144\n",
            "            Linear-3               [-1, 5, 512]         262,144\n",
            "            Linear-4               [-1, 5, 512]         262,144\n",
            "           Softmax-5              [-1, 8, 5, 5]               0\n",
            "           Dropout-6              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-7  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "            Linear-8               [-1, 5, 512]         262,144\n",
            "           Dropout-9               [-1, 5, 512]               0\n",
            "MultiHeadAttention-10               [-1, 5, 512]               0\n",
            "        LayerNorm-11               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-12               [-1, 5, 512]               0\n",
            "           Linear-13              [-1, 5, 2048]       1,050,624\n",
            "             ReLU-14              [-1, 5, 2048]               0\n",
            "           Linear-15               [-1, 5, 512]       1,049,088\n",
            "          Dropout-16               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-17               [-1, 5, 512]               0\n",
            "        LayerNorm-18               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-19               [-1, 5, 512]               0\n",
            "     EncoderLayer-20               [-1, 5, 512]               0\n",
            "           Linear-21               [-1, 5, 512]         262,144\n",
            "           Linear-22               [-1, 5, 512]         262,144\n",
            "           Linear-23               [-1, 5, 512]         262,144\n",
            "          Softmax-24              [-1, 8, 5, 5]               0\n",
            "          Dropout-25              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-26  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "           Linear-27               [-1, 5, 512]         262,144\n",
            "          Dropout-28               [-1, 5, 512]               0\n",
            "MultiHeadAttention-29               [-1, 5, 512]               0\n",
            "        LayerNorm-30               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-31               [-1, 5, 512]               0\n",
            "           Linear-32              [-1, 5, 2048]       1,050,624\n",
            "             ReLU-33              [-1, 5, 2048]               0\n",
            "           Linear-34               [-1, 5, 512]       1,049,088\n",
            "          Dropout-35               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-36               [-1, 5, 512]               0\n",
            "        LayerNorm-37               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-38               [-1, 5, 512]               0\n",
            "     EncoderLayer-39               [-1, 5, 512]               0\n",
            "           Linear-40               [-1, 5, 512]         262,144\n",
            "           Linear-41               [-1, 5, 512]         262,144\n",
            "           Linear-42               [-1, 5, 512]         262,144\n",
            "          Softmax-43              [-1, 8, 5, 5]               0\n",
            "          Dropout-44              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-45  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "           Linear-46               [-1, 5, 512]         262,144\n",
            "          Dropout-47               [-1, 5, 512]               0\n",
            "MultiHeadAttention-48               [-1, 5, 512]               0\n",
            "        LayerNorm-49               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-50               [-1, 5, 512]               0\n",
            "           Linear-51              [-1, 5, 2048]       1,050,624\n",
            "             ReLU-52              [-1, 5, 2048]               0\n",
            "           Linear-53               [-1, 5, 512]       1,049,088\n",
            "          Dropout-54               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-55               [-1, 5, 512]               0\n",
            "        LayerNorm-56               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-57               [-1, 5, 512]               0\n",
            "     EncoderLayer-58               [-1, 5, 512]               0\n",
            "           Linear-59               [-1, 5, 512]         262,144\n",
            "           Linear-60               [-1, 5, 512]         262,144\n",
            "           Linear-61               [-1, 5, 512]         262,144\n",
            "          Softmax-62              [-1, 8, 5, 5]               0\n",
            "          Dropout-63              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-64  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "           Linear-65               [-1, 5, 512]         262,144\n",
            "          Dropout-66               [-1, 5, 512]               0\n",
            "MultiHeadAttention-67               [-1, 5, 512]               0\n",
            "        LayerNorm-68               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-69               [-1, 5, 512]               0\n",
            "           Linear-70              [-1, 5, 2048]       1,050,624\n",
            "             ReLU-71              [-1, 5, 2048]               0\n",
            "           Linear-72               [-1, 5, 512]       1,049,088\n",
            "          Dropout-73               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-74               [-1, 5, 512]               0\n",
            "        LayerNorm-75               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-76               [-1, 5, 512]               0\n",
            "     EncoderLayer-77               [-1, 5, 512]               0\n",
            "           Linear-78               [-1, 5, 512]         262,144\n",
            "           Linear-79               [-1, 5, 512]         262,144\n",
            "           Linear-80               [-1, 5, 512]         262,144\n",
            "          Softmax-81              [-1, 8, 5, 5]               0\n",
            "          Dropout-82              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-83  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "           Linear-84               [-1, 5, 512]         262,144\n",
            "          Dropout-85               [-1, 5, 512]               0\n",
            "MultiHeadAttention-86               [-1, 5, 512]               0\n",
            "        LayerNorm-87               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-88               [-1, 5, 512]               0\n",
            "           Linear-89              [-1, 5, 2048]       1,050,624\n",
            "             ReLU-90              [-1, 5, 2048]               0\n",
            "           Linear-91               [-1, 5, 512]       1,049,088\n",
            "          Dropout-92               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-93               [-1, 5, 512]               0\n",
            "        LayerNorm-94               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-95               [-1, 5, 512]               0\n",
            "     EncoderLayer-96               [-1, 5, 512]               0\n",
            "           Linear-97               [-1, 5, 512]         262,144\n",
            "           Linear-98               [-1, 5, 512]         262,144\n",
            "           Linear-99               [-1, 5, 512]         262,144\n",
            "         Softmax-100              [-1, 8, 5, 5]               0\n",
            "         Dropout-101              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-102  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "          Linear-103               [-1, 5, 512]         262,144\n",
            "         Dropout-104               [-1, 5, 512]               0\n",
            "MultiHeadAttention-105               [-1, 5, 512]               0\n",
            "       LayerNorm-106               [-1, 5, 512]           1,024\n",
            "      AddAndNorm-107               [-1, 5, 512]               0\n",
            "          Linear-108              [-1, 5, 2048]       1,050,624\n",
            "            ReLU-109              [-1, 5, 2048]               0\n",
            "          Linear-110               [-1, 5, 512]       1,049,088\n",
            "         Dropout-111               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-112               [-1, 5, 512]               0\n",
            "       LayerNorm-113               [-1, 5, 512]           1,024\n",
            "      AddAndNorm-114               [-1, 5, 512]               0\n",
            "    EncoderLayer-115               [-1, 5, 512]               0\n",
            "================================================================\n",
            "Total params: 18,903,040\n",
            "Trainable params: 18,903,040\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 20.82\n",
            "Params size (MB): 72.11\n",
            "Estimated Total Size (MB): 92.94\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yetSAFeOQi7y"
      },
      "source": [
        "## Verify Class: DecoderLayer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512 # Embedding Size\n",
        "h = 8 # Number of attention heads\n",
        "d_ff = 2048 # Intermediate Layer Size\n",
        "dec_layer = DecoderLayer(d_model, h, d_ff, 0, 0).to(device)\n",
        "ts_summary(dec_layer, input_size = [(5, d_model), (5, d_model)])"
      ],
      "metadata": {
        "id": "la-TUxAWQi7z",
        "outputId": "14281616-88bc-43f4-be9f-024d3ffae518",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1               [-1, 5, 512]         262,144\n",
            "            Linear-2               [-1, 5, 512]         262,144\n",
            "            Linear-3               [-1, 5, 512]         262,144\n",
            "           Softmax-4              [-1, 8, 5, 5]               0\n",
            "           Dropout-5              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-6  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "            Linear-7               [-1, 5, 512]         262,144\n",
            "           Dropout-8               [-1, 5, 512]               0\n",
            "MultiHeadAttention-9               [-1, 5, 512]               0\n",
            "        LayerNorm-10               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-11               [-1, 5, 512]               0\n",
            "           Linear-12               [-1, 5, 512]         262,144\n",
            "           Linear-13               [-1, 5, 512]         262,144\n",
            "           Linear-14               [-1, 5, 512]         262,144\n",
            "          Softmax-15              [-1, 8, 5, 5]               0\n",
            "          Dropout-16              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-17  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "           Linear-18               [-1, 5, 512]         262,144\n",
            "          Dropout-19               [-1, 5, 512]               0\n",
            "MultiHeadAttention-20               [-1, 5, 512]               0\n",
            "        LayerNorm-21               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-22               [-1, 5, 512]               0\n",
            "           Linear-23              [-1, 5, 2048]       1,050,624\n",
            "             ReLU-24              [-1, 5, 2048]               0\n",
            "           Linear-25               [-1, 5, 512]       1,049,088\n",
            "          Dropout-26               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-27               [-1, 5, 512]               0\n",
            "        LayerNorm-28               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-29               [-1, 5, 512]               0\n",
            "================================================================\n",
            "Total params: 4,199,936\n",
            "Trainable params: 4,199,936\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 25.00\n",
            "Forward/backward pass size (MB): 7.24\n",
            "Params size (MB): 16.02\n",
            "Estimated Total Size (MB): 48.26\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C2o17WPRupW"
      },
      "source": [
        "## Verify Class: Decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512 # Embedding Size\n",
        "h = 8 # Number of attention heads\n",
        "d_ff = 2048 # Intermediate Layer Size\n",
        "N = 6 # Number of decoder layers\n",
        "dec = Decoder(d_model, h, d_ff, 0, 0, N).to(device)\n",
        "ts_summary(dec, input_size = [(5, d_model), (5, d_model)])"
      ],
      "metadata": {
        "id": "UIwJOgo0RupX",
        "outputId": "b0d2beaf-8f31-42a2-d115-44bc11bb4e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "         LayerNorm-1               [-1, 5, 512]           1,024\n",
            "            Linear-2               [-1, 5, 512]         262,144\n",
            "            Linear-3               [-1, 5, 512]         262,144\n",
            "            Linear-4               [-1, 5, 512]         262,144\n",
            "           Softmax-5              [-1, 8, 5, 5]               0\n",
            "           Dropout-6              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-7  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "            Linear-8               [-1, 5, 512]         262,144\n",
            "           Dropout-9               [-1, 5, 512]               0\n",
            "MultiHeadAttention-10               [-1, 5, 512]               0\n",
            "        LayerNorm-11               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-12               [-1, 5, 512]               0\n",
            "           Linear-13               [-1, 5, 512]         262,144\n",
            "           Linear-14               [-1, 5, 512]         262,144\n",
            "           Linear-15               [-1, 5, 512]         262,144\n",
            "          Softmax-16              [-1, 8, 5, 5]               0\n",
            "          Dropout-17              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-18  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "           Linear-19               [-1, 5, 512]         262,144\n",
            "          Dropout-20               [-1, 5, 512]               0\n",
            "MultiHeadAttention-21               [-1, 5, 512]               0\n",
            "        LayerNorm-22               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-23               [-1, 5, 512]               0\n",
            "           Linear-24              [-1, 5, 2048]       1,050,624\n",
            "             ReLU-25              [-1, 5, 2048]               0\n",
            "           Linear-26               [-1, 5, 512]       1,049,088\n",
            "          Dropout-27               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-28               [-1, 5, 512]               0\n",
            "        LayerNorm-29               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-30               [-1, 5, 512]               0\n",
            "     DecoderLayer-31               [-1, 5, 512]               0\n",
            "           Linear-32               [-1, 5, 512]         262,144\n",
            "           Linear-33               [-1, 5, 512]         262,144\n",
            "           Linear-34               [-1, 5, 512]         262,144\n",
            "          Softmax-35              [-1, 8, 5, 5]               0\n",
            "          Dropout-36              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-37  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "           Linear-38               [-1, 5, 512]         262,144\n",
            "          Dropout-39               [-1, 5, 512]               0\n",
            "MultiHeadAttention-40               [-1, 5, 512]               0\n",
            "        LayerNorm-41               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-42               [-1, 5, 512]               0\n",
            "           Linear-43               [-1, 5, 512]         262,144\n",
            "           Linear-44               [-1, 5, 512]         262,144\n",
            "           Linear-45               [-1, 5, 512]         262,144\n",
            "          Softmax-46              [-1, 8, 5, 5]               0\n",
            "          Dropout-47              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-48  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "           Linear-49               [-1, 5, 512]         262,144\n",
            "          Dropout-50               [-1, 5, 512]               0\n",
            "MultiHeadAttention-51               [-1, 5, 512]               0\n",
            "        LayerNorm-52               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-53               [-1, 5, 512]               0\n",
            "           Linear-54              [-1, 5, 2048]       1,050,624\n",
            "             ReLU-55              [-1, 5, 2048]               0\n",
            "           Linear-56               [-1, 5, 512]       1,049,088\n",
            "          Dropout-57               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-58               [-1, 5, 512]               0\n",
            "        LayerNorm-59               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-60               [-1, 5, 512]               0\n",
            "     DecoderLayer-61               [-1, 5, 512]               0\n",
            "           Linear-62               [-1, 5, 512]         262,144\n",
            "           Linear-63               [-1, 5, 512]         262,144\n",
            "           Linear-64               [-1, 5, 512]         262,144\n",
            "          Softmax-65              [-1, 8, 5, 5]               0\n",
            "          Dropout-66              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-67  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "           Linear-68               [-1, 5, 512]         262,144\n",
            "          Dropout-69               [-1, 5, 512]               0\n",
            "MultiHeadAttention-70               [-1, 5, 512]               0\n",
            "        LayerNorm-71               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-72               [-1, 5, 512]               0\n",
            "           Linear-73               [-1, 5, 512]         262,144\n",
            "           Linear-74               [-1, 5, 512]         262,144\n",
            "           Linear-75               [-1, 5, 512]         262,144\n",
            "          Softmax-76              [-1, 8, 5, 5]               0\n",
            "          Dropout-77              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-78  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "           Linear-79               [-1, 5, 512]         262,144\n",
            "          Dropout-80               [-1, 5, 512]               0\n",
            "MultiHeadAttention-81               [-1, 5, 512]               0\n",
            "        LayerNorm-82               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-83               [-1, 5, 512]               0\n",
            "           Linear-84              [-1, 5, 2048]       1,050,624\n",
            "             ReLU-85              [-1, 5, 2048]               0\n",
            "           Linear-86               [-1, 5, 512]       1,049,088\n",
            "          Dropout-87               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-88               [-1, 5, 512]               0\n",
            "        LayerNorm-89               [-1, 5, 512]           1,024\n",
            "       AddAndNorm-90               [-1, 5, 512]               0\n",
            "     DecoderLayer-91               [-1, 5, 512]               0\n",
            "           Linear-92               [-1, 5, 512]         262,144\n",
            "           Linear-93               [-1, 5, 512]         262,144\n",
            "           Linear-94               [-1, 5, 512]         262,144\n",
            "          Softmax-95              [-1, 8, 5, 5]               0\n",
            "          Dropout-96              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-97  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "           Linear-98               [-1, 5, 512]         262,144\n",
            "          Dropout-99               [-1, 5, 512]               0\n",
            "MultiHeadAttention-100               [-1, 5, 512]               0\n",
            "       LayerNorm-101               [-1, 5, 512]           1,024\n",
            "      AddAndNorm-102               [-1, 5, 512]               0\n",
            "          Linear-103               [-1, 5, 512]         262,144\n",
            "          Linear-104               [-1, 5, 512]         262,144\n",
            "          Linear-105               [-1, 5, 512]         262,144\n",
            "         Softmax-106              [-1, 8, 5, 5]               0\n",
            "         Dropout-107              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-108  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "          Linear-109               [-1, 5, 512]         262,144\n",
            "         Dropout-110               [-1, 5, 512]               0\n",
            "MultiHeadAttention-111               [-1, 5, 512]               0\n",
            "       LayerNorm-112               [-1, 5, 512]           1,024\n",
            "      AddAndNorm-113               [-1, 5, 512]               0\n",
            "          Linear-114              [-1, 5, 2048]       1,050,624\n",
            "            ReLU-115              [-1, 5, 2048]               0\n",
            "          Linear-116               [-1, 5, 512]       1,049,088\n",
            "         Dropout-117               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-118               [-1, 5, 512]               0\n",
            "       LayerNorm-119               [-1, 5, 512]           1,024\n",
            "      AddAndNorm-120               [-1, 5, 512]               0\n",
            "    DecoderLayer-121               [-1, 5, 512]               0\n",
            "          Linear-122               [-1, 5, 512]         262,144\n",
            "          Linear-123               [-1, 5, 512]         262,144\n",
            "          Linear-124               [-1, 5, 512]         262,144\n",
            "         Softmax-125              [-1, 8, 5, 5]               0\n",
            "         Dropout-126              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-127  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "          Linear-128               [-1, 5, 512]         262,144\n",
            "         Dropout-129               [-1, 5, 512]               0\n",
            "MultiHeadAttention-130               [-1, 5, 512]               0\n",
            "       LayerNorm-131               [-1, 5, 512]           1,024\n",
            "      AddAndNorm-132               [-1, 5, 512]               0\n",
            "          Linear-133               [-1, 5, 512]         262,144\n",
            "          Linear-134               [-1, 5, 512]         262,144\n",
            "          Linear-135               [-1, 5, 512]         262,144\n",
            "         Softmax-136              [-1, 8, 5, 5]               0\n",
            "         Dropout-137              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-138  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "          Linear-139               [-1, 5, 512]         262,144\n",
            "         Dropout-140               [-1, 5, 512]               0\n",
            "MultiHeadAttention-141               [-1, 5, 512]               0\n",
            "       LayerNorm-142               [-1, 5, 512]           1,024\n",
            "      AddAndNorm-143               [-1, 5, 512]               0\n",
            "          Linear-144              [-1, 5, 2048]       1,050,624\n",
            "            ReLU-145              [-1, 5, 2048]               0\n",
            "          Linear-146               [-1, 5, 512]       1,049,088\n",
            "         Dropout-147               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-148               [-1, 5, 512]               0\n",
            "       LayerNorm-149               [-1, 5, 512]           1,024\n",
            "      AddAndNorm-150               [-1, 5, 512]               0\n",
            "    DecoderLayer-151               [-1, 5, 512]               0\n",
            "          Linear-152               [-1, 5, 512]         262,144\n",
            "          Linear-153               [-1, 5, 512]         262,144\n",
            "          Linear-154               [-1, 5, 512]         262,144\n",
            "         Softmax-155              [-1, 8, 5, 5]               0\n",
            "         Dropout-156              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-157  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "          Linear-158               [-1, 5, 512]         262,144\n",
            "         Dropout-159               [-1, 5, 512]               0\n",
            "MultiHeadAttention-160               [-1, 5, 512]               0\n",
            "       LayerNorm-161               [-1, 5, 512]           1,024\n",
            "      AddAndNorm-162               [-1, 5, 512]               0\n",
            "          Linear-163               [-1, 5, 512]         262,144\n",
            "          Linear-164               [-1, 5, 512]         262,144\n",
            "          Linear-165               [-1, 5, 512]         262,144\n",
            "         Softmax-166              [-1, 8, 5, 5]               0\n",
            "         Dropout-167              [-1, 8, 5, 5]               0\n",
            "ScaledDotProductAttention-168  [[-1, 8, 5, 64], [-1, 8, 5, 5]]               0\n",
            "          Linear-169               [-1, 5, 512]         262,144\n",
            "         Dropout-170               [-1, 5, 512]               0\n",
            "MultiHeadAttention-171               [-1, 5, 512]               0\n",
            "       LayerNorm-172               [-1, 5, 512]           1,024\n",
            "      AddAndNorm-173               [-1, 5, 512]               0\n",
            "          Linear-174              [-1, 5, 2048]       1,050,624\n",
            "            ReLU-175              [-1, 5, 2048]               0\n",
            "          Linear-176               [-1, 5, 512]       1,049,088\n",
            "         Dropout-177               [-1, 5, 512]               0\n",
            "PositionwiseFeedForward-178               [-1, 5, 512]               0\n",
            "       LayerNorm-179               [-1, 5, 512]           1,024\n",
            "      AddAndNorm-180               [-1, 5, 512]               0\n",
            "    DecoderLayer-181               [-1, 5, 512]               0\n",
            "================================================================\n",
            "Total params: 25,200,640\n",
            "Trainable params: 25,200,640\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 25.00\n",
            "Forward/backward pass size (MB): 43.30\n",
            "Params size (MB): 96.13\n",
            "Estimated Total Size (MB): 164.44\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nemvzRguYmaE"
      },
      "source": [
        "## Verify Class: Generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512 # Embedding Size\n",
        "vocab = 11 # Size of vocabulary\n",
        "gen = Generator(d_model, vocab).to(device) # Generator instance\n",
        "ts_summary(gen, input_size = (5, d_model))"
      ],
      "metadata": {
        "id": "BsZEE21RYmaE",
        "outputId": "daf39620-8de9-4524-9523-4b107404c23f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                [-1, 5, 11]           5,643\n",
            "        LogSoftmax-2                [-1, 5, 11]               0\n",
            "================================================================\n",
            "Total params: 5,643\n",
            "Trainable params: 5,643\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 0.03\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCgoLmKfY50q"
      },
      "source": [
        "## Verify Class: EncoderDecoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512 # Embedding Size\n",
        "h = 8 # Number of attention heads\n",
        "d_ff = 2048 # Intermediate Layer Size\n",
        "N = 6 # Number of encoder layers\n",
        "enc = Encoder(d_model, h, d_ff, 0, 0, N).to(device)\n",
        "dec = Decoder(d_model, h, d_ff, 0, 0, N).to(device)\n",
        "\n",
        "src_vocab = 11 # Size of source vocab vector\n",
        "tgt_vocab = 11 # Size of target vocab vector\n",
        "emb_enc = Embeddings(d_model, src_vocab).to(device)\n",
        "emb_dec = Embeddings(d_model, tgt_vocab).to(device)\n",
        "gen = Generator(d_model, tgt_vocab).to(device) # Generator instance\n",
        "\n",
        "model = EncoderDecoder(enc, dec, emb_enc, emb_dec, gen)\n",
        "\n",
        "input_size = (2, 5)\n",
        "x_sample = torch.zeros(input_size, dtype=torch.long, device=torch.device('cuda'))\n",
        "summaryx(model, x_sample, x_sample)"
      ],
      "metadata": {
        "id": "v5UJ_uM_Y50r",
        "outputId": "de62549c-9428-44ad-d269-e7c68534897c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================================================\n",
            "                                                   Kernel Shape  Output Shape  \\\n",
            "Layer                                                                           \n",
            "0_src_embed.Embedding_emb                             [512, 11]   [2, 5, 512]   \n",
            "1_encoder.LayerNorm_norm                                  [512]   [2, 5, 512]   \n",
            "2_encoder.enclayer_stack.0.MHA_unit.Linear_wi_q      [512, 512]   [2, 5, 512]   \n",
            "3_encoder.enclayer_stack.0.MHA_unit.Linear_wi_k      [512, 512]   [2, 5, 512]   \n",
            "4_encoder.enclayer_stack.0.MHA_unit.Linear_wi_v      [512, 512]   [2, 5, 512]   \n",
            "5_encoder.enclayer_stack.0.MHA_unit.sdpa.Softma...            -  [2, 8, 5, 5]   \n",
            "6_encoder.enclayer_stack.0.MHA_unit.Dropout_dro...            -  [2, 8, 5, 5]   \n",
            "7_encoder.enclayer_stack.0.MHA_unit.Linear_fc        [512, 512]   [2, 5, 512]   \n",
            "8_encoder.enclayer_stack.0.MHA_unit.Dropout_dro...            -   [2, 5, 512]   \n",
            "9_encoder.enclayer_stack.0.addandnorm_MHA.Layer...        [512]   [2, 5, 512]   \n",
            "10_encoder.enclayer_stack.0.PWFFN.Linear_fc_1       [512, 2048]  [2, 5, 2048]   \n",
            "11_encoder.enclayer_stack.0.PWFFN.ReLU_relu                   -  [2, 5, 2048]   \n",
            "12_encoder.enclayer_stack.0.PWFFN.Linear_fc_2       [2048, 512]   [2, 5, 512]   \n",
            "13_encoder.enclayer_stack.0.PWFFN.Dropout_dropout             -   [2, 5, 512]   \n",
            "14_encoder.enclayer_stack.0.addandnorm_PWFFN.La...        [512]   [2, 5, 512]   \n",
            "15_encoder.enclayer_stack.1.MHA_unit.Linear_wi_q     [512, 512]   [2, 5, 512]   \n",
            "16_encoder.enclayer_stack.1.MHA_unit.Linear_wi_k     [512, 512]   [2, 5, 512]   \n",
            "17_encoder.enclayer_stack.1.MHA_unit.Linear_wi_v     [512, 512]   [2, 5, 512]   \n",
            "18_encoder.enclayer_stack.1.MHA_unit.sdpa.Softm...            -  [2, 8, 5, 5]   \n",
            "19_encoder.enclayer_stack.1.MHA_unit.Dropout_dr...            -  [2, 8, 5, 5]   \n",
            "20_encoder.enclayer_stack.1.MHA_unit.Linear_fc       [512, 512]   [2, 5, 512]   \n",
            "21_encoder.enclayer_stack.1.MHA_unit.Dropout_dr...            -   [2, 5, 512]   \n",
            "22_encoder.enclayer_stack.1.addandnorm_MHA.Laye...        [512]   [2, 5, 512]   \n",
            "23_encoder.enclayer_stack.1.PWFFN.Linear_fc_1       [512, 2048]  [2, 5, 2048]   \n",
            "24_encoder.enclayer_stack.1.PWFFN.ReLU_relu                   -  [2, 5, 2048]   \n",
            "25_encoder.enclayer_stack.1.PWFFN.Linear_fc_2       [2048, 512]   [2, 5, 512]   \n",
            "26_encoder.enclayer_stack.1.PWFFN.Dropout_dropout             -   [2, 5, 512]   \n",
            "27_encoder.enclayer_stack.1.addandnorm_PWFFN.La...        [512]   [2, 5, 512]   \n",
            "28_encoder.enclayer_stack.2.MHA_unit.Linear_wi_q     [512, 512]   [2, 5, 512]   \n",
            "29_encoder.enclayer_stack.2.MHA_unit.Linear_wi_k     [512, 512]   [2, 5, 512]   \n",
            "30_encoder.enclayer_stack.2.MHA_unit.Linear_wi_v     [512, 512]   [2, 5, 512]   \n",
            "31_encoder.enclayer_stack.2.MHA_unit.sdpa.Softm...            -  [2, 8, 5, 5]   \n",
            "32_encoder.enclayer_stack.2.MHA_unit.Dropout_dr...            -  [2, 8, 5, 5]   \n",
            "33_encoder.enclayer_stack.2.MHA_unit.Linear_fc       [512, 512]   [2, 5, 512]   \n",
            "34_encoder.enclayer_stack.2.MHA_unit.Dropout_dr...            -   [2, 5, 512]   \n",
            "35_encoder.enclayer_stack.2.addandnorm_MHA.Laye...        [512]   [2, 5, 512]   \n",
            "36_encoder.enclayer_stack.2.PWFFN.Linear_fc_1       [512, 2048]  [2, 5, 2048]   \n",
            "37_encoder.enclayer_stack.2.PWFFN.ReLU_relu                   -  [2, 5, 2048]   \n",
            "38_encoder.enclayer_stack.2.PWFFN.Linear_fc_2       [2048, 512]   [2, 5, 512]   \n",
            "39_encoder.enclayer_stack.2.PWFFN.Dropout_dropout             -   [2, 5, 512]   \n",
            "40_encoder.enclayer_stack.2.addandnorm_PWFFN.La...        [512]   [2, 5, 512]   \n",
            "41_encoder.enclayer_stack.3.MHA_unit.Linear_wi_q     [512, 512]   [2, 5, 512]   \n",
            "42_encoder.enclayer_stack.3.MHA_unit.Linear_wi_k     [512, 512]   [2, 5, 512]   \n",
            "43_encoder.enclayer_stack.3.MHA_unit.Linear_wi_v     [512, 512]   [2, 5, 512]   \n",
            "44_encoder.enclayer_stack.3.MHA_unit.sdpa.Softm...            -  [2, 8, 5, 5]   \n",
            "45_encoder.enclayer_stack.3.MHA_unit.Dropout_dr...            -  [2, 8, 5, 5]   \n",
            "46_encoder.enclayer_stack.3.MHA_unit.Linear_fc       [512, 512]   [2, 5, 512]   \n",
            "47_encoder.enclayer_stack.3.MHA_unit.Dropout_dr...            -   [2, 5, 512]   \n",
            "48_encoder.enclayer_stack.3.addandnorm_MHA.Laye...        [512]   [2, 5, 512]   \n",
            "49_encoder.enclayer_stack.3.PWFFN.Linear_fc_1       [512, 2048]  [2, 5, 2048]   \n",
            "50_encoder.enclayer_stack.3.PWFFN.ReLU_relu                   -  [2, 5, 2048]   \n",
            "51_encoder.enclayer_stack.3.PWFFN.Linear_fc_2       [2048, 512]   [2, 5, 512]   \n",
            "52_encoder.enclayer_stack.3.PWFFN.Dropout_dropout             -   [2, 5, 512]   \n",
            "53_encoder.enclayer_stack.3.addandnorm_PWFFN.La...        [512]   [2, 5, 512]   \n",
            "54_encoder.enclayer_stack.4.MHA_unit.Linear_wi_q     [512, 512]   [2, 5, 512]   \n",
            "55_encoder.enclayer_stack.4.MHA_unit.Linear_wi_k     [512, 512]   [2, 5, 512]   \n",
            "56_encoder.enclayer_stack.4.MHA_unit.Linear_wi_v     [512, 512]   [2, 5, 512]   \n",
            "57_encoder.enclayer_stack.4.MHA_unit.sdpa.Softm...            -  [2, 8, 5, 5]   \n",
            "58_encoder.enclayer_stack.4.MHA_unit.Dropout_dr...            -  [2, 8, 5, 5]   \n",
            "59_encoder.enclayer_stack.4.MHA_unit.Linear_fc       [512, 512]   [2, 5, 512]   \n",
            "60_encoder.enclayer_stack.4.MHA_unit.Dropout_dr...            -   [2, 5, 512]   \n",
            "61_encoder.enclayer_stack.4.addandnorm_MHA.Laye...        [512]   [2, 5, 512]   \n",
            "62_encoder.enclayer_stack.4.PWFFN.Linear_fc_1       [512, 2048]  [2, 5, 2048]   \n",
            "63_encoder.enclayer_stack.4.PWFFN.ReLU_relu                   -  [2, 5, 2048]   \n",
            "64_encoder.enclayer_stack.4.PWFFN.Linear_fc_2       [2048, 512]   [2, 5, 512]   \n",
            "65_encoder.enclayer_stack.4.PWFFN.Dropout_dropout             -   [2, 5, 512]   \n",
            "66_encoder.enclayer_stack.4.addandnorm_PWFFN.La...        [512]   [2, 5, 512]   \n",
            "67_encoder.enclayer_stack.5.MHA_unit.Linear_wi_q     [512, 512]   [2, 5, 512]   \n",
            "68_encoder.enclayer_stack.5.MHA_unit.Linear_wi_k     [512, 512]   [2, 5, 512]   \n",
            "69_encoder.enclayer_stack.5.MHA_unit.Linear_wi_v     [512, 512]   [2, 5, 512]   \n",
            "70_encoder.enclayer_stack.5.MHA_unit.sdpa.Softm...            -  [2, 8, 5, 5]   \n",
            "71_encoder.enclayer_stack.5.MHA_unit.Dropout_dr...            -  [2, 8, 5, 5]   \n",
            "72_encoder.enclayer_stack.5.MHA_unit.Linear_fc       [512, 512]   [2, 5, 512]   \n",
            "73_encoder.enclayer_stack.5.MHA_unit.Dropout_dr...            -   [2, 5, 512]   \n",
            "74_encoder.enclayer_stack.5.addandnorm_MHA.Laye...        [512]   [2, 5, 512]   \n",
            "75_encoder.enclayer_stack.5.PWFFN.Linear_fc_1       [512, 2048]  [2, 5, 2048]   \n",
            "76_encoder.enclayer_stack.5.PWFFN.ReLU_relu                   -  [2, 5, 2048]   \n",
            "77_encoder.enclayer_stack.5.PWFFN.Linear_fc_2       [2048, 512]   [2, 5, 512]   \n",
            "78_encoder.enclayer_stack.5.PWFFN.Dropout_dropout             -   [2, 5, 512]   \n",
            "79_encoder.enclayer_stack.5.addandnorm_PWFFN.La...        [512]   [2, 5, 512]   \n",
            "80_tgt_embed.Embedding_emb                            [512, 11]   [2, 5, 512]   \n",
            "81_decoder.LayerNorm_norm                                 [512]   [2, 5, 512]   \n",
            "82_decoder.declayer_stack.0.self_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "83_decoder.declayer_stack.0.self_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "84_decoder.declayer_stack.0.self_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "85_decoder.declayer_stack.0.self_MHA_unit.sdpa....            -  [2, 8, 5, 5]   \n",
            "86_decoder.declayer_stack.0.self_MHA_unit.Dropo...            -  [2, 8, 5, 5]   \n",
            "87_decoder.declayer_stack.0.self_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "88_decoder.declayer_stack.0.self_MHA_unit.Dropo...            -   [2, 5, 512]   \n",
            "89_decoder.declayer_stack.0.addandnorm_self_MHA...        [512]   [2, 5, 512]   \n",
            "90_decoder.declayer_stack.0.enc_MHA_unit.Linear...   [512, 512]   [2, 5, 512]   \n",
            "91_decoder.declayer_stack.0.enc_MHA_unit.Linear...   [512, 512]   [2, 5, 512]   \n",
            "92_decoder.declayer_stack.0.enc_MHA_unit.Linear...   [512, 512]   [2, 5, 512]   \n",
            "93_decoder.declayer_stack.0.enc_MHA_unit.sdpa.S...            -  [2, 8, 5, 5]   \n",
            "94_decoder.declayer_stack.0.enc_MHA_unit.Dropou...            -  [2, 8, 5, 5]   \n",
            "95_decoder.declayer_stack.0.enc_MHA_unit.Linear_fc   [512, 512]   [2, 5, 512]   \n",
            "96_decoder.declayer_stack.0.enc_MHA_unit.Dropou...            -   [2, 5, 512]   \n",
            "97_decoder.declayer_stack.0.addandnorm_enc_MHA....        [512]   [2, 5, 512]   \n",
            "98_decoder.declayer_stack.0.PWFFN.Linear_fc_1       [512, 2048]  [2, 5, 2048]   \n",
            "99_decoder.declayer_stack.0.PWFFN.ReLU_relu                   -  [2, 5, 2048]   \n",
            "100_decoder.declayer_stack.0.PWFFN.Linear_fc_2      [2048, 512]   [2, 5, 512]   \n",
            "101_decoder.declayer_stack.0.PWFFN.Dropout_dropout            -   [2, 5, 512]   \n",
            "102_decoder.declayer_stack.0.addandnorm_PWFFN.L...        [512]   [2, 5, 512]   \n",
            "103_decoder.declayer_stack.1.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "104_decoder.declayer_stack.1.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "105_decoder.declayer_stack.1.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "106_decoder.declayer_stack.1.self_MHA_unit.sdpa...            -  [2, 8, 5, 5]   \n",
            "107_decoder.declayer_stack.1.self_MHA_unit.Drop...            -  [2, 8, 5, 5]   \n",
            "108_decoder.declayer_stack.1.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "109_decoder.declayer_stack.1.self_MHA_unit.Drop...            -   [2, 5, 512]   \n",
            "110_decoder.declayer_stack.1.addandnorm_self_MH...        [512]   [2, 5, 512]   \n",
            "111_decoder.declayer_stack.1.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "112_decoder.declayer_stack.1.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "113_decoder.declayer_stack.1.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "114_decoder.declayer_stack.1.enc_MHA_unit.sdpa....            -  [2, 8, 5, 5]   \n",
            "115_decoder.declayer_stack.1.enc_MHA_unit.Dropo...            -  [2, 8, 5, 5]   \n",
            "116_decoder.declayer_stack.1.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "117_decoder.declayer_stack.1.enc_MHA_unit.Dropo...            -   [2, 5, 512]   \n",
            "118_decoder.declayer_stack.1.addandnorm_enc_MHA...        [512]   [2, 5, 512]   \n",
            "119_decoder.declayer_stack.1.PWFFN.Linear_fc_1      [512, 2048]  [2, 5, 2048]   \n",
            "120_decoder.declayer_stack.1.PWFFN.ReLU_relu                  -  [2, 5, 2048]   \n",
            "121_decoder.declayer_stack.1.PWFFN.Linear_fc_2      [2048, 512]   [2, 5, 512]   \n",
            "122_decoder.declayer_stack.1.PWFFN.Dropout_dropout            -   [2, 5, 512]   \n",
            "123_decoder.declayer_stack.1.addandnorm_PWFFN.L...        [512]   [2, 5, 512]   \n",
            "124_decoder.declayer_stack.2.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "125_decoder.declayer_stack.2.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "126_decoder.declayer_stack.2.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "127_decoder.declayer_stack.2.self_MHA_unit.sdpa...            -  [2, 8, 5, 5]   \n",
            "128_decoder.declayer_stack.2.self_MHA_unit.Drop...            -  [2, 8, 5, 5]   \n",
            "129_decoder.declayer_stack.2.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "130_decoder.declayer_stack.2.self_MHA_unit.Drop...            -   [2, 5, 512]   \n",
            "131_decoder.declayer_stack.2.addandnorm_self_MH...        [512]   [2, 5, 512]   \n",
            "132_decoder.declayer_stack.2.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "133_decoder.declayer_stack.2.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "134_decoder.declayer_stack.2.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "135_decoder.declayer_stack.2.enc_MHA_unit.sdpa....            -  [2, 8, 5, 5]   \n",
            "136_decoder.declayer_stack.2.enc_MHA_unit.Dropo...            -  [2, 8, 5, 5]   \n",
            "137_decoder.declayer_stack.2.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "138_decoder.declayer_stack.2.enc_MHA_unit.Dropo...            -   [2, 5, 512]   \n",
            "139_decoder.declayer_stack.2.addandnorm_enc_MHA...        [512]   [2, 5, 512]   \n",
            "140_decoder.declayer_stack.2.PWFFN.Linear_fc_1      [512, 2048]  [2, 5, 2048]   \n",
            "141_decoder.declayer_stack.2.PWFFN.ReLU_relu                  -  [2, 5, 2048]   \n",
            "142_decoder.declayer_stack.2.PWFFN.Linear_fc_2      [2048, 512]   [2, 5, 512]   \n",
            "143_decoder.declayer_stack.2.PWFFN.Dropout_dropout            -   [2, 5, 512]   \n",
            "144_decoder.declayer_stack.2.addandnorm_PWFFN.L...        [512]   [2, 5, 512]   \n",
            "145_decoder.declayer_stack.3.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "146_decoder.declayer_stack.3.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "147_decoder.declayer_stack.3.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "148_decoder.declayer_stack.3.self_MHA_unit.sdpa...            -  [2, 8, 5, 5]   \n",
            "149_decoder.declayer_stack.3.self_MHA_unit.Drop...            -  [2, 8, 5, 5]   \n",
            "150_decoder.declayer_stack.3.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "151_decoder.declayer_stack.3.self_MHA_unit.Drop...            -   [2, 5, 512]   \n",
            "152_decoder.declayer_stack.3.addandnorm_self_MH...        [512]   [2, 5, 512]   \n",
            "153_decoder.declayer_stack.3.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "154_decoder.declayer_stack.3.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "155_decoder.declayer_stack.3.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "156_decoder.declayer_stack.3.enc_MHA_unit.sdpa....            -  [2, 8, 5, 5]   \n",
            "157_decoder.declayer_stack.3.enc_MHA_unit.Dropo...            -  [2, 8, 5, 5]   \n",
            "158_decoder.declayer_stack.3.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "159_decoder.declayer_stack.3.enc_MHA_unit.Dropo...            -   [2, 5, 512]   \n",
            "160_decoder.declayer_stack.3.addandnorm_enc_MHA...        [512]   [2, 5, 512]   \n",
            "161_decoder.declayer_stack.3.PWFFN.Linear_fc_1      [512, 2048]  [2, 5, 2048]   \n",
            "162_decoder.declayer_stack.3.PWFFN.ReLU_relu                  -  [2, 5, 2048]   \n",
            "163_decoder.declayer_stack.3.PWFFN.Linear_fc_2      [2048, 512]   [2, 5, 512]   \n",
            "164_decoder.declayer_stack.3.PWFFN.Dropout_dropout            -   [2, 5, 512]   \n",
            "165_decoder.declayer_stack.3.addandnorm_PWFFN.L...        [512]   [2, 5, 512]   \n",
            "166_decoder.declayer_stack.4.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "167_decoder.declayer_stack.4.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "168_decoder.declayer_stack.4.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "169_decoder.declayer_stack.4.self_MHA_unit.sdpa...            -  [2, 8, 5, 5]   \n",
            "170_decoder.declayer_stack.4.self_MHA_unit.Drop...            -  [2, 8, 5, 5]   \n",
            "171_decoder.declayer_stack.4.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "172_decoder.declayer_stack.4.self_MHA_unit.Drop...            -   [2, 5, 512]   \n",
            "173_decoder.declayer_stack.4.addandnorm_self_MH...        [512]   [2, 5, 512]   \n",
            "174_decoder.declayer_stack.4.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "175_decoder.declayer_stack.4.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "176_decoder.declayer_stack.4.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "177_decoder.declayer_stack.4.enc_MHA_unit.sdpa....            -  [2, 8, 5, 5]   \n",
            "178_decoder.declayer_stack.4.enc_MHA_unit.Dropo...            -  [2, 8, 5, 5]   \n",
            "179_decoder.declayer_stack.4.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "180_decoder.declayer_stack.4.enc_MHA_unit.Dropo...            -   [2, 5, 512]   \n",
            "181_decoder.declayer_stack.4.addandnorm_enc_MHA...        [512]   [2, 5, 512]   \n",
            "182_decoder.declayer_stack.4.PWFFN.Linear_fc_1      [512, 2048]  [2, 5, 2048]   \n",
            "183_decoder.declayer_stack.4.PWFFN.ReLU_relu                  -  [2, 5, 2048]   \n",
            "184_decoder.declayer_stack.4.PWFFN.Linear_fc_2      [2048, 512]   [2, 5, 512]   \n",
            "185_decoder.declayer_stack.4.PWFFN.Dropout_dropout            -   [2, 5, 512]   \n",
            "186_decoder.declayer_stack.4.addandnorm_PWFFN.L...        [512]   [2, 5, 512]   \n",
            "187_decoder.declayer_stack.5.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "188_decoder.declayer_stack.5.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "189_decoder.declayer_stack.5.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "190_decoder.declayer_stack.5.self_MHA_unit.sdpa...            -  [2, 8, 5, 5]   \n",
            "191_decoder.declayer_stack.5.self_MHA_unit.Drop...            -  [2, 8, 5, 5]   \n",
            "192_decoder.declayer_stack.5.self_MHA_unit.Line...   [512, 512]   [2, 5, 512]   \n",
            "193_decoder.declayer_stack.5.self_MHA_unit.Drop...            -   [2, 5, 512]   \n",
            "194_decoder.declayer_stack.5.addandnorm_self_MH...        [512]   [2, 5, 512]   \n",
            "195_decoder.declayer_stack.5.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "196_decoder.declayer_stack.5.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "197_decoder.declayer_stack.5.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "198_decoder.declayer_stack.5.enc_MHA_unit.sdpa....            -  [2, 8, 5, 5]   \n",
            "199_decoder.declayer_stack.5.enc_MHA_unit.Dropo...            -  [2, 8, 5, 5]   \n",
            "200_decoder.declayer_stack.5.enc_MHA_unit.Linea...   [512, 512]   [2, 5, 512]   \n",
            "201_decoder.declayer_stack.5.enc_MHA_unit.Dropo...            -   [2, 5, 512]   \n",
            "202_decoder.declayer_stack.5.addandnorm_enc_MHA...        [512]   [2, 5, 512]   \n",
            "203_decoder.declayer_stack.5.PWFFN.Linear_fc_1      [512, 2048]  [2, 5, 2048]   \n",
            "204_decoder.declayer_stack.5.PWFFN.ReLU_relu                  -  [2, 5, 2048]   \n",
            "205_decoder.declayer_stack.5.PWFFN.Linear_fc_2      [2048, 512]   [2, 5, 512]   \n",
            "206_decoder.declayer_stack.5.PWFFN.Dropout_dropout            -   [2, 5, 512]   \n",
            "207_decoder.declayer_stack.5.addandnorm_PWFFN.L...        [512]   [2, 5, 512]   \n",
            "\n",
            "                                                       Params  Mult-Adds  \n",
            "Layer                                                                     \n",
            "0_src_embed.Embedding_emb                              5.632k     5.632k  \n",
            "1_encoder.LayerNorm_norm                               1.024k      512.0  \n",
            "2_encoder.enclayer_stack.0.MHA_unit.Linear_wi_q      262.144k   262.144k  \n",
            "3_encoder.enclayer_stack.0.MHA_unit.Linear_wi_k      262.144k   262.144k  \n",
            "4_encoder.enclayer_stack.0.MHA_unit.Linear_wi_v      262.144k   262.144k  \n",
            "5_encoder.enclayer_stack.0.MHA_unit.sdpa.Softma...          -          -  \n",
            "6_encoder.enclayer_stack.0.MHA_unit.Dropout_dro...          -          -  \n",
            "7_encoder.enclayer_stack.0.MHA_unit.Linear_fc        262.144k   262.144k  \n",
            "8_encoder.enclayer_stack.0.MHA_unit.Dropout_dro...          -          -  \n",
            "9_encoder.enclayer_stack.0.addandnorm_MHA.Layer...     1.024k      512.0  \n",
            "10_encoder.enclayer_stack.0.PWFFN.Linear_fc_1       1.050624M  1.048576M  \n",
            "11_encoder.enclayer_stack.0.PWFFN.ReLU_relu                 -          -  \n",
            "12_encoder.enclayer_stack.0.PWFFN.Linear_fc_2       1.049088M  1.048576M  \n",
            "13_encoder.enclayer_stack.0.PWFFN.Dropout_dropout           -          -  \n",
            "14_encoder.enclayer_stack.0.addandnorm_PWFFN.La...     1.024k      512.0  \n",
            "15_encoder.enclayer_stack.1.MHA_unit.Linear_wi_q     262.144k   262.144k  \n",
            "16_encoder.enclayer_stack.1.MHA_unit.Linear_wi_k     262.144k   262.144k  \n",
            "17_encoder.enclayer_stack.1.MHA_unit.Linear_wi_v     262.144k   262.144k  \n",
            "18_encoder.enclayer_stack.1.MHA_unit.sdpa.Softm...          -          -  \n",
            "19_encoder.enclayer_stack.1.MHA_unit.Dropout_dr...          -          -  \n",
            "20_encoder.enclayer_stack.1.MHA_unit.Linear_fc       262.144k   262.144k  \n",
            "21_encoder.enclayer_stack.1.MHA_unit.Dropout_dr...          -          -  \n",
            "22_encoder.enclayer_stack.1.addandnorm_MHA.Laye...     1.024k      512.0  \n",
            "23_encoder.enclayer_stack.1.PWFFN.Linear_fc_1       1.050624M  1.048576M  \n",
            "24_encoder.enclayer_stack.1.PWFFN.ReLU_relu                 -          -  \n",
            "25_encoder.enclayer_stack.1.PWFFN.Linear_fc_2       1.049088M  1.048576M  \n",
            "26_encoder.enclayer_stack.1.PWFFN.Dropout_dropout           -          -  \n",
            "27_encoder.enclayer_stack.1.addandnorm_PWFFN.La...     1.024k      512.0  \n",
            "28_encoder.enclayer_stack.2.MHA_unit.Linear_wi_q     262.144k   262.144k  \n",
            "29_encoder.enclayer_stack.2.MHA_unit.Linear_wi_k     262.144k   262.144k  \n",
            "30_encoder.enclayer_stack.2.MHA_unit.Linear_wi_v     262.144k   262.144k  \n",
            "31_encoder.enclayer_stack.2.MHA_unit.sdpa.Softm...          -          -  \n",
            "32_encoder.enclayer_stack.2.MHA_unit.Dropout_dr...          -          -  \n",
            "33_encoder.enclayer_stack.2.MHA_unit.Linear_fc       262.144k   262.144k  \n",
            "34_encoder.enclayer_stack.2.MHA_unit.Dropout_dr...          -          -  \n",
            "35_encoder.enclayer_stack.2.addandnorm_MHA.Laye...     1.024k      512.0  \n",
            "36_encoder.enclayer_stack.2.PWFFN.Linear_fc_1       1.050624M  1.048576M  \n",
            "37_encoder.enclayer_stack.2.PWFFN.ReLU_relu                 -          -  \n",
            "38_encoder.enclayer_stack.2.PWFFN.Linear_fc_2       1.049088M  1.048576M  \n",
            "39_encoder.enclayer_stack.2.PWFFN.Dropout_dropout           -          -  \n",
            "40_encoder.enclayer_stack.2.addandnorm_PWFFN.La...     1.024k      512.0  \n",
            "41_encoder.enclayer_stack.3.MHA_unit.Linear_wi_q     262.144k   262.144k  \n",
            "42_encoder.enclayer_stack.3.MHA_unit.Linear_wi_k     262.144k   262.144k  \n",
            "43_encoder.enclayer_stack.3.MHA_unit.Linear_wi_v     262.144k   262.144k  \n",
            "44_encoder.enclayer_stack.3.MHA_unit.sdpa.Softm...          -          -  \n",
            "45_encoder.enclayer_stack.3.MHA_unit.Dropout_dr...          -          -  \n",
            "46_encoder.enclayer_stack.3.MHA_unit.Linear_fc       262.144k   262.144k  \n",
            "47_encoder.enclayer_stack.3.MHA_unit.Dropout_dr...          -          -  \n",
            "48_encoder.enclayer_stack.3.addandnorm_MHA.Laye...     1.024k      512.0  \n",
            "49_encoder.enclayer_stack.3.PWFFN.Linear_fc_1       1.050624M  1.048576M  \n",
            "50_encoder.enclayer_stack.3.PWFFN.ReLU_relu                 -          -  \n",
            "51_encoder.enclayer_stack.3.PWFFN.Linear_fc_2       1.049088M  1.048576M  \n",
            "52_encoder.enclayer_stack.3.PWFFN.Dropout_dropout           -          -  \n",
            "53_encoder.enclayer_stack.3.addandnorm_PWFFN.La...     1.024k      512.0  \n",
            "54_encoder.enclayer_stack.4.MHA_unit.Linear_wi_q     262.144k   262.144k  \n",
            "55_encoder.enclayer_stack.4.MHA_unit.Linear_wi_k     262.144k   262.144k  \n",
            "56_encoder.enclayer_stack.4.MHA_unit.Linear_wi_v     262.144k   262.144k  \n",
            "57_encoder.enclayer_stack.4.MHA_unit.sdpa.Softm...          -          -  \n",
            "58_encoder.enclayer_stack.4.MHA_unit.Dropout_dr...          -          -  \n",
            "59_encoder.enclayer_stack.4.MHA_unit.Linear_fc       262.144k   262.144k  \n",
            "60_encoder.enclayer_stack.4.MHA_unit.Dropout_dr...          -          -  \n",
            "61_encoder.enclayer_stack.4.addandnorm_MHA.Laye...     1.024k      512.0  \n",
            "62_encoder.enclayer_stack.4.PWFFN.Linear_fc_1       1.050624M  1.048576M  \n",
            "63_encoder.enclayer_stack.4.PWFFN.ReLU_relu                 -          -  \n",
            "64_encoder.enclayer_stack.4.PWFFN.Linear_fc_2       1.049088M  1.048576M  \n",
            "65_encoder.enclayer_stack.4.PWFFN.Dropout_dropout           -          -  \n",
            "66_encoder.enclayer_stack.4.addandnorm_PWFFN.La...     1.024k      512.0  \n",
            "67_encoder.enclayer_stack.5.MHA_unit.Linear_wi_q     262.144k   262.144k  \n",
            "68_encoder.enclayer_stack.5.MHA_unit.Linear_wi_k     262.144k   262.144k  \n",
            "69_encoder.enclayer_stack.5.MHA_unit.Linear_wi_v     262.144k   262.144k  \n",
            "70_encoder.enclayer_stack.5.MHA_unit.sdpa.Softm...          -          -  \n",
            "71_encoder.enclayer_stack.5.MHA_unit.Dropout_dr...          -          -  \n",
            "72_encoder.enclayer_stack.5.MHA_unit.Linear_fc       262.144k   262.144k  \n",
            "73_encoder.enclayer_stack.5.MHA_unit.Dropout_dr...          -          -  \n",
            "74_encoder.enclayer_stack.5.addandnorm_MHA.Laye...     1.024k      512.0  \n",
            "75_encoder.enclayer_stack.5.PWFFN.Linear_fc_1       1.050624M  1.048576M  \n",
            "76_encoder.enclayer_stack.5.PWFFN.ReLU_relu                 -          -  \n",
            "77_encoder.enclayer_stack.5.PWFFN.Linear_fc_2       1.049088M  1.048576M  \n",
            "78_encoder.enclayer_stack.5.PWFFN.Dropout_dropout           -          -  \n",
            "79_encoder.enclayer_stack.5.addandnorm_PWFFN.La...     1.024k      512.0  \n",
            "80_tgt_embed.Embedding_emb                             5.632k     5.632k  \n",
            "81_decoder.LayerNorm_norm                              1.024k      512.0  \n",
            "82_decoder.declayer_stack.0.self_MHA_unit.Linea...   262.144k   262.144k  \n",
            "83_decoder.declayer_stack.0.self_MHA_unit.Linea...   262.144k   262.144k  \n",
            "84_decoder.declayer_stack.0.self_MHA_unit.Linea...   262.144k   262.144k  \n",
            "85_decoder.declayer_stack.0.self_MHA_unit.sdpa....          -          -  \n",
            "86_decoder.declayer_stack.0.self_MHA_unit.Dropo...          -          -  \n",
            "87_decoder.declayer_stack.0.self_MHA_unit.Linea...   262.144k   262.144k  \n",
            "88_decoder.declayer_stack.0.self_MHA_unit.Dropo...          -          -  \n",
            "89_decoder.declayer_stack.0.addandnorm_self_MHA...     1.024k      512.0  \n",
            "90_decoder.declayer_stack.0.enc_MHA_unit.Linear...   262.144k   262.144k  \n",
            "91_decoder.declayer_stack.0.enc_MHA_unit.Linear...   262.144k   262.144k  \n",
            "92_decoder.declayer_stack.0.enc_MHA_unit.Linear...   262.144k   262.144k  \n",
            "93_decoder.declayer_stack.0.enc_MHA_unit.sdpa.S...          -          -  \n",
            "94_decoder.declayer_stack.0.enc_MHA_unit.Dropou...          -          -  \n",
            "95_decoder.declayer_stack.0.enc_MHA_unit.Linear_fc   262.144k   262.144k  \n",
            "96_decoder.declayer_stack.0.enc_MHA_unit.Dropou...          -          -  \n",
            "97_decoder.declayer_stack.0.addandnorm_enc_MHA....     1.024k      512.0  \n",
            "98_decoder.declayer_stack.0.PWFFN.Linear_fc_1       1.050624M  1.048576M  \n",
            "99_decoder.declayer_stack.0.PWFFN.ReLU_relu                 -          -  \n",
            "100_decoder.declayer_stack.0.PWFFN.Linear_fc_2      1.049088M  1.048576M  \n",
            "101_decoder.declayer_stack.0.PWFFN.Dropout_dropout          -          -  \n",
            "102_decoder.declayer_stack.0.addandnorm_PWFFN.L...     1.024k      512.0  \n",
            "103_decoder.declayer_stack.1.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "104_decoder.declayer_stack.1.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "105_decoder.declayer_stack.1.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "106_decoder.declayer_stack.1.self_MHA_unit.sdpa...          -          -  \n",
            "107_decoder.declayer_stack.1.self_MHA_unit.Drop...          -          -  \n",
            "108_decoder.declayer_stack.1.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "109_decoder.declayer_stack.1.self_MHA_unit.Drop...          -          -  \n",
            "110_decoder.declayer_stack.1.addandnorm_self_MH...     1.024k      512.0  \n",
            "111_decoder.declayer_stack.1.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "112_decoder.declayer_stack.1.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "113_decoder.declayer_stack.1.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "114_decoder.declayer_stack.1.enc_MHA_unit.sdpa....          -          -  \n",
            "115_decoder.declayer_stack.1.enc_MHA_unit.Dropo...          -          -  \n",
            "116_decoder.declayer_stack.1.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "117_decoder.declayer_stack.1.enc_MHA_unit.Dropo...          -          -  \n",
            "118_decoder.declayer_stack.1.addandnorm_enc_MHA...     1.024k      512.0  \n",
            "119_decoder.declayer_stack.1.PWFFN.Linear_fc_1      1.050624M  1.048576M  \n",
            "120_decoder.declayer_stack.1.PWFFN.ReLU_relu                -          -  \n",
            "121_decoder.declayer_stack.1.PWFFN.Linear_fc_2      1.049088M  1.048576M  \n",
            "122_decoder.declayer_stack.1.PWFFN.Dropout_dropout          -          -  \n",
            "123_decoder.declayer_stack.1.addandnorm_PWFFN.L...     1.024k      512.0  \n",
            "124_decoder.declayer_stack.2.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "125_decoder.declayer_stack.2.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "126_decoder.declayer_stack.2.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "127_decoder.declayer_stack.2.self_MHA_unit.sdpa...          -          -  \n",
            "128_decoder.declayer_stack.2.self_MHA_unit.Drop...          -          -  \n",
            "129_decoder.declayer_stack.2.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "130_decoder.declayer_stack.2.self_MHA_unit.Drop...          -          -  \n",
            "131_decoder.declayer_stack.2.addandnorm_self_MH...     1.024k      512.0  \n",
            "132_decoder.declayer_stack.2.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "133_decoder.declayer_stack.2.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "134_decoder.declayer_stack.2.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "135_decoder.declayer_stack.2.enc_MHA_unit.sdpa....          -          -  \n",
            "136_decoder.declayer_stack.2.enc_MHA_unit.Dropo...          -          -  \n",
            "137_decoder.declayer_stack.2.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "138_decoder.declayer_stack.2.enc_MHA_unit.Dropo...          -          -  \n",
            "139_decoder.declayer_stack.2.addandnorm_enc_MHA...     1.024k      512.0  \n",
            "140_decoder.declayer_stack.2.PWFFN.Linear_fc_1      1.050624M  1.048576M  \n",
            "141_decoder.declayer_stack.2.PWFFN.ReLU_relu                -          -  \n",
            "142_decoder.declayer_stack.2.PWFFN.Linear_fc_2      1.049088M  1.048576M  \n",
            "143_decoder.declayer_stack.2.PWFFN.Dropout_dropout          -          -  \n",
            "144_decoder.declayer_stack.2.addandnorm_PWFFN.L...     1.024k      512.0  \n",
            "145_decoder.declayer_stack.3.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "146_decoder.declayer_stack.3.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "147_decoder.declayer_stack.3.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "148_decoder.declayer_stack.3.self_MHA_unit.sdpa...          -          -  \n",
            "149_decoder.declayer_stack.3.self_MHA_unit.Drop...          -          -  \n",
            "150_decoder.declayer_stack.3.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "151_decoder.declayer_stack.3.self_MHA_unit.Drop...          -          -  \n",
            "152_decoder.declayer_stack.3.addandnorm_self_MH...     1.024k      512.0  \n",
            "153_decoder.declayer_stack.3.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "154_decoder.declayer_stack.3.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "155_decoder.declayer_stack.3.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "156_decoder.declayer_stack.3.enc_MHA_unit.sdpa....          -          -  \n",
            "157_decoder.declayer_stack.3.enc_MHA_unit.Dropo...          -          -  \n",
            "158_decoder.declayer_stack.3.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "159_decoder.declayer_stack.3.enc_MHA_unit.Dropo...          -          -  \n",
            "160_decoder.declayer_stack.3.addandnorm_enc_MHA...     1.024k      512.0  \n",
            "161_decoder.declayer_stack.3.PWFFN.Linear_fc_1      1.050624M  1.048576M  \n",
            "162_decoder.declayer_stack.3.PWFFN.ReLU_relu                -          -  \n",
            "163_decoder.declayer_stack.3.PWFFN.Linear_fc_2      1.049088M  1.048576M  \n",
            "164_decoder.declayer_stack.3.PWFFN.Dropout_dropout          -          -  \n",
            "165_decoder.declayer_stack.3.addandnorm_PWFFN.L...     1.024k      512.0  \n",
            "166_decoder.declayer_stack.4.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "167_decoder.declayer_stack.4.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "168_decoder.declayer_stack.4.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "169_decoder.declayer_stack.4.self_MHA_unit.sdpa...          -          -  \n",
            "170_decoder.declayer_stack.4.self_MHA_unit.Drop...          -          -  \n",
            "171_decoder.declayer_stack.4.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "172_decoder.declayer_stack.4.self_MHA_unit.Drop...          -          -  \n",
            "173_decoder.declayer_stack.4.addandnorm_self_MH...     1.024k      512.0  \n",
            "174_decoder.declayer_stack.4.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "175_decoder.declayer_stack.4.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "176_decoder.declayer_stack.4.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "177_decoder.declayer_stack.4.enc_MHA_unit.sdpa....          -          -  \n",
            "178_decoder.declayer_stack.4.enc_MHA_unit.Dropo...          -          -  \n",
            "179_decoder.declayer_stack.4.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "180_decoder.declayer_stack.4.enc_MHA_unit.Dropo...          -          -  \n",
            "181_decoder.declayer_stack.4.addandnorm_enc_MHA...     1.024k      512.0  \n",
            "182_decoder.declayer_stack.4.PWFFN.Linear_fc_1      1.050624M  1.048576M  \n",
            "183_decoder.declayer_stack.4.PWFFN.ReLU_relu                -          -  \n",
            "184_decoder.declayer_stack.4.PWFFN.Linear_fc_2      1.049088M  1.048576M  \n",
            "185_decoder.declayer_stack.4.PWFFN.Dropout_dropout          -          -  \n",
            "186_decoder.declayer_stack.4.addandnorm_PWFFN.L...     1.024k      512.0  \n",
            "187_decoder.declayer_stack.5.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "188_decoder.declayer_stack.5.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "189_decoder.declayer_stack.5.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "190_decoder.declayer_stack.5.self_MHA_unit.sdpa...          -          -  \n",
            "191_decoder.declayer_stack.5.self_MHA_unit.Drop...          -          -  \n",
            "192_decoder.declayer_stack.5.self_MHA_unit.Line...   262.144k   262.144k  \n",
            "193_decoder.declayer_stack.5.self_MHA_unit.Drop...          -          -  \n",
            "194_decoder.declayer_stack.5.addandnorm_self_MH...     1.024k      512.0  \n",
            "195_decoder.declayer_stack.5.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "196_decoder.declayer_stack.5.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "197_decoder.declayer_stack.5.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "198_decoder.declayer_stack.5.enc_MHA_unit.sdpa....          -          -  \n",
            "199_decoder.declayer_stack.5.enc_MHA_unit.Dropo...          -          -  \n",
            "200_decoder.declayer_stack.5.enc_MHA_unit.Linea...   262.144k   262.144k  \n",
            "201_decoder.declayer_stack.5.enc_MHA_unit.Dropo...          -          -  \n",
            "202_decoder.declayer_stack.5.addandnorm_enc_MHA...     1.024k      512.0  \n",
            "203_decoder.declayer_stack.5.PWFFN.Linear_fc_1      1.050624M  1.048576M  \n",
            "204_decoder.declayer_stack.5.PWFFN.ReLU_relu                -          -  \n",
            "205_decoder.declayer_stack.5.PWFFN.Linear_fc_2      1.049088M  1.048576M  \n",
            "206_decoder.declayer_stack.5.PWFFN.Dropout_dropout          -          -  \n",
            "207_decoder.declayer_stack.5.addandnorm_PWFFN.L...     1.024k      512.0  \n",
            "----------------------------------------------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params          44.114944M\n",
            "Trainable params      44.114944M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds              44.06784M\n",
            "================================================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   Kernel Shape  Output Shape  \\\n",
              "Layer                                                                           \n",
              "0_src_embed.Embedding_emb                             [512, 11]   [2, 5, 512]   \n",
              "1_encoder.LayerNorm_norm                                  [512]   [2, 5, 512]   \n",
              "2_encoder.enclayer_stack.0.MHA_unit.Linear_wi_q      [512, 512]   [2, 5, 512]   \n",
              "3_encoder.enclayer_stack.0.MHA_unit.Linear_wi_k      [512, 512]   [2, 5, 512]   \n",
              "4_encoder.enclayer_stack.0.MHA_unit.Linear_wi_v      [512, 512]   [2, 5, 512]   \n",
              "...                                                         ...           ...   \n",
              "203_decoder.declayer_stack.5.PWFFN.Linear_fc_1      [512, 2048]  [2, 5, 2048]   \n",
              "204_decoder.declayer_stack.5.PWFFN.ReLU_relu                  -  [2, 5, 2048]   \n",
              "205_decoder.declayer_stack.5.PWFFN.Linear_fc_2      [2048, 512]   [2, 5, 512]   \n",
              "206_decoder.declayer_stack.5.PWFFN.Dropout_dropout            -   [2, 5, 512]   \n",
              "207_decoder.declayer_stack.5.addandnorm_PWFFN.L...        [512]   [2, 5, 512]   \n",
              "\n",
              "                                                       Params  Mult-Adds  \n",
              "Layer                                                                     \n",
              "0_src_embed.Embedding_emb                              5632.0     5632.0  \n",
              "1_encoder.LayerNorm_norm                               1024.0      512.0  \n",
              "2_encoder.enclayer_stack.0.MHA_unit.Linear_wi_q      262144.0   262144.0  \n",
              "3_encoder.enclayer_stack.0.MHA_unit.Linear_wi_k      262144.0   262144.0  \n",
              "4_encoder.enclayer_stack.0.MHA_unit.Linear_wi_v      262144.0   262144.0  \n",
              "...                                                       ...        ...  \n",
              "203_decoder.declayer_stack.5.PWFFN.Linear_fc_1      1050624.0  1048576.0  \n",
              "204_decoder.declayer_stack.5.PWFFN.ReLU_relu              NaN        NaN  \n",
              "205_decoder.declayer_stack.5.PWFFN.Linear_fc_2      1049088.0  1048576.0  \n",
              "206_decoder.declayer_stack.5.PWFFN.Dropout_dropout        NaN        NaN  \n",
              "207_decoder.declayer_stack.5.addandnorm_PWFFN.L...     1024.0      512.0  \n",
              "\n",
              "[208 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa719f97-92ab-40e5-84df-42374fc61ddd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_src_embed.Embedding_emb</th>\n",
              "      <td>[512, 11]</td>\n",
              "      <td>[2, 5, 512]</td>\n",
              "      <td>5632.0</td>\n",
              "      <td>5632.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_encoder.LayerNorm_norm</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[2, 5, 512]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_encoder.enclayer_stack.0.MHA_unit.Linear_wi_q</th>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>[2, 5, 512]</td>\n",
              "      <td>262144.0</td>\n",
              "      <td>262144.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_encoder.enclayer_stack.0.MHA_unit.Linear_wi_k</th>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>[2, 5, 512]</td>\n",
              "      <td>262144.0</td>\n",
              "      <td>262144.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_encoder.enclayer_stack.0.MHA_unit.Linear_wi_v</th>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>[2, 5, 512]</td>\n",
              "      <td>262144.0</td>\n",
              "      <td>262144.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203_decoder.declayer_stack.5.PWFFN.Linear_fc_1</th>\n",
              "      <td>[512, 2048]</td>\n",
              "      <td>[2, 5, 2048]</td>\n",
              "      <td>1050624.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204_decoder.declayer_stack.5.PWFFN.ReLU_relu</th>\n",
              "      <td>-</td>\n",
              "      <td>[2, 5, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205_decoder.declayer_stack.5.PWFFN.Linear_fc_2</th>\n",
              "      <td>[2048, 512]</td>\n",
              "      <td>[2, 5, 512]</td>\n",
              "      <td>1049088.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206_decoder.declayer_stack.5.PWFFN.Dropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[2, 5, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207_decoder.declayer_stack.5.addandnorm_PWFFN.LayerNorm_norm</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[2, 5, 512]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>208 rows  4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa719f97-92ab-40e5-84df-42374fc61ddd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aa719f97-92ab-40e5-84df-42374fc61ddd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aa719f97-92ab-40e5-84df-42374fc61ddd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUlOoGM4XtlT"
      },
      "source": [
        "# Backup_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma7LmcBBDayM"
      },
      "source": [
        "## Define classes and Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aFIfv_2RXq9P"
      },
      "source": [
        "# # Small example model.\n",
        "# tmp_model = make_model(10, 10, 2)\n",
        "# tmp_model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "BruCIK5kXq9J"
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cw-bWMGXq9K"
      },
      "source": [
        "# plt.figure(figsize=(5,5))\n",
        "# plt.imshow(subsequent_mask(20)[0])\n",
        "# None"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "C_hrhDpSXq9P"
      },
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = \\\n",
        "                self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "yDpJ74V1Xq9Q"
      },
      "source": [
        "def run_epoch(data_iter, model, loss_compute):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        out = model.forward(batch.src, batch.trg, \n",
        "                            batch.src_mask, batch.trg_mask)\n",
        "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        tokens += batch.ntokens\n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "XxlRc2ooXq9Q"
      },
      "source": [
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ZSMNBwjzXq9R"
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "        \n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkTG4VoeXq9R"
      },
      "source": [
        "# # Three settings of the lrate hyperparameters.\n",
        "# opts = [NoamOpt(512, 1, 4000, None), \n",
        "#         NoamOpt(512, 1, 8000, None),\n",
        "#         NoamOpt(256, 1, 4000, None)]\n",
        "# plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
        "# plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
        "# None"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_cwDcAbQXq9S"
      },
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19Gk4m0NXq9T"
      },
      "source": [
        "# #Example of label smoothing.\n",
        "# crit = LabelSmoothing(5, 0, 0.4)\n",
        "# predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
        "#                              [0, 0.2, 0.7, 0.1, 0], \n",
        "#                              [0, 0.2, 0.7, 0.1, 0]])\n",
        "# v = crit(Variable(predict.log()), \n",
        "#          Variable(torch.LongTensor([2, 1, 0])))\n",
        "\n",
        "# # Show the target distributions expected by the system.\n",
        "# plt.imshow(crit.true_dist)\n",
        "# None"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic5GcDWEXq9T"
      },
      "source": [
        "# crit = LabelSmoothing(5, 0, 0.1)\n",
        "# def loss(x):\n",
        "#     d = x + 3 * 1\n",
        "#     predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
        "#                                  ])\n",
        "#     #print(predict)\n",
        "#     return crit(Variable(predict.log()),\n",
        "#                  Variable(torch.LongTensor([1]))).item()\n",
        "# plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])\n",
        "# None"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DOg6L-rXq9U"
      },
      "source": [
        "## Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "S0aXeGP0Xq9U"
      },
      "source": [
        "def data_gen(V, batch, nbatches):\n",
        "    \"Generate random data for a src-tgt copy task.\"\n",
        "    for i in range(nbatches):\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
        "        data[:, 0] = 1\n",
        "        src = Variable(data, requires_grad=False)\n",
        "        tgt = Variable(data, requires_grad=False)\n",
        "        yield Batch(src, tgt, 0)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULRFkGb8Xq9U"
      },
      "source": [
        "## Loss Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7Ae8ht1RXq9V"
      },
      "source": [
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
        "                              y.contiguous().view(-1)) / norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        #return loss.data[0] * norm\n",
        "        return loss.item() * norm"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_5N7srEXq9V"
      },
      "source": [
        "## Greedy Decoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V = 11\n",
        "model = make_model(V, V, N=2)\n",
        "data_iter = data_gen(V, 2, 1)\n",
        "for i, batch in enumerate(data_iter):\n",
        "  #print(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
        "  print(batch.src.shape, batch.trg.shape, batch.src_mask.shape, batch.trg_mask.shape)"
      ],
      "metadata": {
        "id": "mN6FZqLTq728",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb40db7-1c14-493b-f444-50edcc9d6cc1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10]) torch.Size([2, 9]) torch.Size([2, 1, 10]) torch.Size([2, 9, 9])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/transformer_utils.py:451: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(p)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch.src)\n",
        "print(batch.trg)\n",
        "print(batch.src_mask)\n",
        "print(batch.trg_mask)"
      ],
      "metadata": {
        "id": "Q_nR5pF5PlLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch.src_mask"
      ],
      "metadata": {
        "id": "-GyT9nCz-3i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch.src)\n",
        "print(batch.trg)"
      ],
      "metadata": {
        "id": "itkmfGqA-iAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_out = model.encode(batch.src, batch.src_mask)\n",
        "print(enc_out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xcPMh-SRhXE",
        "outputId": "44d2e2db-70f6-4af0-d9cf-0dd7f6947fcc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_tgt = model.tgt_embed(batch.trg)\n",
        "#model.decoder.declayer(emb_tgt, enc_out, batch.src_mask, batch.trg_mask)\n",
        "layer = model.decoder.declayer.self_MHA_unit.sdpa\n",
        "x = emb_tgt\n",
        "layer(x, x, x, batch.src_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "ebXEeYX8T3t9",
        "outputId": "62f6b41c-bb67-4ec0-8fcf-8e452d3bce10"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-c6a51433122b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeclayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_MHA_unit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb_tgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Q, K, V, attn_mask, attn_dropout)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaling\u001b[0m \u001b[0;31m# Apply Scaling to maintain original variance - Computes Q(K.T)/sqrt(dk). Shape = [nb, h, nw, nw]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mattn_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Apply mask (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m       \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Compute softmax. Shape = [nb, h, nw, nw]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Apply dropout. Shape = [nb, h, nw, nw]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (9) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.decode(batch.trg, enc_out, batch.src_mask, batch.trg_mask)"
      ],
      "metadata": {
        "id": "wYJY1NK3R1sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fANlh8SzXq9V",
        "outputId": "61050143-dcdd-4a17-9716-2e056a636604",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train the simple copy task.\n",
        "V = 11\n",
        "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "model = make_model(V, V, N=2)\n",
        "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
        "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    run_epoch(data_gen(V, 30, 20), model, \n",
        "              SimpleLossCompute(model.generator, criterion, model_opt))\n",
        "    model.eval()\n",
        "    print(run_epoch(data_gen(V, 30, 5), model, \n",
        "                    SimpleLossCompute(model.generator, criterion, None)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/content/transformer_utils.py:451: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(p)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Step: 1 Loss: 3.271187 Tokens per Sec: 429.255920\n",
            "Epoch Step: 1 Loss: 1.921017 Tokens per Sec: 579.715942\n",
            "tensor(1.9087)\n",
            "Epoch Step: 1 Loss: 1.845874 Tokens per Sec: 477.798828\n",
            "Epoch Step: 1 Loss: 1.748855 Tokens per Sec: 402.966034\n",
            "tensor(1.7128)\n",
            "Epoch Step: 1 Loss: 1.907193 Tokens per Sec: 486.636841\n",
            "Epoch Step: 1 Loss: 1.464710 Tokens per Sec: 569.645081\n",
            "tensor(1.4500)\n",
            "Epoch Step: 1 Loss: 2.099236 Tokens per Sec: 481.579163\n",
            "Epoch Step: 1 Loss: 1.196683 Tokens per Sec: 586.728394\n",
            "tensor(1.1900)\n",
            "Epoch Step: 1 Loss: 2.261800 Tokens per Sec: 467.359314\n",
            "Epoch Step: 1 Loss: 1.059663 Tokens per Sec: 582.958618\n",
            "tensor(1.0483)\n",
            "Epoch Step: 1 Loss: 1.648451 Tokens per Sec: 488.679443\n",
            "Epoch Step: 1 Loss: 0.996762 Tokens per Sec: 574.097900\n",
            "tensor(0.9710)\n",
            "Epoch Step: 1 Loss: 2.332916 Tokens per Sec: 481.171051\n",
            "Epoch Step: 1 Loss: 0.866144 Tokens per Sec: 582.970459\n",
            "tensor(0.8783)\n",
            "Epoch Step: 1 Loss: 1.351856 Tokens per Sec: 479.442596\n",
            "Epoch Step: 1 Loss: 0.790241 Tokens per Sec: 587.153992\n",
            "tensor(0.7668)\n",
            "Epoch Step: 1 Loss: 1.220293 Tokens per Sec: 485.356781\n",
            "Epoch Step: 1 Loss: 0.576261 Tokens per Sec: 591.672852\n",
            "tensor(0.5494)\n",
            "Epoch Step: 1 Loss: 2.181015 Tokens per Sec: 481.422150\n",
            "Epoch Step: 1 Loss: 0.707518 Tokens per Sec: 567.154480\n",
            "tensor(0.7194)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcVpHCSZXq9V"
      },
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZM2or8QEE2P",
        "outputId": "e60b1c18-506a-48cd-d3ba-850efea49f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.eval()\n",
        "src = Variable(torch.LongTensor([[1,8,5,7,3,6,7,8,9,10]])).to(device)\n",
        "#src = Variable(torch.LongTensor([[1,8,5,7,3,6,7,8,9,10]]))\n",
        "src_mask = Variable(torch.ones(1, 1, 10)).to(device)\n",
        "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-408318326585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#src = Variable(torch.LongTensor([[1,8,5,7,3,6,7,8,9,10]]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_symbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-9f44eb0aa859>\u001b[0m in \u001b[0;36mgreedy_decode\u001b[0;34m(model, src, src_mask, max_len, start_symbol)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgreedy_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         out = model.decode(memory, src_mask, \n",
            "\u001b[0;32m/content/transformer_utils.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_dec_attn_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/transformer_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0mEmbedded\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m### Class: PositionalEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYTscxa9DPqD"
      },
      "source": [
        "# Model Verification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Y7cPvDD_Rl"
      },
      "source": [
        "mod_input = Variable(torch.LongTensor([[1, 2, 1, 2, 2, 1, 1, 1, 1, 1]]))\n",
        "inp_mask = Variable(torch.ones(1, 1, 10))\n",
        "num_posn = mod_input.size(1) # Number of positions in input\n",
        "d_model = 512 # Embedding size\n",
        "print(mod_input.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpdV2qOfR9eE"
      },
      "source": [
        "## Source Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlxTeduhSAi5"
      },
      "source": [
        "### Embedding Layer output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmDyiFNSEOHP"
      },
      "source": [
        "l_emb_only = model.src_embed.get_submodule(\"0.emb\") # Embedding layer without scaling\n",
        "l_emb_only_out = l_emb_only.forward(mod_input) # Output of above layer\n",
        "l_emb = model.src_embed.get_submodule(\"0\") # Embedding layer with scaling\n",
        "l_emb_out = l_emb.forward(mod_input) # Output of full embedding layer (with scaling)\n",
        "\n",
        "print(l_emb_only_out.shape,',',l_emb_out.shape)\n",
        "print()\n",
        "print(l_emb_only_out[0, :6, :5])\n",
        "print()\n",
        "print(l_emb_out[0, :6, :5])\n",
        "print()\n",
        "print(l_emb_only_out[0, :6, :5] * math.sqrt(d_model))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaORflhBSDx8"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJEuQGTFGRKx"
      },
      "source": [
        "l_pos_enc = model.src_embed.get_submodule(\"1\") # Positional Encoding layer\n",
        "pe_arr = next(l_pos_enc.buffers()) # Extract positional encoding buffer\n",
        "src_emb_calc = l_emb_out + pe_arr[:, :(num_posn)] # Manually compute source embedding\n",
        "src_emb = model.src_embed.forward(mod_input) # Extract source embedding from model\n",
        "\n",
        "print(pe_arr.shape)\n",
        "print()\n",
        "print(pe_arr[0, :6, :5])\n",
        "print()\n",
        "print(src_emb_calc[0, :6, :5])\n",
        "print()\n",
        "print(src_emb[0, :6, :5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGIjrojzI00X"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rTCHtV81blQ"
      },
      "source": [
        "model.encoder.forward(src_emb, inp_mask)[0, :6, :6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-igXPDNLCdz"
      },
      "source": [
        "seaborn.heatmap(model.encoder.enclayer_stack[1].MHA_unit.attn[0, 1].data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRVGQYz-JP4L"
      },
      "source": [
        "memory = model.encode(mod_input, inp_mask)\n",
        "print(memory.shape)\n",
        "print()\n",
        "print(memory[0, :6, :6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqd-HvTLIzXg"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WmXbG_aJkCq"
      },
      "source": [
        "start_symbol = 1\n",
        "max_len = 10\n",
        "tgt = torch.ones(1, 1).fill_(start_symbol).type_as(mod_input.data)\n",
        "print(tgt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1FCJ_obTn3J"
      },
      "source": [
        "for ind in range(max_len - 1):\n",
        "  tgt_mask = Variable(subsequent_mask(tgt.size(1)).type_as(mod_input.data))\n",
        "  tgt = Variable(tgt)\n",
        "  print(ind, tgt, tgt_mask.shape, tgt_mask)\n",
        "  out = model.decode(memory, inp_mask, tgt, tgt_mask)\n",
        "  prob = model.generator(out[:, -1])\n",
        "  _, next_word = torch.max(prob, dim = 1)\n",
        "  next_word = next_word.data[0]\n",
        "  tgt = torch.cat([tgt, torch.ones(1, 1).type_as(mod_input.data).fill_(next_word)], dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAljVuS7b02y"
      },
      "source": [
        "print(tgt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWLc73apL9Au"
      },
      "source": [
        "tgt = Variable(ys)\n",
        "tgt_mask = Variable(subsequent_mask(ys.size(1)).type_as(mod_input.data))\n",
        "print(tgt)\n",
        "print()\n",
        "print(tgt_mask.shape)\n",
        "print()\n",
        "print(tgt_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpt0N_NlLa0m"
      },
      "source": [
        "out = model.decode(memory, inp_mask, tgt, tgt_mask)\n",
        "prob = model.generator(out[:, -1])\n",
        "print(prob)\n",
        "_, next_word = torch.max(prob, dim = 1)\n",
        "next_word = next_word.data[0]\n",
        "ys = torch.cat([ys,torch.ones(1, 1).type_as(mod_input.data).fill_(next_word)], dim=1)\n",
        "ys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJPgvAmGTFm6"
      },
      "source": [
        "tgt = Variable(ys)\n",
        "tgt_mask = Variable(subsequent_mask(ys.size(1)).type_as(mod_input.data))\n",
        "print(tgt)\n",
        "print()\n",
        "print(tgt_mask.shape)\n",
        "print()\n",
        "print(tgt_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmXMK2T5TSeD"
      },
      "source": [
        "out = model.decode(memory, inp_mask, tgt, tgt_mask)\n",
        "prob = model.generator(out[:, -1])\n",
        "print(prob)\n",
        "_, next_word = torch.max(prob, dim = 1)\n",
        "next_word = next_word.data[0]\n",
        "ys = torch.cat([ys,torch.ones(1, 1).type_as(mod_input.data).fill_(next_word)], dim=1)\n",
        "ys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpzqeAHtTf-3"
      },
      "source": [
        "tgt = Variable(ys)\n",
        "tgt_mask = Variable(subsequent_mask(ys.size(1)).type_as(mod_input.data))\n",
        "print(tgt)\n",
        "print()\n",
        "print(tgt_mask.shape)\n",
        "print()\n",
        "print(tgt_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s60lRth6JEpJ"
      },
      "source": [
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RynFrZjDJln"
      },
      "source": [
        "# Backup_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmkUG3JYXq9W"
      },
      "source": [
        "# A Real World Example\n",
        "\n",
        "> Now we consider a real-world example using the IWSLT German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. We also show how to use multi-gpu processing to make it really fast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rLFMHnFqXq9W"
      },
      "source": [
        "#!pip install torchtext spacy\n",
        "#!python -m spacy download en\n",
        "#!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG4pUsT7Xq9W"
      },
      "source": [
        "## Data Loading\n",
        "> We will load the dataset using torchtext and spacy for tokenization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "df4hc1trXq9W"
      },
      "source": [
        "# For data loading.\n",
        "from torchtext import data, datasets\n",
        "\n",
        "if True:\n",
        "    import spacy\n",
        "    spacy_de = spacy.load('de')\n",
        "    spacy_en = spacy.load('en')\n",
        "\n",
        "    def tokenize_de(text):\n",
        "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "    BOS_WORD = '<s>'\n",
        "    EOS_WORD = '</s>'\n",
        "    BLANK_WORD = \"<blank>\"\n",
        "    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
        "    TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
        "                     eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "    MAX_LEN = 100\n",
        "    train, val, test = datasets.IWSLT.splits(\n",
        "        exts=('.de', '.en'), fields=(SRC, TGT), \n",
        "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "            len(vars(x)['trg']) <= MAX_LEN)\n",
        "    MIN_FREQ = 2\n",
        "    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG5hcjj9Xq9W"
      },
      "source": [
        "> Batching matters a ton for speed. We want to have very evenly divided batches, with absolutely minimal padding. To do this we have to hack a bit around the default torchtext batching. This code patches their default batching to make sure we search over enough sentences to find tight batches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkPpP95_Xq9W"
      },
      "source": [
        "## Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FBlqMAiTXq9X"
      },
      "source": [
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    return Batch(src, trg, pad_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "k4IXY8BgXq9X"
      },
      "source": [
        "## Multi-GPU Training\n",
        "\n",
        "> Finally to really target fast training, we will use multi-gpu. This code implements multi-gpu word generation. It is not specific to transformer so I won't go into too much detail. The idea is to split up word generation at training time into chunks to be processed in parallel across many different gpus. We do this using pytorch parallel primitives:\n",
        "\n",
        "* replicate - split modules onto different gpus.\n",
        "* scatter - split batches onto different gpus\n",
        "* parallel_apply - apply module to batches on different gpus\n",
        "* gather - pull scattered data back onto one gpu. \n",
        "* nn.DataParallel - a special module wrapper that calls these all before evaluating. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cAvWWpBhXq9X"
      },
      "source": [
        "# Skip if not interested in multigpu.\n",
        "class MultiGPULossCompute:\n",
        "    \"A multi-gpu loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\n",
        "        # Send out to different gpus.\n",
        "        self.generator = generator\n",
        "        self.criterion = nn.parallel.replicate(criterion, \n",
        "                                               devices=devices)\n",
        "        self.opt = opt\n",
        "        self.devices = devices\n",
        "        self.chunk_size = chunk_size\n",
        "        \n",
        "    def __call__(self, out, targets, normalize):\n",
        "        total = 0.0\n",
        "        generator = nn.parallel.replicate(self.generator, \n",
        "                                                devices=self.devices)\n",
        "        out_scatter = nn.parallel.scatter(out, \n",
        "                                          target_gpus=self.devices)\n",
        "        out_grad = [[] for _ in out_scatter]\n",
        "        targets = nn.parallel.scatter(targets, \n",
        "                                      target_gpus=self.devices)\n",
        "\n",
        "        # Divide generating into chunks.\n",
        "        chunk_size = self.chunk_size\n",
        "        for i in range(0, out_scatter[0].size(1), chunk_size):\n",
        "            # Predict distributions\n",
        "            out_column = [[Variable(o[:, i:i+chunk_size].data, \n",
        "                                    requires_grad=self.opt is not None)] \n",
        "                           for o in out_scatter]\n",
        "            gen = nn.parallel.parallel_apply(generator, out_column)\n",
        "\n",
        "            # Compute loss. \n",
        "            y = [(g.contiguous().view(-1, g.size(-1)), \n",
        "                  t[:, i:i+chunk_size].contiguous().view(-1)) \n",
        "                 for g, t in zip(gen, targets)]\n",
        "            loss = nn.parallel.parallel_apply(self.criterion, y)\n",
        "\n",
        "            # Sum and normalize loss\n",
        "            l = nn.parallel.gather(loss, \n",
        "                                   target_device=self.devices[0])\n",
        "            l = l.sum()[0] / normalize\n",
        "            total += l.data[0]\n",
        "\n",
        "            # Backprop loss to output of transformer\n",
        "            if self.opt is not None:\n",
        "                l.backward()\n",
        "                for j, l in enumerate(loss):\n",
        "                    out_grad[j].append(out_column[j][0].grad.data.clone())\n",
        "\n",
        "        # Backprop all loss through transformer.            \n",
        "        if self.opt is not None:\n",
        "            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n",
        "            o1 = out\n",
        "            o2 = nn.parallel.gather(out_grad, \n",
        "                                    target_device=self.devices[0])\n",
        "            o1.backward(gradient=o2)\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        return total * normalize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJO1OfStXq9X"
      },
      "source": [
        "> Now we create our model, criterion, optimizer, data iterators, and paralelization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "sDzqwkH6Xq9X"
      },
      "source": [
        "# GPUs to use\n",
        "devices = [0, 1, 2, 3]\n",
        "if True:\n",
        "    pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "    model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
        "    model.cuda()\n",
        "    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
        "    criterion.cuda()\n",
        "    BATCH_SIZE = 12000\n",
        "    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                            batch_size_fn=batch_size_fn, train=True)\n",
        "    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                            batch_size_fn=batch_size_fn, train=False)\n",
        "    model_par = nn.DataParallel(model, device_ids=devices)\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOXBIVZ-Xq9Y"
      },
      "source": [
        "> Now we train the model. I will play with the warmup steps a bit, but everything else uses the default parameters.  On an AWS p3.8xlarge with 4 Tesla V100s, this runs at ~27,000 tokens per second with a batch size of 12,000 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVTYJe4UXq9Y"
      },
      "source": [
        "## Training the System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bDezrLqnXq9Y"
      },
      "source": [
        "#!wget https://s3.amazonaws.com/opennmt-models/iwslt.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fGpcKDDiXq9Y"
      },
      "source": [
        "if False:\n",
        "    model_opt = NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "    for epoch in range(10):\n",
        "        model_par.train()\n",
        "        run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
        "                  model_par, \n",
        "                  MultiGPULossCompute(model.generator, criterion, \n",
        "                                      devices=devices, opt=model_opt))\n",
        "        model_par.eval()\n",
        "        loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
        "                          model_par, \n",
        "                          MultiGPULossCompute(model.generator, criterion, \n",
        "                          devices=devices, opt=None))\n",
        "        print(loss)\n",
        "else:\n",
        "    model = torch.load(\"iwslt.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gascy-8zXq9Y"
      },
      "source": [
        "> Once trained we can decode the model to produce a set of translations. Here we simply translate the first sentence in the validation set. This dataset is pretty small so the translations with greedy search are reasonably accurate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2VLz8iMXq9Y"
      },
      "source": [
        "for i, batch in enumerate(valid_iter):\n",
        "    src = batch.src.transpose(0, 1)[:1]\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "    out = greedy_decode(model, src, src_mask, \n",
        "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print(\"Target:\", end=\"\\t\")\n",
        "    for i in range(1, batch.trg.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "R3J-zwLXXq9Z"
      },
      "source": [
        "# Additional Components: BPE, Search, Averaging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7d2mzrlXq9Z"
      },
      "source": [
        "> So this mostly covers the transformer model itself. There are four aspects that we didn't cover explicitly. We also have all these additional features implemented in [OpenNMT-py](https://github.com/opennmt/opennmt-py).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUxdLmPkXq9Z"
      },
      "source": [
        "> 1) BPE/ Word-piece: We can use a library to first preprocess the data into subword units. See Rico Sennrich's [subword-nmt](https://github.com/rsennrich/subword-nmt) implementation. These models will transform the training data to look like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doE_CQ_LXq9Z"
      },
      "source": [
        "Die Protokoll datei kann  heimlich per E - Mail oder FTP an einen bestimmte n Empfnger gesendet werden ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8JSATiSXq9Z"
      },
      "source": [
        "> 2) Shared Embeddings: When using BPE with shared vocabulary we can share the same weight vectors between the source / target / generator. See the [(cite)](https://arxiv.org/abs/1608.05859) for details. To add this to the model simply do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gqJZgf2TXq9Z"
      },
      "source": [
        "if False:\n",
        "    model.src_embed[0].lut.weight = model.tgt_embeddings[0].lut.weight\n",
        "    model.generator.lut.weight = model.tgt_embed[0].lut.weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuhaIOX9Xq9a"
      },
      "source": [
        "> 3) Beam Search: This is a bit too complicated to cover here. See the [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/Beam.py) for a pytorch implementation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiiXO-M6Xq9a"
      },
      "source": [
        "> 4) Model Averaging: The paper averages the last k checkpoints to create an ensembling effect. We can do this after the fact if we have a bunch of models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iZeCLDtvXq9a"
      },
      "source": [
        "def average(model, models):\n",
        "    \"Average models into model\"\n",
        "    for ps in zip(*[m.params() for m in [model] + models]):\n",
        "        p[0].copy_(torch.sum(*ps[1:]) / len(ps[1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OsT2OfFXq9a"
      },
      "source": [
        "# Results\n",
        "\n",
        "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
        "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
        "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\n",
        "listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\n",
        "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
        "the competitive models.\n",
        "\n",
        "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
        "outperforming all of the previously published single models, at less than 1/4 the training cost of the\n",
        "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
        "dropout rate Pdrop = 0.1, instead of 0.3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icUdn2VfXq9a"
      },
      "source": [
        "Image(filename=\"images/results.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4JNgHe4Xq9b"
      },
      "source": [
        "> The code we have written here is a version of the base model. There are fully trained version of this system available here  [(Example Models)](http://opennmt.net/Models-py/).\n",
        ">\n",
        "> With the addtional extensions in the last section, the OpenNMT-py replication gets to 26.9 on EN-DE WMT. Here I have loaded in those parameters to our reimplemenation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ENGPLODDXq9b"
      },
      "source": [
        "!wget https://s3.amazonaws.com/opennmt-models/en-de-model.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_-0aCFrkXq9b"
      },
      "source": [
        "model, SRC, TGT = torch.load(\"en-de-model.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Y30xCLShXq9b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOWY9S1VXq9b"
      },
      "source": [
        "model.eval()\n",
        "sent = \"The log file can be sent secret ly with email or FTP to a specified receiver\".split()\n",
        "src = torch.LongTensor([[SRC.stoi[w] for w in sent]])\n",
        "src = Variable(src)\n",
        "src_mask = (src != SRC.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "out = greedy_decode(model, src, src_mask, \n",
        "                    max_len=60, start_symbol=TGT.stoi[\"<s>\"])\n",
        "print(\"Translation:\", end=\"\\t\")\n",
        "trans = \"<s> \"\n",
        "for i in range(1, out.size(1)):\n",
        "    sym = TGT.itos[out[0, i]]\n",
        "    if sym == \"</s>\": break\n",
        "    trans += sym + \" \"\n",
        "print(trans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cgclZwFXq9b"
      },
      "source": [
        "## Attention Visualization\n",
        "\n",
        "> Even with a greedy decoder the translation looks pretty good. We can further visualize it to see what is happening at each layer of the attention "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_TXuzFFXq9c"
      },
      "source": [
        "tgt_sent = trans.split()\n",
        "def draw(data, x, y, ax):\n",
        "    seaborn.heatmap(data, \n",
        "                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, \n",
        "                    cbar=False, ax=ax)\n",
        "    \n",
        "for layer in range(1, 6, 2):\n",
        "    fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
        "    print(\"Encoder Layer\", layer+1)\n",
        "    for h in range(4):\n",
        "        draw(model.encoder.layers[layer].self_attn.attn[0, h].data, \n",
        "            sent, sent if h ==0 else [], ax=axs[h])\n",
        "    plt.show()\n",
        "    \n",
        "for layer in range(1, 6, 2):\n",
        "    fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
        "    print(\"Decoder Self Layer\", layer+1)\n",
        "    for h in range(4):\n",
        "        draw(model.decoder.layers[layer].self_attn.attn[0, h].data[:len(tgt_sent), :len(tgt_sent)], \n",
        "            tgt_sent, tgt_sent if h ==0 else [], ax=axs[h])\n",
        "    plt.show()\n",
        "    print(\"Decoder Src Layer\", layer+1)\n",
        "    fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
        "    for h in range(4):\n",
        "        draw(model.decoder.layers[layer].self_attn.attn[0, h].data[:len(tgt_sent), :len(sent)], \n",
        "            sent, tgt_sent if h ==0 else [], ax=axs[h])\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MhlgGUqXq9c"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "> Hopefully this code is useful for future research. Please reach out if you have any issues. If you find this code helpful, also check out our other OpenNMT tools.\n",
        "\n",
        "```\n",
        "@inproceedings{opennmt,\n",
        "  author    = {Guillaume Klein and\n",
        "               Yoon Kim and\n",
        "               Yuntian Deng and\n",
        "               Jean Senellart and\n",
        "               Alexander M. Rush},\n",
        "  title     = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},\n",
        "  booktitle = {Proc. ACL},\n",
        "  year      = {2017},\n",
        "  url       = {https://doi.org/10.18653/v1/P17-4012},\n",
        "  doi       = {10.18653/v1/P17-4012}\n",
        "}\n",
        "```\n",
        "\n",
        "> Cheers,\n",
        "> srush"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1-u5nm5Xq9c"
      },
      "source": [
        "{::options parse_block_html=\"true\" /}\n",
        "<div id=\"disqus_thread\"></div>\n",
        "<script>\n",
        "\n",
        "/**\n",
        "*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.\n",
        "*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/\n",
        "/*\n",
        "var disqus_config = function () {\n",
        "this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable\n",
        "this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable\n",
        "};\n",
        "*/\n",
        "(function() { // DON'T EDIT BELOW THIS LINE\n",
        "var d = document, s = d.createElement('script');\n",
        "s.src = 'https://harvard-nlp.disqus.com/embed.js';\n",
        "s.setAttribute('data-timestamp', +new Date());\n",
        "(d.head || d.body).appendChild(s);\n",
        "})();\n",
        "</script>\n",
        "<noscript>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\">comments powered by Disqus.</a></noscript>\n",
        "                            "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZVfU915A-JkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8WJUdrFXq9c"
      },
      "source": [
        "<div id=\"disqus_thread\"></div>\n",
        "<script>\n",
        "    /**\n",
        "     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.\n",
        "     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables\n",
        "     */\n",
        "    /*\n",
        "    var disqus_config = function () {\n",
        "        this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable\n",
        "        this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable\n",
        "    };\n",
        "    */\n",
        "    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW\n",
        "        var d = document, s = d.createElement('script');\n",
        "        \n",
        "        s.src = 'https://EXAMPLE.disqus.com/embed.js';  // IMPORTANT: Replace EXAMPLE with your forum shortname!\n",
        "        \n",
        "        s.setAttribute('data-timestamp', +new Date());\n",
        "        (d.head || d.body).appendChild(s);\n",
        "    })();\n",
        "</script>\n",
        "<noscript>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\" rel=\"nofollow\">comments powered by Disqus.</a></noscript>"
      ]
    }
  ]
}