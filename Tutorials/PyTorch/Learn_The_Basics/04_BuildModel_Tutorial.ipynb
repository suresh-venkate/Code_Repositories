{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network\n",
    "\n",
    "The [`torch.nn`](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks needed to build our own neural network. Every module in PyTorch subclasses the [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU availability\n",
    "\n",
    "We want to be able to train our model on a hardware accelerator like the GPU, if it is available. Let's check to see if [`torch.cuda`](https://pytorch.org/docs/stable/notes/cuda.html) is available, else we continue to use the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Model Layers\n",
    "\n",
    "Lets take a sample minibatch of 3 images of size 28 x 28 and see what happens to it as we pass it through the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input_image is torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(f\"Size of input_image is {input_image.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`nn.Flatten`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer converts each 2D 28 x 28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim = 0) is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of flat_image is torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten() # Define flatten layer\n",
    "flat_image = flatten(input_image) # Get output of flatten layer\n",
    "print(f\"Shape of flat_image is {flat_image.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layer\n",
    "\n",
    "The [`linear layer`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using it's stored weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of hidden1 is torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features = 28 * 28, out_features = 20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(f\"Shape of hidden1 is {hidden1.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "[`nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.3560,  0.4836, -0.1674, -0.1033,  0.3106, -0.3254,  0.3303,  0.2531,\n",
      "         -0.2452, -0.1073, -0.3930,  0.3560,  0.4534, -0.1133, -0.0222,  0.1346,\n",
      "         -0.3885, -0.3978, -0.2721, -0.1094],\n",
      "        [-0.3520,  0.4087, -0.2760, -0.0027,  0.1285, -0.3497,  0.0465,  0.6129,\n",
      "         -0.3857, -0.2560, -0.6055,  0.1396,  0.6784, -0.0702,  0.2206,  0.1016,\n",
      "         -0.2497, -0.1468, -0.3845,  0.1320],\n",
      "        [-0.0162,  0.5943,  0.3098,  0.0744,  0.4060, -0.6053,  0.3440,  0.3050,\n",
      "         -0.4018, -0.2303, -0.2049, -0.0671,  0.6359, -0.3214,  0.3185, -0.1236,\n",
      "         -0.5617,  0.0919, -0.1434,  0.0260]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.4836, 0.0000, 0.0000, 0.3106, 0.0000, 0.3303, 0.2531, 0.0000,\n",
      "         0.0000, 0.0000, 0.3560, 0.4534, 0.0000, 0.0000, 0.1346, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0000, 0.4087, 0.0000, 0.0000, 0.1285, 0.0000, 0.0465, 0.6129, 0.0000,\n",
      "         0.0000, 0.0000, 0.1396, 0.6784, 0.0000, 0.2206, 0.1016, 0.0000, 0.0000,\n",
      "         0.0000, 0.1320],\n",
      "        [0.0000, 0.5943, 0.3098, 0.0744, 0.4060, 0.0000, 0.3440, 0.3050, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.6359, 0.0000, 0.3185, 0.0000, 0.0000, 0.0919,\n",
      "         0.0000, 0.0260]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered \n",
    "container of modules. The data is passed through all the modules in the same order as defined. We can use sequential containers to put together a quick network like ``seq_modules``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits is torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)\n",
    "print(f\"Shape of logits is {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax layer\n",
    "\n",
    "The last linear layer of the neural network returns `logits` - raw values in [-\\infty, \\infty] - which are passed to the [`nn.Softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values [0, 1] representing the model's predicted probabilities for each class. ``dim`` parameter indicates the dimension along which the values must sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pred_probab is torch.Size([3, 10])\n",
      "Value of pred_probab[0] is tensor([0.0986, 0.0830, 0.0724, 0.1274, 0.1200, 0.0969, 0.1057, 0.0987, 0.1036,\n",
      "        0.0938], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim = 1)\n",
    "pred_probab = softmax(logits)\n",
    "print(f\"Shape of pred_probab is {pred_probab.shape}\")\n",
    "print(f\"Value of pred_probab[0] is {pred_probab[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the NN Class\n",
    "\n",
    "We define our neural network by subclassing ``nn.Module``, and initialize the neural network layers in ``__init__``. Every ``nn.Module`` subclass implements the operations on input data in the ``forward`` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512), # First hidden layer with 512 output neurons\n",
    "            nn.ReLU(), # ReLU activation for first hidden layer\n",
    "            nn.Linear(512, 512), # Second hidden layer with 512 output neurons\n",
    "            nn.ReLU(), # ReLU activation for second hidden layer\n",
    "            nn.Linear(512, 10), # Output Layer with 10 output neurons\n",
    "            nn.ReLU() # ReLU activation for output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model\n",
    "\n",
    "We create an instance of ``NeuralNetwork``, and move it to the ``device``, and print its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 512]         401,920\n",
      "              ReLU-3                  [-1, 512]               0\n",
      "            Linear-4                  [-1, 512]         262,656\n",
      "              ReLU-5                  [-1, 512]               0\n",
      "            Linear-6                   [-1, 10]           5,130\n",
      "              ReLU-7                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 2.55\n",
      "Estimated Total Size (MB): 2.58\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(summary(model, input_size = (1, 28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass on model\n",
    "\n",
    "To use the model, we pass it the input data. This executes the model's ``forward``, along with some [`background operations`](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866)\n",
    "\n",
    "Do not call ``model.forward()`` directly!\n",
    "\n",
    "Calling the model on the input returns a 10-dimensional tensor with raw predicted values for each class. We get the prediction probabilities by passing it through an instance of the ``nn.Softmax`` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits is torch.Size([1, 10])\n",
      "Predicted class: tensor([5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device = device)\n",
    "logits = model(X)\n",
    "print(f\"Shape of logits is {logits.shape}\")\n",
    "pred_probab = nn.Softmax(dim = 1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters\n",
    "\n",
    "Many layers inside a neural network are *parameterized*, i.e. have associated weights and biases that are optimized during training. Subclassing ``nn.Module`` automatically tracks all fields defined inside the model object, and makes all parameters accessible using our model's ``parameters()`` or ``named_parameters()`` methods. \n",
    "\n",
    "In the below section, we iterate over each parameter, and print its size and a preview of its values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# First print model structure\n",
    "print(\"Model Structure:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of layer.parameter is linear_relu_stack.0.weight\n",
      "Size of layer.parameter is torch.Size([512, 784])\n",
      "\n",
      "Name of layer.parameter is linear_relu_stack.0.bias\n",
      "Size of layer.parameter is torch.Size([512])\n",
      "\n",
      "Name of layer.parameter is linear_relu_stack.2.weight\n",
      "Size of layer.parameter is torch.Size([512, 512])\n",
      "\n",
      "Name of layer.parameter is linear_relu_stack.2.bias\n",
      "Size of layer.parameter is torch.Size([512])\n",
      "\n",
      "Name of layer.parameter is linear_relu_stack.4.weight\n",
      "Size of layer.parameter is torch.Size([10, 512])\n",
      "\n",
      "Name of layer.parameter is linear_relu_stack.4.bias\n",
      "Size of layer.parameter is torch.Size([10])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Name of layer.parameter is {name}\")\n",
    "    print(f\"Size of layer.parameter is {param.size()}\")\n",
    "    print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "- [`torch.nn API`](https://pytorch.org/docs/stable/nn.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
