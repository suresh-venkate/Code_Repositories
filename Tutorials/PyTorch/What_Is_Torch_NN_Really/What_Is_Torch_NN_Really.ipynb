{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "What_Is_Torch_NN_Really.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suresh-venkate/Code_Repositories/blob/main/Tutorials/PyTorch/What_Is_Torch_NN_Really/What_Is_Torch_NN_Really.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDGdhtt-ii_N"
      },
      "source": [
        "# What is Torch.NN Really? - PyTorch Tutorial\n",
        "\n",
        "**Author:** Suresh Venkatesan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J45t9UP_iqFa"
      },
      "source": [
        "# Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqS5hMOiiZTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c18d89-7f63-403f-eaac-5d675c7449c7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import shutil\n",
        "import pytz\n",
        "import math\n",
        "import requests\n",
        "import pickle\n",
        "import gzip\n",
        "%matplotlib inline\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Ignore the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in7HJmGUoA0f"
      },
      "source": [
        "# Import dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbCn3ovUiZTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "536fea6a-eb6c-4675-a991-621a922088b5"
      },
      "source": [
        "# Download mnist.pkl.gz and copy to /data/mnist folder\n",
        "\n",
        "data_path = Path(\"data\") / \"mnist\" # Define /data/mnist path as PosixPath object\n",
        "data_path.mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "mnist_url = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
        "mnist_fname = \"mnist.pkl.gz\"\n",
        "\n",
        "# Download zip file\n",
        "if not (data_path / mnist_fname).exists():\n",
        "  content = requests.get(mnist_url + mnist_fname).content\n",
        "  (data_path / mnist_fname).open(\"wb\").write(content)\n",
        "  print(\"mnist.pkl.gz created\")\n",
        "else:\n",
        "  print(\"mnist.pkl.gz already exists.\")\n",
        "print()\n",
        "\n",
        "# Unzip zip file and load pickle file contents\n",
        "with gzip.open((data_path / mnist_fname).as_posix(), \"rb\") as f:\n",
        "        ((X_train, y_train), (X_val, y_val), (X_test, y_test)) = pickle.load(f, encoding=\"latin-1\")\n",
        "print(f\"Shape of X_train is {X_train.shape}\")\n",
        "print(f\"Shape of X_val is {X_val.shape}\")\n",
        "print(f\"Shape of X_test is {X_test.shape}\")\n",
        "print()\n",
        "print(f\"Shape of y_train is {y_train.shape}\")\n",
        "print(f\"Shape of y_val is {y_val.shape}\")\n",
        "print(f\"Shape of y_test is {y_test.shape}\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist.pkl.gz already exists.\n",
            "\n",
            "Shape of X_train is (50000, 784)\n",
            "Shape of X_val is (10000, 784)\n",
            "Shape of X_test is (10000, 784)\n",
            "\n",
            "Shape of y_train is (50000,)\n",
            "Shape of y_val is (10000,)\n",
            "Shape of y_test is (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-vQ2vCpotI5"
      },
      "source": [
        "# View one image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UflFPEabovEk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "0bebe862-672a-4c14-c6c4-b82d4eb74526"
      },
      "source": [
        "img = X_train[0].reshape((28, 28))\n",
        "plt.imshow(img, cmap = 'gray');"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMM0yS5So02X"
      },
      "source": [
        "# Convert numpy arrays to PyTorch tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_K_8Bmxo2yI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e07bd7b-688e-4055-df83-26b1bed03b94"
      },
      "source": [
        "X_train, X_val, X_test = map(torch.tensor, (X_train, X_val, X_test))\n",
        "y_train, y_val, y_test = map(torch.tensor, (y_train, y_val, y_test)) \n",
        "\n",
        "print(f\"Shape of X_train is {X_train.shape}\")\n",
        "print(f\"Shape of X_val is {X_val.shape}\")\n",
        "print(f\"Shape of X_test is {X_test.shape}\")\n",
        "print()\n",
        "print(f\"Shape of y_train is {y_train.shape}\")\n",
        "print(f\"Shape of y_val is {y_val.shape}\")\n",
        "print(f\"Shape of y_test is {y_test.shape}\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train is torch.Size([50000, 784])\n",
            "Shape of X_val is torch.Size([10000, 784])\n",
            "Shape of X_test is torch.Size([10000, 784])\n",
            "\n",
            "Shape of y_train is torch.Size([50000])\n",
            "Shape of y_val is torch.Size([10000])\n",
            "Shape of y_test is torch.Size([10000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN1qJKppo8BY"
      },
      "source": [
        "# Build NN from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VRjeGbZo9-J"
      },
      "source": [
        "## Define weights and bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrx68dbFpAPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede6b8d5-6df2-4d0a-8688-67ad5604df6e"
      },
      "source": [
        "# Define weights and bias \n",
        "weights = torch.randn(784, 10) / math.sqrt(784)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(10, requires_grad=True)\n",
        "print(weights.shape, bias.shape)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([784, 10]) torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5di1z3epFTT"
      },
      "source": [
        "## Define functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8jlPQA-pHQg"
      },
      "source": [
        "def log_softmax(x): # Log softmax function\n",
        "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "def model(xb): # Logistic regression model\n",
        "    return log_softmax(xb @ weights + bias)   \n",
        "def loss_func(input, target): # Negative Log-Likelihood loss function\n",
        "    return -input[range(target.shape[0]), target].mean() \n",
        "def accuracy(out, yb): # Accuracy computation\n",
        "    preds = torch.argmax(out, dim = 1)\n",
        "    return (preds == yb).float().mean()"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lyDVhlFpLcH"
      },
      "source": [
        "## Check functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD0VEwOLpMy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af1e5fd-b8cc-42a2-f141-f5fa4331e33f"
      },
      "source": [
        "bat_size = 64\n",
        "X_batch = X_train[0:bat_size]\n",
        "y_batch = y_train[0:bat_size]\n",
        "pred_batch = model(X_batch)\n",
        "\n",
        "n, c = X_train.shape\n",
        "print(n, c)\n",
        "\n",
        "print(loss_func(pred_batch, y_batch))\n",
        "print(accuracy(pred_batch, y_batch))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 784\n",
            "tensor(2.4511, grad_fn=<NegBackward>)\n",
            "tensor(0.0625)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLeA32GfpmoV"
      },
      "source": [
        "## Build and run training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDhdaRwtppjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67f52cdc-2b5d-4cf0-be62-ed4b14f97ced"
      },
      "source": [
        "lr = 0.5  # learning rate\n",
        "epochs = 2  # how many epochs to train for\n",
        "\n",
        "for epoch in range(epochs): # Loop through epochs\n",
        "    for i in range((n - 1) // bat_size + 1): # Loop through batchs\n",
        "        start_i = i * bat_size\n",
        "        end_i = start_i + bat_size\n",
        "        xb = X_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "        loss.backward()\n",
        "        # set_trace()\n",
        "        acc = 100 * accuracy(pred, yb)\n",
        "        # print(f\"Epoch: {epoch}, Batch: {i}, Loss:{loss:0.4f}, Accuracy: {acc:0.2f}\")\n",
        "        with torch.no_grad():\n",
        "            weights -= weights.grad * lr\n",
        "            bias -= bias.grad * lr\n",
        "            weights.grad.zero_()\n",
        "            bias.grad.zero_()\n",
        "    print(f\"Epoch {epoch + 1} completed\")            "
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed\n",
            "Epoch 2 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV4KDbEfqFgn"
      },
      "source": [
        "## Evaluate loss function and accuracy after training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l8EMGb_iZT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006bcc10-7fb8-4b2d-e475-c0670cb6f6a7"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0830, grad_fn=<NegBackward>) tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbLEi5blqNEA"
      },
      "source": [
        "# Refactor using torch.nn.functional\n",
        "\n",
        "Remove manual activation function and loss function. Replace with F.cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUT4nePuqY0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09cfd147-0dd8-46c8-bf9b-0b6bfd017142"
      },
      "source": [
        "loss_func = F.cross_entropy # Cross-entropy loss function (includes log-softmax and NLL loss function)\n",
        "\n",
        "def model(xb): # Remove log-softmax from model\n",
        "    return xb @ weights + bias\n",
        "\n",
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0830, grad_fn=<NllLossBackward>) tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvjLrGGBqjKY"
      },
      "source": [
        "<b>Same results as above obtained.</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7oQf7BViZT6"
      },
      "source": [
        "# Refactor using nn.Module\n",
        "\n",
        "Next up, we'll use ``nn.Module`` and ``nn.Parameter``, for a clearer and more concise training loop. We subclass ``nn.Module`` (which itself is a class and\n",
        "able to keep track of state).  In this case, we want to create a class that holds our weights, bias, and method for the forward step.  ``nn.Module`` has a\n",
        "number of attributes and methods (such as ``.parameters()`` and ``.zero_grad()``) which we will be using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIjSq04KiZT7"
      },
      "source": [
        "# Define class Mnist_Logistic\n",
        "class Mnist_Logistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
        "        self.bias = nn.Parameter(torch.zeros(10))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return xb @ self.weights + self.bias"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRLcXZOnMJiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3b5fe27-ec96-48ad-e966-7ed5da07b464"
      },
      "source": [
        "# Instantiate a model of the above class\n",
        "model = Mnist_Logistic()\n",
        "print(model.weights.shape)\n",
        "print(model.bias.shape)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([784, 10])\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHCfyzlJiZT8"
      },
      "source": [
        "<b>Note: ``nn.Module`` objects are used as if they are functions (i.e they are *callable*), but behind the scenes Pytorch will call our ``forward`` method automatically.</b>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PPml6VJMjor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd84d9c-723c-4d28-d615-2b915276d9a8"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.5404, grad_fn=<NllLossBackward>) tensor(0.0625)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhg0NmAwiZT8"
      },
      "source": [
        "Previously for our training loop we had to update the values for each parameter by name, and manually zero out the grads for each parameter separately, like this:\n",
        "::\n",
        " \n",
        "  with torch.no_grad():\n",
        "      weights -= weights.grad * lr\n",
        "      bias -= bias.grad * lr\n",
        "      weights.grad.zero_()\n",
        "      bias.grad.zero_()\n",
        "\n",
        "\n",
        "Now we can take advantage of model.parameters() and model.zero_grad() (which are both defined by PyTorch for ``nn.Module``) to make those steps more concise\n",
        "and less prone to the error of forgetting some of our parameters, particularly if we had a more complicated model:\n",
        "::\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for p in model.parameters(): p -= p.grad * lr\n",
        "      model.zero_grad()\n",
        "\n",
        "\n",
        "We'll wrap our little training loop in a ``fit`` function so we can run it again later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scb6qvF5iZT9"
      },
      "source": [
        "# Define model fit function\n",
        "def fit():\n",
        "    for epoch in range(epochs):\n",
        "        for i in range((n - 1) // bat_size + 1):\n",
        "            start_i = i * bat_size\n",
        "            end_i = start_i + bat_size\n",
        "            xb = X_train[start_i:end_i]\n",
        "            yb = y_train[start_i:end_i]\n",
        "            pred = model(xb)\n",
        "            loss = loss_func(pred, yb)\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                for p in model.parameters():\n",
        "                    p -= p.grad * lr\n",
        "                model.zero_grad()\n",
        "        acc = 100 * accuracy(model(xb), yb)\n",
        "        print(f\"Epoch: {epoch + 1}, Loss: {loss:0.2f}, Accuracy: {acc:0.2f}\")"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy3Zeu9BN8W0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0913451b-65ab-415c-f091-3cd23e35403c"
      },
      "source": [
        "# Fit model\n",
        "fit()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 0.40, Accuracy: 100.00\n",
            "Epoch: 2, Loss: 0.32, Accuracy: 100.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ici-Ci21OeN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da550edc-0d22-493b-8fef-7f8acf032627"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0819, grad_fn=<NllLossBackward>) tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A08k8NL1iZT9"
      },
      "source": [
        "# Refactor using nn.Linear\n",
        "\n",
        "Next, instead of manually defining and initializing ``self.weights`` and ``self.bias``, and calculating ``xb  @ self.weights + self.bias``, we will instead use the Pytorch class [`nn.Linear`](https://pytorch.org/docs/stable/nn.html#linear-layers) for a\n",
        "linear layer, which does all that for us. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDLlRU1DiZT-"
      },
      "source": [
        "# Re-define Mnist_Logistic class using nn.Linear class\n",
        "class Mnist_Logistic(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(784, 10)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.lin(xb)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj01n14viZT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48af720c-e30e-4112-f18a-e50c8a56c26f"
      },
      "source": [
        "# Instantiate model\n",
        "model = Mnist_Logistic()\n",
        "for param in model.parameters():\n",
        "  print(param)\n",
        "  print(param.shape)\n",
        "  print()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0112,  0.0333,  0.0332,  ..., -0.0110,  0.0094,  0.0325],\n",
            "        [-0.0143,  0.0055, -0.0306,  ..., -0.0313,  0.0230,  0.0299],\n",
            "        [-0.0058, -0.0084,  0.0110,  ..., -0.0158, -0.0115,  0.0170],\n",
            "        ...,\n",
            "        [ 0.0201, -0.0259,  0.0346,  ..., -0.0261, -0.0228, -0.0132],\n",
            "        [-0.0238,  0.0284, -0.0282,  ..., -0.0091,  0.0093,  0.0090],\n",
            "        [ 0.0353, -0.0168, -0.0240,  ..., -0.0148,  0.0100, -0.0020]],\n",
            "       requires_grad=True)\n",
            "torch.Size([10, 784])\n",
            "\n",
            "Parameter containing:\n",
            "tensor([ 0.0158, -0.0060,  0.0308,  0.0111, -0.0225,  0.0280,  0.0206, -0.0354,\n",
            "         0.0239, -0.0069], requires_grad=True)\n",
            "torch.Size([10])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wToSpuGKQCuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75e43f0-0805-4f7a-b0b8-a76014fdc1c3"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.2483, grad_fn=<NllLossBackward>) tensor(0.2500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOVDoWP7iZT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643388dc-d357-46e8-f369-3c4c063df653"
      },
      "source": [
        "# Fit model\n",
        "fit()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 0.40, Accuracy: 100.00\n",
            "Epoch: 2, Loss: 0.31, Accuracy: 100.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHt4BpXsQaNW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc96151f-b0f1-4bb0-841c-82abb6ecbc3f"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0806, grad_fn=<NllLossBackward>) tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKn27xx3iZT_"
      },
      "source": [
        "# Refactor using optim\n",
        "\n",
        "Pytorch also has a package with various optimization algorithms, ``torch.optim``. We can use the ``step`` method from our optimizer to take a forward step, instead of manually updating each parameter.\n",
        "\n",
        "This will let us replace our previous manually coded optimization step:\n",
        "\n",
        "::\n",
        "  with torch.no_grad():\n",
        "      for p in model.parameters(): p -= p.grad * lr\n",
        "      model.zero_grad()\n",
        "\n",
        "and instead use just:\n",
        "::\n",
        "\n",
        "  opt.step()\n",
        "  opt.zero_grad()\n",
        "\n",
        "(``optim.zero_grad()`` resets the gradient to 0 and we need to call it before computing the gradient for the next minibatch.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR8Y3A8FQ80L"
      },
      "source": [
        "# Define get_model function\n",
        "def get_model():\n",
        "    model = Mnist_Logistic()\n",
        "    return model, optim.SGD(model.parameters(), lr = lr)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gbgvyFHRGiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecbef37e-22bb-4a2a-bb6e-6310017c18f2"
      },
      "source": [
        "model, opt = get_model()\n",
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.2177, grad_fn=<NllLossBackward>) tensor(0.2500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7_Ef34siZUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e58df14-2905-4c8a-e2bd-e0c78896c82c"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    for i in range((n - 1) // bat_size + 1):\n",
        "        start_i = i * bat_size\n",
        "        end_i = start_i + bat_size\n",
        "        xb = X_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    acc = 100 * accuracy(model(xb), yb)\n",
        "    print(f\"Epoch: {epoch + 1}, Loss: {loss:0.2f}, Accuracy: {acc:0.2f}\")        "
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 0.40, Accuracy: 100.00\n",
            "Epoch: 2, Loss: 0.31, Accuracy: 100.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9tYlWCNRSpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16eb9d4c-4b33-44a5-b255-728a6544570b"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0821, grad_fn=<NllLossBackward>) tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOVhcmJIiZUA"
      },
      "source": [
        "# Refactor using Dataset\n",
        "\n",
        "PyTorch has an abstract Dataset class.  A Dataset can be anything that has a ``__len__`` function (called by Python's standard ``len`` function) and a ``__getitem__`` function as a way of indexing into it.\n",
        "[`This tutorial`](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) walks through a nice example of creating a custom `FacialLandmarkDataset`` class as a subclass of ``Dataset``.\n",
        "\n",
        "PyTorch's [`TensorDataset`](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset) is a Dataset wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first\n",
        "dimension (batch dimension) of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QNOQT4uiZUB"
      },
      "source": [
        "Both ``x_train`` and ``y_train`` can be combined in a single ``TensorDataset``,\n",
        "which will be easier to iterate over and slice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWZDneT9iZUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b586ff2-a024-442e-a279-9fb0cd857e01"
      },
      "source": [
        "train_ds = TensorDataset(X_train, y_train)\n",
        "print(type(train_ds))\n",
        "print(len(train_ds))\n",
        "X_temp, y_temp = train_ds[0] # Generates the first sample (X_train[0], y_train[0]) as a tuple\n",
        "print(X_temp.shape, y_temp.shape)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.utils.data.dataset.TensorDataset'>\n",
            "50000\n",
            "torch.Size([784]) torch.Size([])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL--f8KoQC7C",
        "outputId": "d4c32c13-f40b-42c4-e505-3487cc61948a"
      },
      "source": [
        "model, opt = get_model()\n",
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3874, grad_fn=<NllLossBackward>) tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g74mNIUhiZUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553efd38-33cb-45f2-ff68-4862ea788252"
      },
      "source": [
        "# Fit model using train_ds\n",
        "for epoch in range(epochs):\n",
        "    for i in range((n - 1) // bat_size + 1):\n",
        "        xb, yb = train_ds[i * bat_size: i * bat_size + bat_size] # Generates both xb and yb in one line\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    acc = 100 * accuracy(model(xb), yb)\n",
        "    print(f\"Epoch: {epoch + 1}, Loss: {loss:0.2f}, Accuracy: {acc:0.2f}\")"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 0.40, Accuracy: 100.00\n",
            "Epoch: 2, Loss: 0.31, Accuracy: 100.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22shmvFNQciV",
        "outputId": "553735b8-c2d4-4535-b52e-325902aa5ac6"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0813, grad_fn=<NllLossBackward>) tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u81NHckIiZUC"
      },
      "source": [
        "# Refactor using DataLoader\n",
        "\n",
        "Pytorch's ``DataLoader`` is responsible for managing batches. We can create a ``DataLoader`` from any ``Dataset``. ``DataLoader`` makes it easier\n",
        "to iterate over batches. Rather than having to use ``train_ds[i*bs : i*bs+bs]``, the DataLoader gives us each minibatch automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH8nXceViZUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577dd421-8735-4dce-95f9-a8ff9db113dd"
      },
      "source": [
        "# Combine X_train and y_train tensors into a single TensorDataset object\n",
        "train_ds = TensorDataset(X_train, y_train) \n",
        "# Generate a dataloader object from train_ds with a batch size of bat_size\n",
        "train_dl = DataLoader(train_ds, batch_size = bat_size)\n",
        "print(f\"Type of train_dl is {type(train_dl)}\")\n",
        "print(f\"Batch size of train_dl is {train_dl.batch_size}\")\n",
        "print(f\"Number of batches in train_dl is {len(train_dl)}\")\n",
        "ind = 0\n",
        "for xb, yb in train_dl:\n",
        "  if (ind == 0):\n",
        "    print(xb.shape, yb.shape)\n",
        "    ind += 1"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of train_dl is <class 'torch.utils.data.dataloader.DataLoader'>\n",
            "Batch size of train_dl is 64\n",
            "Number of batches in train_dl is 782\n",
            "torch.Size([64, 784]) torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0NO5zpUVH1G",
        "outputId": "585be7c9-0e99-48fd-d072-ef886a69599e"
      },
      "source": [
        "model, opt = get_model()\n",
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.2687, grad_fn=<NllLossBackward>) tensor(0.1250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC2f_nVhiZUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52ab73d5-22a5-4acd-a627-55d7d6934aa8"
      },
      "source": [
        "# Fit model using train_dl\n",
        "for epoch in range(epochs):\n",
        "    for xb, yb in train_dl: # train_dl automatically generates batches of xb and yb\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "    acc = 100 * accuracy(model(xb), yb)\n",
        "    print(f\"Epoch: {epoch + 1}, Loss: {loss:0.2f}, Accuracy: {acc:0.2f}\")"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 0.40, Accuracy: 100.00\n",
            "Epoch: 2, Loss: 0.31, Accuracy: 100.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1svKVmLNVe_n",
        "outputId": "4a19351e-b74c-4599-c934-69d917c53c00"
      },
      "source": [
        "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0819, grad_fn=<NllLossBackward>) tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN8KHLlViZUD"
      },
      "source": [
        "# Add validation\n",
        "\n",
        "* We now add a validation set to check for overfitting.\n",
        "* Shuffling the training data is [`important`](https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks) to prevent correlation between batches and overfitting. \n",
        "* On the other hand, the validation loss will be identical whether we shuffle the validation set or not.\n",
        "Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n",
        "* We'll use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn't need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBf0dsUsiZUE"
      },
      "source": [
        "train_ds = TensorDataset(X_train, y_train)\n",
        "train_dl = DataLoader(train_ds, batch_size = bat_size, shuffle = True)\n",
        "\n",
        "val_ds = TensorDataset(X_val, y_val)\n",
        "val_dl = DataLoader(val_ds, batch_size = bat_size * 2)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3uk-495iZUE"
      },
      "source": [
        "We will calculate and print the validation loss at the end of each epoch.\n",
        "\n",
        "(Note that we always call ``model.train()`` before training, and ``model.eval()``\n",
        "before inference, because these are used by layers such as ``nn.BatchNorm2d``\n",
        "and ``nn.Dropout`` to ensure appropriate behaviour for these different phases.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoK8crKPkUmJ"
      },
      "source": [
        "model, opt = get_model()"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjzGR8CniZUE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ce4f2f-6e49-415d-d6a8-162d964f9adf"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_dl:\n",
        "        pred = model(xb)\n",
        "        loss = loss_func(pred, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = sum(loss_func(model(xb), yb) for xb, yb in val_dl)\n",
        "\n",
        "    print(epoch, val_loss / len(val_dl))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.3350)\n",
            "1 tensor(0.4243)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBW-izoxiZUF"
      },
      "source": [
        "# Create fit() and get_data()\n",
        "\n",
        "We'll now do a little refactoring of our own. Since we go through a similar\n",
        "process twice of calculating the loss for both the training set and the\n",
        "validation set, let's make that into its own function, ``loss_batch``, which\n",
        "computes the loss for one batch.\n",
        "\n",
        "We pass an optimizer in for the training set, and use it to perform\n",
        "backprop.  For the validation set, we don't pass an optimizer, so the\n",
        "method doesn't perform backprop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujjYAzw4iZUF"
      },
      "source": [
        "def loss_batch(model, loss_func, xb, yb, opt = None):\n",
        "    loss = loss_func(model(xb), yb)\n",
        "\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    return loss.item(), len(xb)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBc6lizEiZUF"
      },
      "source": [
        "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_dl:\n",
        "            loss_batch(model, loss_func, xb, yb, opt)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            losses, nums = zip(\n",
        "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
        "            )\n",
        "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "\n",
        "        print(epoch, val_loss)\n",
        "    return losses, nums"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm_nvKU3iZUG"
      },
      "source": [
        "def get_data(train_ds, val_ds, bat_size):\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size = bat_size, shuffle = True),\n",
        "        DataLoader(val_ds, batch_size = bat_size * 2),\n",
        "    )"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n-CqJHLiZUG"
      },
      "source": [
        "<b>Now, the whole process of obtaining the data loaders and fitting the model can be run in just 3 lines of code:</b>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2LhPWJpiZUH",
        "outputId": "d412ea55-adc6-4b09-a95d-a27d3dcf07c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dl, val_dl = get_data(train_ds, val_ds, bat_size)\n",
        "model, opt = get_model()\n",
        "losses, nums = fit(epochs, model, loss_func, opt, train_dl, val_dl)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.32407091336250304\n",
            "1 0.31521545573472975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NAs9PHCqnmS"
      },
      "source": [
        "# Pending"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfFAaJmGiZUH"
      },
      "source": [
        "You can use these basic 3 lines of code to train a wide variety of models.\n",
        "Let's see if we can use them to train a convolutional neural network (CNN)!\n",
        "\n",
        "Switch to CNN\n",
        "-------------\n",
        "\n",
        "We are now going to build our neural network with three convolutional layers.\n",
        "Because none of the functions in the previous section assume anything about\n",
        "the model form, we'll be able to use them to train a CNN without any modification.\n",
        "\n",
        "We will use Pytorch's predefined\n",
        "`Conv2d <https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d>`_ class\n",
        "as our convolutional layer. We define a CNN with 3 convolutional layers.\n",
        "Each convolution is followed by a ReLU.  At the end, we perform an\n",
        "average pooling.  (Note that ``view`` is PyTorch's version of numpy's\n",
        "``reshape``)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO8iDSAJiZUI"
      },
      "source": [
        "class Mnist_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        xb = xb.view(-1, 1, 28, 28)\n",
        "        xb = F.relu(self.conv1(xb))\n",
        "        xb = F.relu(self.conv2(xb))\n",
        "        xb = F.relu(self.conv3(xb))\n",
        "        xb = F.avg_pool2d(xb, 4)\n",
        "        return xb.view(-1, xb.size(1))\n",
        "\n",
        "lr = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9VnucEXiZUJ"
      },
      "source": [
        "`Momentum <https://cs231n.github.io/neural-networks-3/#sgd>`_ is a variation on\n",
        "stochastic gradient descent that takes previous updates into account as well\n",
        "and generally leads to faster training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH0b0Z4niZUJ"
      },
      "source": [
        "model = Mnist_CNN()\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gACPLVjiZUJ"
      },
      "source": [
        "nn.Sequential\n",
        "------------------------\n",
        "\n",
        "``torch.nn`` has another handy class we can use to simplify our code:\n",
        "`Sequential <https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential>`_ .\n",
        "A ``Sequential`` object runs each of the modules contained within it, in a\n",
        "sequential manner. This is a simpler way of writing our neural network.\n",
        "\n",
        "To take advantage of this, we need to be able to easily define a\n",
        "**custom layer** from a given function.  For instance, PyTorch doesn't\n",
        "have a `view` layer, and we need to create one for our network. ``Lambda``\n",
        "will create a layer that we can then use when defining a network with\n",
        "``Sequential``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJVzlUNhiZUK"
      },
      "source": [
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.func(x)\n",
        "\n",
        "\n",
        "def preprocess(x):\n",
        "    return x.view(-1, 1, 28, 28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3qR0HlgiZUL"
      },
      "source": [
        "The model created with ``Sequential`` is simply:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8XlkUCGiZUL"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    Lambda(preprocess),\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AvgPool2d(4),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        ")\n",
        "\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0WdVnFTiZUM"
      },
      "source": [
        "Wrapping DataLoader\n",
        "-----------------------------\n",
        "\n",
        "Our CNN is fairly concise, but it only works with MNIST, because:\n",
        " - It assumes the input is a 28\\*28 long vector\n",
        " - It assumes that the final CNN grid size is 4\\*4 (since that's the average\n",
        "pooling kernel size we used)\n",
        "\n",
        "Let's get rid of these two assumptions, so our model works with any 2d\n",
        "single channel image. First, we can remove the initial Lambda layer by\n",
        "moving the data preprocessing into a generator:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1jb7Lp0iZUM"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    return x.view(-1, 1, 28, 28), y\n",
        "\n",
        "\n",
        "class WrappedDataLoader:\n",
        "    def __init__(self, dl, func):\n",
        "        self.dl = dl\n",
        "        self.func = func\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "    def __iter__(self):\n",
        "        batches = iter(self.dl)\n",
        "        for b in batches:\n",
        "            yield (self.func(*b))\n",
        "\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
        "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOMvJAbUiZUM"
      },
      "source": [
        "Next, we can replace ``nn.AvgPool2d`` with ``nn.AdaptiveAvgPool2d``, which\n",
        "allows us to define the size of the *output* tensor we want, rather than\n",
        "the *input* tensor we have. As a result, our model will work with any\n",
        "size input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzOttizBiZUM"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
        ")\n",
        "\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUHmlJ-wiZUN"
      },
      "source": [
        "Let's try it out:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUS7qJ2TiZUN"
      },
      "source": [
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqhW3o0AiZUN"
      },
      "source": [
        "Using your GPU\n",
        "---------------\n",
        "\n",
        "If you're lucky enough to have access to a CUDA-capable GPU (you can\n",
        "rent one for about $0.50/hour from most cloud providers) you can\n",
        "use it to speed up your code. First check that your GPU is working in\n",
        "Pytorch:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh73uVOoiZUN"
      },
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST0SSkcTiZUO"
      },
      "source": [
        "And then create a device object for it:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFrgltV-iZUO"
      },
      "source": [
        "dev = torch.device(\n",
        "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7DNi-31iZUO"
      },
      "source": [
        "Let's update ``preprocess`` to move batches to the GPU:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMZzfl8UiZUO"
      },
      "source": [
        "def preprocess(x, y):\n",
        "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
        "\n",
        "\n",
        "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
        "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
        "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72cdFecviZUP"
      },
      "source": [
        "Finally, we can move our model to the GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jYFAHZ-iZUP"
      },
      "source": [
        "model.to(dev)\n",
        "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC0KJ9ABiZUQ"
      },
      "source": [
        "You should find it runs faster now:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jye6RrCGiZUQ"
      },
      "source": [
        "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAqtuO4YiZUR"
      },
      "source": [
        "Closing thoughts\n",
        "-----------------\n",
        "\n",
        "We now have a general data pipeline and training loop which you can use for\n",
        "training many types of models using Pytorch. To see how simple training a model\n",
        "can now be, take a look at the `mnist_sample` sample notebook.\n",
        "\n",
        "Of course, there are many things you'll want to add, such as data augmentation,\n",
        "hyperparameter tuning, monitoring training, transfer learning, and so forth.\n",
        "These features are available in the fastai library, which has been developed\n",
        "using the same design approach shown in this tutorial, providing a natural\n",
        "next step for practitioners looking to take their models further.\n",
        "\n",
        "We promised at the start of this tutorial we'd explain through example each of\n",
        "``torch.nn``, ``torch.optim``, ``Dataset``, and ``DataLoader``. So let's summarize\n",
        "what we've seen:\n",
        "\n",
        " - **torch.nn**\n",
        "\n",
        "   + ``Module``: creates a callable which behaves like a function, but can also\n",
        "     contain state(such as neural net layer weights). It knows what ``Parameter`` (s) it\n",
        "     contains and can zero all their gradients, loop through them for weight updates, etc.\n",
        "   + ``Parameter``: a wrapper for a tensor that tells a ``Module`` that it has weights\n",
        "     that need updating during backprop. Only tensors with the `requires_grad` attribute set are updated\n",
        "   + ``functional``: a module(usually imported into the ``F`` namespace by convention)\n",
        "     which contains activation functions, loss functions, etc, as well as non-stateful\n",
        "     versions of layers such as convolutional and linear layers.\n",
        " - ``torch.optim``: Contains optimizers such as ``SGD``, which update the weights\n",
        "   of ``Parameter`` during the backward step\n",
        " - ``Dataset``: An abstract interface of objects with a ``__len__`` and a ``__getitem__``,\n",
        "   including classes provided with Pytorch such as ``TensorDataset``\n",
        " - ``DataLoader``: Takes any ``Dataset`` and creates an iterator which returns batches of data.\n",
        "\n"
      ]
    }
  ]
}